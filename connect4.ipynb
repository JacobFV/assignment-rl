{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math, random, time, datetime\n",
    "import pickle, itertools, functools\n",
    "# Many default parameters are included in jnumpy and are optional.\n",
    "# I only resort to using `Optional` in the type annotations where the\n",
    "# context does not make this clear.  \n",
    "from typing import Tuple, List, Mapping, Optional, Union, NamedTuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jnumpy: Jacob's numpy library for machine learning\n",
    "# Copyright (c) 2021 Jacob F. Valdez. Released under the MIT license.\n",
    "\n",
    "\n",
    "V = np.array # V is for Value type\n",
    "Vs = Tuple[V]\n",
    "Vss = Union[V,Vs]\n",
    "\n",
    "\n",
    "class ExecutionMode:\n",
    "    EAGER=1\n",
    "    STATIC=2  # STATIC execution mode not supported\n",
    "    \n",
    "EXECUTION_MODE = ExecutionMode.EAGER\n",
    "\n",
    "\n",
    "class T:\n",
    "    \"\"\"Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, val: Optional[V] = None):\n",
    "        self.val = val\n",
    "        \n",
    "        if val is None:\n",
    "            raise 'STATIC execution mode not supported'\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return eval(\"Index\")(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        raise NotImplementedError('slice assign not yet supported')\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return eval(\"Add\")(self, other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return eval('Neg')(self)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return eval(\"Sub\")(self, other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return eval(\"Mul\")(self, other)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        return eval(\"Pow\")(self, other)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        return eval(\"MatMul\")(self, other)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.val.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.val.ndim\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.val.size\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.val.dtype\n",
    "\n",
    "    @property\n",
    "    def T(self, axes: Tuple[int] = None):\n",
    "        return eval(\"Transpose\")(self, axes=axes)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.val == other.val\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.val)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.val)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.val)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.val.__getstate__()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.val = state\n",
    "\n",
    "    def __array__(self):\n",
    "        return self.val.__array__()\n",
    "\n",
    "\n",
    "Ts = Tuple[T]\n",
    "Tss = Union[T,Ts]\n",
    "\n",
    "\n",
    "class Var(T):\n",
    "    \"\"\"Variable Tensor\"\"\"\n",
    "    def __init__(self, val: Optional[V] = None, trainable: bool = True):\n",
    "        \n",
    "        self.trainable = trainable\n",
    "        super().__init__(val=val)\n",
    "\n",
    "\n",
    "class Op(T):\n",
    "    \"\"\"Operation-backed Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, *inputs: T):\n",
    "        \"\"\"Make sure to set any variables you might need in `forward` \n",
    "        before initializing when the graph is in eager execution mode\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_ts = inputs\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.EAGER:\n",
    "            val = self.forward(tuple(i.val for i in inputs))\n",
    "        else:\n",
    "            val = None\n",
    "        \n",
    "        super().__init__(val=val)\n",
    "        \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "\n",
    "\n",
    "class Transpose(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axes: Tuple[int] = None):\n",
    "        \n",
    "        self.forward_kwargs = dict()\n",
    "        self.reverse_kwargs = dict()\n",
    "        \n",
    "        if axes is not None:\n",
    "            self.forward_kwargs['axes'] = axes\n",
    "            self.reverse_kwargs['axes'] = tuple(reversed(axes))\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.transpose(**self.forward_kwargs)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "\n",
    "        dX = dY.transpose(**self.reverse_kwargs)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Reshape(Op):\n",
    "    \n",
    "    def __init__(self, t: T, shape: Tuple[int]):\n",
    "        \n",
    "        self.reshape_shape = shape\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.reshape(self.reshape_shape)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = dY.reshape(tuple(reversed(self.reshape_shape)))\n",
    "        \n",
    "        return (dX,)\n",
    " \n",
    "\n",
    "class Concat(Op):\n",
    "    \n",
    "    def __init__(self, ts: List[T], axis: int = 0):\n",
    "        \"\"\"Concatenates input tensors along an axis\n",
    "\n",
    "        Args:\n",
    "            t (T): [description]\n",
    "            axis (int, optional): Axis to concatenate along. Defaults to 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.axis = axis\n",
    "        self.orig_axis_lens = [t.shape[axis] for t in ts]\n",
    "\n",
    "        super().__init__(*ts)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        Xs = inputs\n",
    "        \n",
    "        Y = np.concatenate(Xs, axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "        \n",
    "        dXs = np.split(dY, self.orig_axis_dims, axis=self.axis)[0]\n",
    "        \n",
    "        return dXs\n",
    "\n",
    "\n",
    "class Index(Op):\n",
    "    \n",
    "    def __init__(self, t: T, indices):\n",
    "        \"\"\"Slices a tensor along all axes.\n",
    "\n",
    "        Args:\n",
    "            t (T): The tensor to slice\n",
    "            indices (Tuple[slice]):  The partial or full indices to slice on `t`.\n",
    "                Can be an index, single slice, tuple of slices, or Ellipsis.\n",
    "                `None` is not allowed.\n",
    "        \"\"\"\n",
    "        if not isinstance(indices, tuple):\n",
    "            indices = (indices,)\n",
    "\n",
    "        self.indices = indices\n",
    "        \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X[self.indices]\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.zeros(X.shape)\n",
    "        dX[self.indices] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceSum(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.sum(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.repeat(\n",
    "            np.expand_dims(dY, axis=self.axis),\n",
    "            X.shape[self.axis],\n",
    "            axis=self.axis\n",
    "        )\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceMax(Op):\n",
    "    \"\"\"Differentiable max operator\"\"\"\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.max(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        print(X.shape, dY.shape)\n",
    "\n",
    "        dX = np.zeros_like(X)\n",
    "        dX[np.argmax(X, axis=self.axis)] = dY\n",
    "\n",
    "        print(dX.shape)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "class ReduceMin(Op):\n",
    "    \"\"\"Differentiable min operator\"\"\"\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.min(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.zeros_like(X)\n",
    "        dX[np.argmin(X, axis=self.axis)] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class NaN2Num(Op):\n",
    "    \n",
    "    def __init__(self, t: T, posinf: float = 1e3, neginf: float = -1e3):\n",
    "        self.posinf = posinf\n",
    "        self.neginf = neginf\n",
    "            \n",
    "        super().__init__(t)\n",
    "\n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.nan_to_num(X, posinf=self.posinf, neginf=self.neginf)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = np.nan_to_num(dZ, posinf=10., neginf=-10.)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Linear(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class StopGrad(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = np.zeros_like(dZ)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Neg(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = -X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = -dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Add(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X + Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Sub(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X - Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = -dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Mul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X * Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Y * dZ\n",
    "        dY = X * dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class MatMul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X @ Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]  # [A,B]\n",
    "        Y = inputs[1]  # [B,C]\n",
    "        dZ = top_grad  # [A,C]\n",
    "        \n",
    "        dX = dZ @ Y.transpose()\n",
    "        dY = X.transpose() @ dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Exp(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.exp(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Z * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Sigm(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = 1 / (1 + np.exp(-X))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Z * (1 - Z) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Tanh(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.tanh(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = ((1 - Z)**2) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Relu(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X > 0) * X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = (X > 0) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Threshold(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X >= 0)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Pow(Op):\n",
    "    \n",
    "    def __init__(self, x: T, power: int):\n",
    "        \n",
    "        self.power = power\n",
    "        \n",
    "        super().__init__(x)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        \n",
    "        Y = X ** p\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = p * X ** (p-1) * dY\n",
    "        dX = np.nan_to_num(dX, posinf=1e3, neginf=-1e3)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, lr: float = 0.001):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.debug = False\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.STATIC:\n",
    "            raise 'STATIC execution mode not enabled'\n",
    "        \n",
    "        self.bprop(t_out=t, output_grad=-np.ones_like(t.val))\n",
    "        \n",
    "    def bprop(self, t_out: T, output_grad: V):\n",
    "        \n",
    "        output_grad = np.nan_to_num(output_grad, posinf=10., neginf=-10.)\n",
    "        \n",
    "        assert isinstance(t_out, (Var, Op))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f'bp {t_out} output_grad:')\n",
    "            print(output_grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        This approach does not efficiently handle weights that are consumed by multiple nodes\n",
    "        It would be better to treat backpropagation from a spreading-network-delta perspective\n",
    "        than assume everything is a tree (That's also how I should do STATIC execution refresh)\n",
    "        This should still work though, but it's just going to set the same weight multiple times\n",
    "        for every downstream consumer.\n",
    "\n",
    "        Actually, the whole thesis of minibatch gradient descent is that we can approximate a global\n",
    "        gradient by updates on local subsets of data, so it might be sufficient to leave the code\n",
    "        as is.\n",
    "\n",
    "        However this approach will still take unnecessary walks down the tree in depth-first fashion.\n",
    "        Innefficient: Yes; Works: Yes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # iteratively called\n",
    "        if isinstance(t_out, Var):\n",
    "            if t_out.trainable:\n",
    "                #print('output_grad', output_grad.shape)\n",
    "                if self.debug:                    \n",
    "                    print('t_out.val (old)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "                \n",
    "                # yucky duct tape to handle batch size differences\n",
    "                if t_out.shape[0] == 1 and output_grad.shape[0] > 1:\n",
    "                    output_grad = np.sum(output_grad, axis=0)[None, ...]\n",
    "\n",
    "                t_out.val = t_out.val + (self.lr * output_grad)\n",
    "                if self.debug:\n",
    "                    print('t_out.val (new)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "            \n",
    "        elif isinstance(t_out, Op):\n",
    "            input_grads = t_out.reverse_grad(\n",
    "                inputs=tuple(t.val for t in t_out.input_ts),\n",
    "                output=t_out.val, top_grad=output_grad)\n",
    "            \n",
    "            for input_t, input_grad in zip(t_out.input_ts, input_grads):\n",
    "                self.bprop(t_out=input_t, output_grad=input_grad)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._built = False\n",
    "        self._loss = Var(np.zeros(()), trainable=False)\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return self._loss\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return []\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, X_T: T) -> T:\n",
    "        if not self._built:\n",
    "            self.build(X_T.shape)\n",
    "            self._built = True\n",
    "\n",
    "        # reset the regularization loss\n",
    "        self._loss = Var(0., trainable=False)\n",
    "\n",
    "        return self.forward(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.47341449, 0.92215585, 0.49653451, 0.24044087, 0.30329901],\n",
       "        [0.69933922, 0.69892664, 0.33085959, 0.66976066, 0.41326414],\n",
       "        [0.17942165, 0.53263814, 0.79008068, 0.975783  , 0.97230517]]),\n",
       " array([[-0.        ,  0.05338773, -0.        , -0.        ,  0.03566182,\n",
       "          0.11843629, -0.        , -0.        ,  0.00598078, -0.        ],\n",
       "        [-0.        ,  0.04751799, -0.        , -0.        ,  0.00960446,\n",
       "          0.11009793, -0.        , -0.        , -0.        , -0.        ],\n",
       "        [ 0.01193628,  0.05865564, -0.        , -0.        ,  0.02602443,\n",
       "          0.10098204, -0.        , -0.        , -0.        , -0.        ]]),\n",
       " Tensor(0.008879301219145054),\n",
       " [Tensor([[-0.02654085  0.02194476 -0.01114734 -0.00194221 -0.02703878  0.03109212\n",
       "    -0.04229214  0.03544542 -0.01545945 -0.02939066]\n",
       "   [-0.00902686  0.00109665 -0.02005684 -0.04683567  0.02344125  0.04522875\n",
       "    -0.02683444 -0.00233658  0.0251801  -0.00712322]\n",
       "   [ 0.0156141   0.02062577  0.01312846 -0.02250554  0.04827764  0.02442692\n",
       "    -0.04957428  0.02409852 -0.04235379 -0.02794712]\n",
       "   [-0.01502717 -0.02647742 -0.03203181  0.04420397 -0.00972004 -0.0050907\n",
       "    -0.00201962 -0.01498934 -0.04188641  0.02649507]\n",
       "   [ 0.042563    0.03820894 -0.01008864  0.04186276 -0.02313876  0.00878465\n",
       "     0.0154875  -0.04598685  0.03673337 -0.04232571]]),\n",
       "  Tensor([[-1.75510375e-02  2.65235945e-02  3.37735456e-03 -4.23182159e-02\n",
       "     1.22294229e-02  4.84397091e-02 -4.50454628e-02 -3.85841394e-02\n",
       "     1.00396551e-02  4.63057102e-05]])])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "        units: int, \n",
    "        activation: Op = None, \n",
    "        use_bias: bool = True,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None\n",
    "    ):\n",
    "        super(Dense, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "            \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(input_shape[-1], self.units))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.units))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # compute presynaptic input\n",
    "        Z_T = X_T @ self.W_T\n",
    "\n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply activation\n",
    "        Y_T = self.activation(Z_T)\n",
    "        \n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Dense(10, Relu, 0.1, 0.1, 0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(3, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.val, Y_T.val, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 7, 70, 5),\n",
       " (2, 7, 70, 64),\n",
       " Tensor(0.6699146936503166),\n",
       " [Tensor([[ 0.01527881  0.02600752  0.03247172 ... -0.01511944 -0.01647132\n",
       "     0.03316543]\n",
       "   [ 0.01669521  0.03622749 -0.02691839 ...  0.02377475  0.01226977\n",
       "    -0.04830599]\n",
       "   [-0.04641384  0.01781307  0.04249433 ...  0.02487577 -0.04992451\n",
       "    -0.03800749]\n",
       "   ...\n",
       "   [-0.01217828  0.02609123 -0.00124397 ...  0.0140765   0.03210855\n",
       "    -0.04709179]\n",
       "   [ 0.01680349  0.00815955 -0.0072685  ...  0.02740602  0.03211104\n",
       "     0.01199867]\n",
       "   [-0.03913488  0.04613883 -0.04478316 ...  0.01922557 -0.04613788\n",
       "    -0.04832159]])])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv2D(Layer):\n",
    "    \"\"\"Standard 2D Conv layer.\n",
    "    I.E. convolves over Tensors shaped [B, H, W, D]\n",
    "    to produce [B, H-2*kernel_size, W-2*kernel_size, filters]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        filters: int, \n",
    "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
    "        strides: Union[int, Tuple[int, int]] = 1,\n",
    "        padding: str = 'valid',  # 'valid' or 'same'\n",
    "        activation: Op = None,\n",
    "        use_bias: bool = False,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None,\n",
    "    ):\n",
    "        super(Conv2D, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1, 'kernel_size must be odd'\n",
    "        if isinstance(strides, int):\n",
    "            strides = (strides, strides)\n",
    "        assert strides[0] > 0 and strides[1] > 0, 'strides must be positive'\n",
    "        padding = padding.lower()\n",
    "        assert padding in ('valid', 'same'), 'padding must be valid or same'\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(\n",
    "            self.kernel_size[0]*self.kernel_size[1]*input_shape[-1], \n",
    "            self.filters))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.filters))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # maybe pad input\n",
    "        if self.padding == 'same':\n",
    "\n",
    "            # various padding sizes, strides, and offsets\n",
    "            # 0   1   2   3   4\n",
    "            #     0 1 2 3 4\n",
    "            #         0 1 2 3 4 5 6 7 8 9\n",
    "            #         0 1 2 3 4\n",
    "            #         0   1   2   3   4\n",
    "\n",
    "            pad_top = self.strides[0]*(self.kernel_size[0]-1)//2\n",
    "            pad_bottom = pad_top\n",
    "            pad_left = self.strides[1]*(self.kernel_size[1]-1)//2\n",
    "            pad_right = pad_left\n",
    "            B, H_orig, W_orig, C = X_T.shape\n",
    "\n",
    "            # pad height\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, pad_top, W_orig, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, pad_bottom, W_orig, C)), trainable=False),\n",
    "            ], axis=1)\n",
    "\n",
    "            # pad width\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_left, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_right, C)), trainable=False),\n",
    "            ], axis=2)\n",
    "\n",
    "        elif self.padding == 'valid':\n",
    "            pass\n",
    "\n",
    "        # stack the input tensor along the channel axis\n",
    "        # but shifted by all possible kernel shifts\n",
    "        stack = []\n",
    "        for shift in itertools.product(range(0, self.strides[0]*self.kernel_size[0], self.strides[0]),\n",
    "                                       range(0, self.strides[1]*self.kernel_size[1], self.strides[1])):\n",
    "            stack.append(X_T[\n",
    "                :,\n",
    "                shift[0]:,\n",
    "                shift[1]:,\n",
    "                :\n",
    "            ])\n",
    "\n",
    "        # clip stack to greatest common shape\n",
    "        min_shape = np.min(np.array([s.shape for s in stack]), axis=0)\n",
    "        stack = [s[:, :min_shape[1], :min_shape[2], :] for s in stack]\n",
    "\n",
    "        # stack the shifted tensors along the channel axis\n",
    "        stacked = Concat(stack, axis=3)  # [B, H-k_h//2, W-k_w//2, C*k_h*k_w]\n",
    "\n",
    "        # convolve over the stacked tensors\n",
    "        Z_T = stacked @ self.W_T  \n",
    "    \n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply the activation function\n",
    "        Y_T = self.activation(Z_T)\n",
    "\n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Conv2D(filters=64, kernel_size=5, strides=5, padding='same', weight_L2=0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 7, 70, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxisMaxPooling(Layer):\n",
    "\n",
    "    def __init__(self, axis: int):\n",
    "        super(AxisMaxPooling, self).__init__()\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        return ReduceMax(X_T, self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(Layer):\n",
    "\n",
    "    def __init__(self, fn: Callable[[T], T]):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        return self.fn(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3, 4, 5), (2, 60), Tensor(0.0), [])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Flatten(Layer):\n",
    "    \"\"\"Flattens all non-batch dimensions into a single axis\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return []\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        flat_dims = functools.reduce(lambda x, y: x*y, X_T.shape[1:])\n",
    "        Y_T = Reshape(X_T, (X_T.shape[0], flat_dims))\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Flatten()\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 3, 4, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[-0.16262142]])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        super(Sequential, self).__init__()\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        for layer in self.layers:\n",
    "            X_T = layer(X_T)\n",
    "        return X_T\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return functools.reduce(lambda x, y: x+y, \n",
    "            [layer.loss for layer in self.layers])\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        trainable_vars = []\n",
    "        for layer in self.layers:\n",
    "            trainable_vars += layer.trainable_variables\n",
    "        return trainable_vars\n",
    "\n",
    "\n",
    "net = Sequential([\n",
    "    Dense(10, Relu),\n",
    "    Dense(128, Relu),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(X_T)\n",
    "\n",
    "img_T = Var(np.random.uniform(0, 1, size=(1, 28, 28, 1)), trainable=False)\n",
    "net = Sequential([\n",
    "    Conv2D(32, 3, 2, activation=Relu),\n",
    "    Conv2D(64, 3, 2, activation=Relu),\n",
    "    Flatten(),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(img_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standards:\n",
    "- `Step`: uses batch dimension (except `done` which is always a bool)\n",
    "- `Agent` uses batch dimension\n",
    "- `Environment` doesn't use batch dimension\n",
    "\n",
    "This means you will have to use `Step.batch` and `Step.unbatch` in your training/running loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step:\n",
    "    \"\"\"Single step.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        obs: np.ndarray,\n",
    "        next_obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: np.ndarray,\n",
    "        done: bool,\n",
    "        info: any\n",
    "    ):\n",
    "        self.obs = obs\n",
    "        self.next_obs = next_obs\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.info = info\n",
    "\n",
    "    @staticmethod\n",
    "    def unbatch(step: Step) -> List[Step]:\n",
    "        return [\n",
    "            Step(\n",
    "                obs=step.obs[i:i+1],\n",
    "                next_obs=step.next_obs[i:i+1],\n",
    "                action=step.action[i:i+1],\n",
    "                reward=step.reward[i:i+1],\n",
    "                done=step.done,\n",
    "                info=step.info[i:i+1],\n",
    "            )\n",
    "            for i in range(step.obs.shape[0])\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def batch(steps: List[BatchStep]) -> BatchStep:\n",
    "        return BatchStep(\n",
    "            obs=np.concatenate([step.obs for step in steps], axis=0),\n",
    "            next_obs=np.concatenate([step.next_obs for step in steps], axis=0),\n",
    "            action=np.concatenate([step.action for step in steps], axis=0),\n",
    "            reward=np.concatenate([step.reward for step in steps], axis=0),\n",
    "            done=any(step.done for step in steps),\n",
    "            info=[step.info for step in steps])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_no_batch_axis(step: NoBatchStep) -> Step:\n",
    "        return Step(\n",
    "            obs=step.obs[None, ...],\n",
    "            next_obs=step.next_obs[None, ...],\n",
    "            action=step.action[None, ...],\n",
    "            reward=step.reward[None, ...],\n",
    "            done=step.done,\n",
    "            info=[step.info]\n",
    "        )\n",
    "\n",
    "# dimensional type hinting\n",
    "BatchStep = Step\n",
    "NoBatchStep = Step\n",
    "\n",
    "Traj = List[BatchStep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"RL environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        \"\"\"Resets the environment\n",
    "\n",
    "        Returns:\n",
    "            Step: Initial step. The `next_obs` attribute should be set \n",
    "                with an initial observation. `done` should be False. \n",
    "                `obs` and `action` should not be used.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Step:\n",
    "        \"\"\"Computes one logical step in the environment\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): The action to take.\n",
    "\n",
    "        Returns:\n",
    "            Step: Step resulting from taking `action`. The `next_obs` attribute\n",
    "                should be set with the observation resulting from taking the `action`\n",
    "                in the current environment state. `obs` should not be used. If the \n",
    "                environment is turn-based, then the reward should correspond to the \n",
    "                agent that just acted (not the next agent in line to act).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NoBatchEnv(Environment):\n",
    "    \"\"\"Environment with no batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "\n",
    "class BatchEnv(Environment):\n",
    "    \"\"\"Environment with batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> BatchStep:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch1Env(BatchEnv):\n",
    "    \"\"\"Adds a batch axis to all outgoing Steps and strips it off incoming Steps.\"\"\"\n",
    "\n",
    "    def __init__(self, env: NoBatchEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.reset())\n",
    "\n",
    "    def step(self, action: np.array) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.step(action[0]))\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEnv(BatchEnv):\n",
    "    \"\"\"Keeps reseting the same environment in a batch.\n",
    "    \n",
    "    Declares itself done when a total of `batch_size` individual environment\n",
    "    dones are experienced.\n",
    "\n",
    "    NOTE: Individual environments should be `NoBatchEnv`'s.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, env_init_fn: Callable[[], NoBatchEnv]):\n",
    "        self.batch_size = batch_size\n",
    "        self.env_init_fn = env_init_fn\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        self.dones = 0\n",
    "        self.envs = [self.env_init_fn() for _ in range(self.batch_size)]\n",
    "        steps = [env.reset() for env in self.envs]\n",
    "        steps = [Step.from_no_batch_axis(step) for step in steps]\n",
    "        return Step.batch(steps)\n",
    "\n",
    "    def step(self, action: np.array) -> Step:\n",
    "        steps = []\n",
    "        for i, (env, single_action) in enumerate(zip(self.envs, action)):\n",
    "            step = env.step(single_action)\n",
    "            if step.done:\n",
    "                self.envs[i] = self.env_init_fn()\n",
    "                new_step = env.reset()\n",
    "                step.next_obs = new_step.next_obs\n",
    "                self.dones += 1\n",
    "            steps.append(Step.from_no_batch_axis(step))\n",
    "\n",
    "        batched_step = Step.batch(steps)\n",
    "        batched_step.done = self.dones >= self.batch_size\n",
    "        return batched_step\n",
    "\n",
    "    def render(self):\n",
    "        for env in self.envs:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer.\n",
    "\n",
    "    Expects the following hyperparameters:\n",
    "        - `epoch`: The current epoch.\n",
    "        - `batch_size`: Number of trajectories to return at each call.\n",
    "        - `min_sample_len`: Minimum length of trajectories to sample.\n",
    "        - `max_sample_len`: Maximum length of trajectories to sample.\n",
    "        - `num_steps_replay_coef`: Sampling coefficient based on trajectory length.\n",
    "        - `success_replay_coef`: Sampling coefficient based on trajectory success.\n",
    "        - `age_replay_coef`: Sampling coefficient based on trajectory age.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, hparams: dict):\n",
    "        self.hparams = hparams\n",
    "        self.trajs = dict()\n",
    "\n",
    "    @property\n",
    "    def flat_traj(self):\n",
    "        return [step for traj in self.trajs for step in traj]\n",
    "\n",
    "    def add(self, traj: Traj):\n",
    "        \"\"\"Add a new trajectory to the buffer.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): the trajectory to add.\n",
    "        \"\"\"\n",
    "        epoch = self.hparams['epoch']\n",
    "        if epoch not in self.trajs:\n",
    "            self.trajs[epoch] = []\n",
    "        self.trajs[epoch] += traj\n",
    "\n",
    "    def sample(self) -> Traj:\n",
    "        \"\"\"Samples a batched trajectory from the buffer stochastically based on:\n",
    "            - the number of steps in the trajectory (num_steps_replay_coef)\n",
    "            - how well the agent did in the trajectory (success_replay_coef)\n",
    "            - how long ago the trajectory was experienced (age_replay_coef)\n",
    "\n",
    "        Returns:\n",
    "            Traj: a trajectory of batched steps experienced.\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = {\n",
    "            epoch: self.hparams['num_steps_replay_coef'] * len(traj) +\n",
    "                   self.hparams['success_replay_coef'] * sum(np.sum(step.reward) for step in traj) +\n",
    "                   self.hparams['age_replay_coef'] * (epoch - self.hparams['epoch'])\n",
    "            for epoch, traj in self.trajs.items()\n",
    "        }\n",
    "        epoch = np.random.choice(list(weights.keys()), p=list(weights.values())/sum(weights.values()))\n",
    "        return self.trajs[epoch]\n",
    "        #epochs = np.random.choice(list(weights.keys()), size=(), replace=True, p=list(weights.values()))\n",
    "        #trajs = [self.trajs[epoch] for epoch in epochs]\n",
    "        #batched_traj = [BatchStep.batch(steps) for steps in zip(*trajs)]\n",
    "        #return batched_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, policy: Callable):\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, step: BatchStep) -> np.ndarray:\n",
    "        \"\"\"Generates an action for a given observation using `self.policy`. \n",
    "        Override if you want to give your policy more information such as\n",
    "        recurrent state or previous reward.\n",
    "\n",
    "        Args:\n",
    "            step (Step): last step output by the environment. This means the agent\n",
    "                should feed `step.next_obs`, not `step.obs` to its policy. If the \n",
    "                environment is multi-agent, then the `reward` attribute has already\n",
    "                updated to reflect the reward for this agent by the driver.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The action to take\n",
    "        \"\"\"\n",
    "        return self.policy(step.next_obs)\n",
    "\n",
    "    def reward(self, traj: Traj) -> float:\n",
    "        \"\"\"Evaluates the cumulative reward for your agent as the sum of \n",
    "        individual rewards experienced. \n",
    "        \n",
    "        If your agent uses intrinsic rewards, be sure to add them in here.\n",
    "        Do not introduce Q-values or predicted rewards here.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "\n",
    "        Returns:\n",
    "            float: Cumulative (sum) reward over the entire sequence.\n",
    "        \"\"\"\n",
    "        return sum(step.reward for step in traj)\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        raise NotImplemented('Method `train` must be implemented by subclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelDriver:\n",
    "    \"\"\"Drives batched turn-based `BatchEnv` environments with multiple agents.\n",
    "    Also supports single-agent environments as a special case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def drive(self, agents: Mapping[str, Agent], env: BatchEnv) -> Mapping[str, Traj]:\n",
    "        \"\"\"Drives a batched environment with multiple agents.\n",
    "        \n",
    "        Args:\n",
    "            agents (Mapping[str, Agent]): A dictionary of agents to drive.\n",
    "            env (BatchEnv): The environment to drive.\n",
    "\n",
    "        Returns:\n",
    "            Mapping[str, Traj]: A dictionary of trajectories for each agent.\n",
    "                Each trajectory is completely disengaged from the other agent's.\n",
    "                (i.e.: the obs, next_obs, action, reward, done attributes are\n",
    "                individual to each agent for each trajectory.)\n",
    "        \"\"\"\n",
    "\n",
    "        names_it = itertools.cycle(agents.keys())\n",
    "        trajs = {agent_name: [] for agent_name in agents}\n",
    "        prev_rewards = {agent_name: 0. for agent_name in agents}\n",
    "\n",
    "        step = env.reset()\n",
    "        while not step.done:\n",
    "            agent_name = next(names_it)\n",
    "            \n",
    "            # `Agent.forward` only looks at `step.next_obs` and `step.reward`\n",
    "            # but I'm assigning defaults just to be safe.\n",
    "            action = agents[agent_name].forward(Step(\n",
    "                obs=step.obs,  # what the previous agent saw before acting\n",
    "                action=step.action,  # what the previous agent did\n",
    "                next_obs=step.next_obs,  # what the current agent sees before acting\n",
    "                reward=prev_rewards[agent_name],  # the reward this agent experienced following its last action\n",
    "                done=step.done,  # whether the environment was done after the previous agent acted\n",
    "                info=step.info  # any extra information the environment might have output\n",
    "            )) \n",
    "\n",
    "            prev_step = step\n",
    "            step = env.step(action)  # `Environment.step` produces a Step with all fields except `step.obs` set\n",
    "            step.obs = prev_step.next_obs  # the current agent's observation is the previous agent's next observation\n",
    "            prev_rewards[agent_name] = step.reward  # the reward for the action the current agent just took\n",
    "            trajs[agent_name].append(step)  # Step completely corresponding to this agent (obs before action, obs after action, action, reward, done, info)\n",
    "        \n",
    "        # remove the agents' first trajectories since they don't carry Markovian information\n",
    "        trajs = {n: traj[1:] for n, traj in trajs.items()}  \n",
    "        return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTrainer:\n",
    "    \"\"\"Trains `BatchEnv` environments and mutliple agents \n",
    "    (with N=1 single-agent supported as a special case).\n",
    "    \n",
    "    Uses following hyperparameters:\n",
    "    - `epoch`: the current epoch. Reads and writes to this variable.\n",
    "    - `epochs`: the number of epochs to train for.\n",
    "    - `min_steps_per_epoch`: the minimum number of steps to train for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams: dict, callbacks: List[Callable]):\n",
    "        self.hparams = hparams\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "    def train(self, \n",
    "        agents: Mapping[str, Agent], \n",
    "        env: BatchEnv,\n",
    "        test_env: BatchEnv = None,\n",
    "        buffers: Mapping[str, ReplayBuffer] = None,\n",
    "        collect_driver: ParallelDriver = None,\n",
    "        test_driver: ParallelDriver = None,\n",
    "        histories: Mapping[str, Mapping[int, Mapping[str, any]]] = None,\n",
    "        ) -> Mapping[int, Mapping[str, any]]:\n",
    "\n",
    "        agent_names = list(agents.keys())\n",
    "        \n",
    "        # initialize defaults\n",
    "        if test_env is None:\n",
    "            test_env = env\n",
    "        if buffers is None:\n",
    "            buffers = dict()\n",
    "        if collect_driver is None:\n",
    "            collect_driver = ParallelDriver()\n",
    "        if test_driver is None:\n",
    "            test_driver = collect_driver\n",
    "        if histories is None:\n",
    "            histories = dict()  # {agent_name: {epoch: {...data}}}\n",
    "\n",
    "        # build uninitialized agent-specific objects\n",
    "        for agent_name in agent_names:\n",
    "            if agent_name not in buffers:\n",
    "                buffers[agent_name] = ReplayBuffer(hparams=self.hparams)\n",
    "            if agent_name not in histories:\n",
    "                histories[agent_name] = dict()\n",
    "\n",
    "        # run training loop\n",
    "        for epoch in range(self.hparams['epoch'], self.hparams['epochs']):\n",
    "            self.hparams['epoch'] = epoch\n",
    "\n",
    "            # collect trajectories\n",
    "            steps = 0\n",
    "            while steps < self.hparams['min_steps_per_epoch']:\n",
    "                collect_trajs = collect_driver.drive(agents, env)\n",
    "                steps += min(len(traj) for _, traj in collect_trajs.items())\n",
    "                for agent_name in agent_names:\n",
    "                    buffers[agent_name].add(collect_trajs[agent_name])\n",
    "\n",
    "            # train\n",
    "            train_trajs = {agent_name: buffers[agent_name].sample() for agent_name in agent_names}\n",
    "            for agent_name in agent_names:\n",
    "                agents[agent_name].train(train_trajs[agent_name])\n",
    "                \n",
    "            # test\n",
    "            test_trajs = test_driver.drive(agents, env)\n",
    "\n",
    "            # record history and run callbacks\n",
    "            for agent_name in agent_names:\n",
    "                histories[agent_name][epoch] = {\n",
    "                    'epoch': epoch,\n",
    "                    'agent': agents[agent_name],\n",
    "                    'all_agents': agents,\n",
    "                    'env': env,\n",
    "                    'test_env': test_env,\n",
    "                    'collect_traj': collect_trajs[agent_name],\n",
    "                    'train_traj': train_trajs[agent_name],\n",
    "                    'test_traj': test_trajs[agent_name],\n",
    "                    'buffer': buffers[agent_name],\n",
    "                }\n",
    "                for callback in self.callbacks:\n",
    "                    callback(histories[agent_name][epoch])\n",
    "\n",
    "        return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback:\n",
    "\n",
    "    def __init__(self, hparams: dict, print_hparam_keys: List[str] = None, print_data_keys: List[str] = None):\n",
    "        if print_hparam_keys is None:\n",
    "            print_hparam_keys = ['epoch']\n",
    "        if print_data_keys is None:\n",
    "            print_data_keys = []\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.print_hparam_keys = print_hparam_keys\n",
    "        self.print_data_keys = print_data_keys\n",
    "\n",
    "    def __call__(self, data: Mapping[str, any]):\n",
    "        for key in self.print_hparam_keys:\n",
    "            print(f'{key}: {self.hparams[key]}', end='\\t')\n",
    "        for key in self.print_data_keys:\n",
    "            print(f'{key}: {data[key]}', end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEvalCallback:\n",
    "\n",
    "    def __init__(self, \n",
    "        eval_on_collect: bool = True, \n",
    "        eval_on_train: bool = False, \n",
    "        eval_on_test: bool = False):\n",
    "\n",
    "        self.eval_on_collect = eval_on_collect\n",
    "        self.eval_on_train = eval_on_train\n",
    "        self.eval_on_test = eval_on_test\n",
    "\n",
    "    def __call__(self, data: Mapping[str, any]):\n",
    "        agent = data['agent']\n",
    "        if not hasattr(agent, 'q_eval'):\n",
    "            return\n",
    "\n",
    "        if self.eval_on_collect:\n",
    "            traj = data['collect_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_collect'] = q_val\n",
    "\n",
    "        if self.eval_on_train:\n",
    "            traj = data['train_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_train'] = q_val\n",
    "\n",
    "        if self.eval_on_test:\n",
    "            traj = data['test_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_test'] = q_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- make the reward optionally an advantage computation over last round\n",
    "- also make a recurrent DQN agent (estimate q function of a sequence of states)\n",
    "- make a simple greedy connect4 agent\n",
    "- make the preprocessor perform a columnwise mean pool before flattening\n",
    "- train the preprocessor on an auxillary objective to estimate the max connected for each length for self and for oponent\n",
    "- add padding='SAME'|'VALID' to conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomAgent(Agent):\n",
    "    \"\"\"Takes a random action on each timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int):\n",
    "        super(RandomAgent, self).__init__(policy=self._policy)\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        choices = np.random.randint(0, self.num_actions, (obs.shape[0],))\n",
    "        onehots = np.eye(self.num_actions)[choices]\n",
    "        return onehots\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "agent = RandomAgent(num_actions=5)\n",
    "obs = Var(np.array([[1, 2], [3, 4]]))\n",
    "step = Step(obs=None, next_obs=obs, action=None, reward=None, done=None, info=None)\n",
    "agent.forward(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RealDQN(Agent):\n",
    "    \"\"\"'Classic' Deep Q-learning agent.\n",
    "    Implements the approach in https://arxiv.org/pdf/1312.5602.pdf.\n",
    "    \n",
    "    NOTE: This agent expects its encoder to output a per-column vector.\n",
    "        I.E.: [B, H, W, C] --encoder--> [B, W, d_enc]\n",
    "\n",
    "    This agent uses the following hyperparameters:\n",
    "        - activation: activation function to use for the hidden layers.\n",
    "        - hidden_size: hidden layer size.\n",
    "        - discount: discount factor.\n",
    "        - optimizer: optimizer to use.\n",
    "        - epsilon_start: initial epsilon value.\n",
    "        - min_epsilon: minimum epsilon value.\n",
    "        - epsilon_decay: epsilon decay rate.\n",
    "        - epoch: current epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder  # [B, H, W, C] -> [B, W, d_enc]\n",
    "        self.neck = Flatten()\n",
    "        self.head = Sequential([\n",
    "            Dense(hparams['hidden_size'], hparams['activation']), \n",
    "            Dense(1, Linear)\n",
    "        ]) # [B, L+|A|] -> [B, 1]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(RealDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        B = obs.shape[0]\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = max(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            indeces = np.random.randint(0, self.num_actions, (B,))\n",
    "            return np.eye(self.num_actions)[indeces]\n",
    "    \n",
    "        # Otherwise take the action with the highest Q-value\n",
    "        q_vals = np.zeros((B, self.num_actions))  # [B, self.num_actions]\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "\n",
    "        for action_index in range(self.num_actions):\n",
    "\n",
    "            # prepare action\n",
    "            action_T = Var(np.repeat(\n",
    "                np.array([action_index])[None, :], \n",
    "                repeats=B, axis=0))  # [B, 1]\n",
    "\n",
    "            # run the network\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [B, W*d_enc+A]\n",
    "            q_T = self.head(cat_T)  # [B, 1]\n",
    "\n",
    "            # store q-values\n",
    "            q_vals[:, action_index] = q_T.val[:, 0]\n",
    "        \n",
    "        # select the action with the highest Q-value\n",
    "        action_indeces = np.argmax(q_vals, axis=1)  # [B]\n",
    "        onehots = np.eye(self.num_actions)[action_indeces]  # [B, self.num_actions]\n",
    "        return onehots\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_T = Var(step.action, trainable=False)  # [B, A]\n",
    "            next_obs_T = Var(step.next_obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_next_T = Var(self.policy(step.next_obs), trainable=False)  # [B, A]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [B, d_enc+A]\n",
    "            Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "            #                       `int`             `Var`\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            next_enc_T = self.neck(self.encoder(next_obs_T))  # [B, W*d_enc]\n",
    "            next_cat_T = Concat([next_enc_T, action_next_T], axis=1)  # [B, d_enc+A]\n",
    "            Qnext_T = self.head(next_cat_T)[:, 0]  # [B]\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above eq; small updates are \n",
    "            # ensured by small SGD lr instead)\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Qnext_T)) - Qnow_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        action_T = Var(action)  # [B, A]\n",
    "        enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "        cat_T = Concat([enc_T, action_T], axis=1)  # [B, W*d_enc+A]\n",
    "        Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "\n",
    "        return Qnow_T.val\n",
    "\n",
    "\n",
    "test_encoder = Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    AxisMaxPooling(1),\n",
    "])  # [B, H, W, 2] -> [B, W, d_enc]\n",
    "\n",
    "test_batch_size = 5\n",
    "test_num_actions = 7\n",
    "\n",
    "test_step = Step(\n",
    "    obs=np.random.rand(test_batch_size, test_num_actions, test_num_actions, 2),\n",
    "    next_obs=np.random.rand(test_batch_size, test_num_actions, test_num_actions, 2),\n",
    "    action=np.random.rand(test_batch_size, test_num_actions),\n",
    "    reward=np.random.rand(test_batch_size),\n",
    "    done=False,\n",
    "    info={}\n",
    ")\n",
    "\n",
    "test_hparams = {\n",
    "    'hidden_size': 256,\n",
    "    'activation': Relu,\n",
    "    'optimizer': SGD(1e-3),\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_decay': 0.9,\n",
    "    'min_epsilon': 0.1,\n",
    "    'discount': 0.99,\n",
    "    'epoch': 0,\n",
    "}\n",
    "\n",
    "test_agent = RealDQN(num_actions=test_num_actions, encoder=test_encoder, hparams=test_hparams)\n",
    "test_agent.forward(test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04276589, 0.0427985 , 0.04293766, 0.04297295, 0.04303788,\n",
       "        0.042447  , 0.04271075],\n",
       "       [0.04247631, 0.04248234, 0.0429712 , 0.04314574, 0.04287555,\n",
       "        0.04263006, 0.04250694],\n",
       "       [0.04241584, 0.042862  , 0.04322539, 0.0432989 , 0.0430614 ,\n",
       "        0.04244829, 0.04246718],\n",
       "       [0.04272365, 0.04252936, 0.04284562, 0.04315823, 0.0428306 ,\n",
       "        0.04248274, 0.04245597],\n",
       "       [0.04255741, 0.04277969, 0.04325346, 0.04290551, 0.04277428,\n",
       "        0.04258953, 0.04236092]])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CategoricalDQN(Agent):\n",
    "    \"\"\"Categorical deep Q-network agent.\n",
    "    I never read the paper for this architecture, so my implementation may\n",
    "    be different from the origonal researchers.\n",
    "    \n",
    "    NOTE: This agent expects its encoder to output a per-column vector.\n",
    "        I.E.: [B, H, W, C] --encoder--> [B, W, d_enc]\n",
    "\n",
    "    This agent uses the following hyperparameters:\n",
    "        - activation: activation function to use for the hidden layers.\n",
    "        - categorical_hidden_size: hidden layer size.\n",
    "        - discount: discount factor.\n",
    "        - optimizer: optimizer to use.\n",
    "        - epsilon_start: initial epsilon value.\n",
    "        - min_epsilon: minimum epsilon value.\n",
    "        - epsilon_decay: epsilon decay rate.\n",
    "        - epoch: current epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder  # [B, H, W, C] -> [B, W, d_enc]\n",
    "        self.neck = Flatten()\n",
    "        self.head = Sequential([\n",
    "            Dense(hparams['categorical_hidden_size'], hparams['activation']), \n",
    "            Dense(1, Linear)\n",
    "        ]) # [B, W, d_enc] -> [B, A, 1]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(CategoricalDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        B = obs.shape[0]\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = max(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            indeces = np.random.randint(0, self.num_actions, (B,))\n",
    "            return np.eye(self.num_actions)[indeces]\n",
    "        \n",
    "        # Otherwise estimate Q-values for all actions\n",
    "        obs_T = Var(obs, trainable=False)  # [B, H, W, C]\n",
    "        enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "        qvals_T = self.head(enc_T)  # [B, W, 1]\n",
    "        return qvals_T[..., 0].val  # [B, W]\n",
    "\n",
    "    \n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_indeces = np.argmax(step.action, axis=1)  # [B]\n",
    "            obs_next_T = Var(step.next_obs, trainable=False)  # [B, H, W, 2]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "            qvals_T = self.head(enc_T)[..., 0]  # [B, W]\n",
    "            Q_now_T = qvals_T[action_indeces]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            enc_next_T = self.encoder(obs_next_T)  # [B, W, d_enc]\n",
    "            qvals_T = self.head(enc_next_T)[..., 0]  # [B, W]\n",
    "            Q_next_T = ReduceMax(qvals_T, axis=1)  # [B]  \n",
    "            # equivalent to: Q_next_T = ReduceMax(qvals_T, axis=1)\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above; small updates are handled in the SGD step)\n",
    "            # but only update the targets that were actually selected for action at `step_now`.\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Q_next_T)) - Q_now_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        action_indeces = np.argmax(action, axis=1)  # [B]\n",
    "\n",
    "        enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "        qvals_T = self.head(enc_T)[..., 0]  # [B, W]\n",
    "\n",
    "        return qvals_T.val[action_indeces]  # [B]\n",
    "\n",
    "test_hparams['categorical_hidden_size'] = 32\n",
    "test_hparams['epoch'] = 100\n",
    "test_agent = CategoricalDQN(num_actions=test_num_actions, encoder=test_encoder, hparams=test_hparams)\n",
    "test_agent.forward(test_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
       "Turn: 1\n",
       "Winner: 0"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Board:\n",
    "    \"\"\"Drafted by copilot with minor human edits\"\"\"\n",
    "\n",
    "    def __init__(self, size=7, win_length=4):\n",
    "        self.size = size\n",
    "        self.win_length = win_length\n",
    "        self.board = np.zeros((size, size))\n",
    "        self.turn = 1\n",
    "        self.winner = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.board}\\nTurn: {self.turn}\\nWinner: {self.winner}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.board == other.board\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.board.tostring())\n",
    "\n",
    "    def is_full(self):\n",
    "        return np.count_nonzero(self.board) == self.size**2\n",
    "\n",
    "    def is_empty(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return 0 <= col < self.size and self.is_empty(col)\n",
    "\n",
    "    def make_move(self, col):\n",
    "        if self.is_valid_move(col):\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-1]\n",
    "            self.board[highest_row, col] = self.turn\n",
    "            self.turn *= -1\n",
    "\n",
    "    def undo_move(self, col):\n",
    "        if self.is_valid_move(col) and self.board[0, col] != 0:\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-2]\n",
    "            self.board[highest_row, col] = 0\n",
    "            self.turn *= -1\n",
    "\n",
    "    def check_win(self) -> int:\n",
    "        for turn in [-1, 1]:\n",
    "            if self.num_connected(self.win_length, turn) > 0:\n",
    "                self.winner = turn\n",
    "                return True\n",
    "        return self.winner\n",
    "\n",
    "    def num_connected(self, length, turn):\n",
    "        num_connected = 0\n",
    "        # Check horizontal\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size-length+1):\n",
    "                if np.all(self.board[row, col:col+length] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check vertical\n",
    "        for col in range(self.size):\n",
    "            for row in range(self.size-length+1):\n",
    "                if np.all(self.board[row:row+length, col] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(self.size-length+1):\n",
    "                if all(self.board[row+i, col+i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        # Check anti-diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(length-1, self.size):\n",
    "                if all(self.board[row+i, col-i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        return num_connected\n",
    "\n",
    "board = Board()\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [0.26651316 0.40456201 0.93722749 0.10353108 0.59788683 0.198233\n",
      " 0.08187043 0.88691788 0.995581   0.58860407]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.67586735 0.84359196 0.00445218 0.95061257 0.40497481 0.81328438\n",
      " 0.21324059 0.7053486  0.27841892 0.54252497]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.33123088 0.91273516 0.37226563 0.13917417 0.67729837 0.53103511\n",
      " 0.73832913 0.12334317 0.84305861 0.38712597]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  0.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.5606563  0.93662664 0.89007398 0.55773164 0.61344032 0.01134802\n",
      " 0.36549664 0.6429246  0.65665223 0.50132056]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  0.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.30353792 0.25825002 0.41651714 0.55948954 0.41496334 0.17221386\n",
      " 0.08110883 0.67846167 0.04458004 0.14116057]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599453\n",
      "\n",
      "Action: [0.83005075 0.63880398 0.82677217 0.52876858 0.9311223  0.79690796\n",
      " 0.47412693 0.99178293 0.88837708 0.22573876]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599453\n",
      "\n",
      "Action: [0.36208271 0.94008806 0.45052698 0.05110692 0.05958159 0.44120563\n",
      " 0.99468446 0.87291134 0.84170146 0.31251222]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.791759469228055\n",
      "\n",
      "Action: [5.94011639e-04 5.32421932e-01 3.92699689e-01 9.00409910e-01\n",
      " 7.25256262e-01 6.71450042e-01 4.50395055e-01 8.72433228e-01\n",
      " 9.92461320e-01 5.90396624e-03]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.0986122886681096\n",
      "\n",
      "Action: [0.46900677 0.8842072  0.98305335 0.66321587 0.359152   0.35723859\n",
      " 0.13632385 0.59240921 0.94659361 0.30816256]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -4.440892098500626e-16\n",
      "\n",
      "Action: [0.91983197 0.46136454 0.47614321 0.46035415 0.73908812 0.19901531\n",
      " 0.82814689 0.41566278 0.87971146 0.29618525]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 2.220446049250313e-16\n",
      "\n",
      "Action: [0.77237654 0.3717741  0.15259575 0.26830265 0.54901987 0.75006254\n",
      " 0.59609073 0.95198713 0.20093428 0.74359963]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599452\n",
      "\n",
      "Action: [0.70194402 0.4569423  0.03255271 0.83509315 0.43991826 0.22147482\n",
      " 0.14577372 0.52735212 0.95501797 0.89934952]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.3862943611198906\n",
      "\n",
      "Action: [0.25468261 0.80764088 0.78225604 0.05243098 0.79747034 0.634745\n",
      " 0.03980254 0.96939948 0.28976064 0.47720486]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599452\n",
      "\n",
      "Action: [0.68982959 0.73562601 0.36381009 0.82245147 0.02572779 0.66010355\n",
      " 0.35480563 0.33502073 0.36159837 0.6044578 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0. -1.  0. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.98086271 0.85145341 0.33438468 0.92043865 0.4305783  0.53107904\n",
      " 0.23887241 0.87396942 0.62256605 0.87696363]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1. -1.  0. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.14380149 0.02174159 0.86106348 0.50102279 0.05161386 0.80967621\n",
      " 0.86230611 0.34485628 0.26972301 0.54791001]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1. -1.  0. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.0986122886681096\n",
      "\n",
      "Action: [0.16320332 0.80956941 0.41217451 0.14110235 0.0747905  0.55645967\n",
      " 0.56730505 0.33359628 0.24865503 0.18302499]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1. -1.  0. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.0986122886681096\n",
      "\n",
      "Action: [0.67237847 0.17697902 0.89807648 0.97794056 0.53834615 0.73785653\n",
      " 0.29888142 0.5879753  0.08684507 0.92599934]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1. -1.  0. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.0986122886681091\n",
      "\n",
      "Action: [0.60533123 0.12711501 0.7543588  0.31857999 0.62562772 0.70807868\n",
      " 0.73032129 0.18050901 0.21012043 0.50049649]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.2876820724517817\n",
      "\n",
      "Action: [0.83605435 0.17282345 0.25892433 0.92250151 0.94174758 0.96161368\n",
      " 0.22352621 0.41470851 0.96908161 0.3995625 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -0.2876820724517799\n",
      "\n",
      "Action: [0.21065514 0.98928562 0.23959949 0.02118721 0.71802438 0.73681436\n",
      " 0.21380798 0.25161801 0.91027828 0.86122248]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.098612288668111\n",
      "\n",
      "Action: [0.46759122 0.87432302 0.21347282 0.43835856 0.50813124 0.86708145\n",
      " 0.90903026 0.25413128 0.54646476 0.90797299]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599454\n",
      "\n",
      "Action: [0.9368695  0.23100755 0.05761421 0.09175167 0.08900628 0.405838\n",
      " 0.81452626 0.00174785 0.24290128 0.60811879]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  0. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599436\n",
      "\n",
      "Action: [0.94203415 0.11182187 0.92759173 0.82692995 0.62116425 0.81016289\n",
      " 0.14557448 0.8690017  0.94124761 0.45865275]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  1.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  0. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -2.0794415416798344\n",
      "\n",
      "Action: [0.72329097 0.15472237 0.15177423 0.05534352 0.56770936 0.33437534\n",
      " 0.9576106  0.97396235 0.34605754 0.18395904]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  1.  0.  0.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  0. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.7917594692280563\n",
      "\n",
      "Action: [0.45402574 0.27460054 0.48188826 0.94885521 0.42435598 0.732743\n",
      " 0.77177746 0.94139546 0.60876304 0.29640685]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  1.  0. -1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  0. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.386294361119889\n",
      "\n",
      "Action: [0.15145284 0.52030585 0.98222914 0.52846214 0.76120138 0.40055025\n",
      " 0.17072353 0.86126454 0.72825987 0.76452486]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  1.  0. -1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.098612288668111\n",
      "\n",
      "Action: [0.42880478 0.87951762 0.69025069 0.80819581 0.6147945  0.29645245\n",
      " 0.68212561 0.2579242  0.84847735 0.3130194 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  1.  0. -1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -3.5835189384561126\n",
      "\n",
      "Action: [0.0181855  0.78109106 0.0498374  0.49137416 0.16803451 0.48162615\n",
      " 0.80076072 0.49747453 0.50631899 0.15179684]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  1.  0. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.3862943611198908\n",
      "\n",
      "Action: [0.67838947 0.64519094 0.20834918 0.24804251 0.19147189 0.86737249\n",
      " 0.52605069 0.53451874 0.49284769 0.91989808]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  1.  0. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.3862943611198872\n",
      "\n",
      "Action: [0.4214856  0.51028627 0.41397072 0.96211357 0.73003609 0.75485872\n",
      " 0.83513567 0.23322757 0.91155001 0.31371553]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  1.  0. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599472\n",
      "\n",
      "Action: [0.56884026 0.39344257 0.44058788 0.97817678 0.80720566 0.36477218\n",
      " 0.89980516 0.29631213 0.08504291 0.94645034]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  1.  0. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.08701795 0.19100152 0.69261373 0.97967531 0.02305022 0.79651385\n",
      " 0.94811693 0.74539371 0.98159144 0.19305832]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1.  0. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.3862943611198908\n",
      "\n",
      "Action: [0.62482159 0.38442978 0.92113479 0.90911997 0.38014399 0.69512383\n",
      " 0.24519697 0.62991439 0.27027465 0.23777099]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.7917594692280545\n",
      "\n",
      "Action: [0.85259823 0.46101791 0.74758605 0.44902135 0.1173047  0.58516051\n",
      " 0.73900575 0.60399408 0.90569578 0.46286858]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599472\n",
      "\n",
      "Action: [0.44969024 0.70058625 0.87447554 0.38168414 0.60927056 0.27339567\n",
      " 0.09143648 0.75883539 0.53061255 0.01234595]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.28768207245178345\n",
      "\n",
      "Action: [0.19803861 0.76211224 0.9285286  0.43072302 0.5523486  0.46764074\n",
      " 0.05799563 0.33053523 0.71791372 0.48404472]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -2.0794415416798344\n",
      "\n",
      "Action: [0.65920239 0.36124798 0.86923717 0.87783166 0.0383439  0.11732099\n",
      " 0.05029853 0.89148016 0.44077536 0.184235  ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599472\n",
      "\n",
      "Action: [0.33266154 0.75709606 0.44379828 0.59576342 0.32121557 0.60213158\n",
      " 0.57940888 0.06757114 0.29506096 0.5026688 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599472\n",
      "\n",
      "Action: [0.66950521 0.79145895 0.33684196 0.43125288 0.3235663  0.37097134\n",
      " 0.17715404 0.87962261 0.98683411 0.3398789 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -3.552713678800501e-15\n",
      "\n",
      "Action: [0.20622033 0.35456441 0.78549185 0.92779522 0.036693   0.28670378\n",
      " 0.91309046 0.91536271 0.91021769 0.51823624]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 3.552713678800501e-15\n",
      "\n",
      "Action: [0.64426648 0.58869146 0.68949408 0.12180432 0.36634647 0.29049226\n",
      " 0.28987814 0.09247163 0.91107977 0.26719967]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [5.37192359e-04 3.96549912e-02 3.25857186e-01 7.06314751e-01\n",
      " 6.70830045e-01 4.61751445e-01 8.06681935e-01 8.99265883e-01\n",
      " 1.02189992e-02 7.83284258e-01]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1.  0.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -7.105427357601002e-15\n",
      "\n",
      "Action: [0.81139212 0.51625604 0.88333483 0.73537865 0.95952691 0.92250444\n",
      " 0.85804189 0.30524855 0.67510831 0.55553313]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  0.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599472\n",
      "\n",
      "Action: [0.49537965 0.58876345 0.45427001 0.00576639 0.77185881 0.1024495\n",
      " 0.93443281 0.10568887 0.20628288 0.10440842]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.7917594692280616\n",
      "\n",
      "Action: [0.48876922 0.36868224 0.6186209  0.90203946 0.23690773 0.33514863\n",
      " 0.46299777 0.90484709 0.66839372 0.67593586]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.7917594692280545\n",
      "\n",
      "Action: [0.11606127 0.20908864 0.95462915 0.66923945 0.4748003  0.18213798\n",
      " 0.28016297 0.7268418  0.79875095 0.63133403]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1.  0.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599401\n",
      "\n",
      "Action: [0.38798025 0.39270261 0.44454744 0.82528225 0.42097464 0.11607955\n",
      " 0.30040103 0.88116439 0.07318556 0.97883948]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 2.4849066497880017\n",
      "\n",
      "Action: [0.1828974  0.20877411 0.7079473  0.23719461 0.23040921 0.13419837\n",
      " 0.55690513 0.2936053  0.41745093 0.53861324]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -2.0794415416798344\n",
      "\n",
      "Action: [0.94760554 0.50341684 0.86926658 0.06334059 0.22288393 0.72620923\n",
      " 0.08417954 0.08760027 0.11586797 0.15279569]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.87232429 0.74101709 0.03744946 0.83139439 0.05763894 0.66511478\n",
      " 0.49326495 0.04504379 0.85999369 0.04172369]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599472\n",
      "\n",
      "Action: [0.79606926 0.55572186 0.4027139  0.97466803 0.22272464 0.36951889\n",
      " 0.377421   0.83909618 0.26005482 0.90087631]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.7917594692280545\n",
      "\n",
      "Action: [0.54999514 0.15507303 0.97073073 0.6898259  0.78168326 0.25252818\n",
      " 0.15892734 0.88433386 0.44758586 0.5513882 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 3.1780538303479418\n",
      "\n",
      "Action: [0.34439037 0.12354117 0.26861603 0.55182993 0.13905932 0.21537172\n",
      " 0.08268421 0.99392922 0.20138901 0.39331253]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.791759469228058\n",
      "\n",
      "Action: [0.48702164 0.00670594 0.35887725 0.94989006 0.82743135 0.79047645\n",
      " 0.04621271 0.90690868 0.37644766 0.74251956]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 3.552713678800501e-15\n",
      "\n",
      "Action: [0.03930553 0.61813333 0.79949049 0.09911056 0.65300513 0.5157451\n",
      " 0.26544582 0.26253327 0.83174516 0.01525021]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.7917594692280616\n",
      "\n",
      "Action: [0.83953105 0.62789784 0.11192897 0.23894446 0.26963628 0.56764989\n",
      " 0.37653915 0.6195504  0.45047651 0.43140029]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.7917594692280545\n",
      "\n",
      "Action: [0.09629074 0.98131955 0.94238352 0.42297505 0.23241668 0.66014535\n",
      " 0.32043951 0.51918878 0.00293347 0.03072035]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.3862943611198943\n",
      "\n",
      "Action: [0.37347172 0.54380488 0.27346048 0.86229512 0.42205765 0.66676812\n",
      " 0.89410419 0.22941227 0.99905113 0.17392156]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.89067051 0.6832834  0.93832409 0.60696283 0.6756683  0.81744982\n",
      " 0.65888761 0.33292991 0.11452514 0.05732106]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.57620021 0.59871279 0.81051717 0.49289901 0.28231424 0.12890853\n",
      " 0.57271497 0.12095713 0.72512632 0.26593991]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  0.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.21878286 0.5806996  0.09155486 0.78893084 0.85605101 0.90310225\n",
      " 0.8951019  0.8161266  0.58296431 0.75265896]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 3.178053830347949\n",
      "\n",
      "Action: [0.43043556 0.74043578 0.41665996 0.34414566 0.80040229 0.46916583\n",
      " 0.48626359 0.80203088 0.95906167 0.37431746]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.33267783 0.25355407 0.95249812 0.41874804 0.39085632 0.40735371\n",
      " 0.74776347 0.32801494 0.26302081 0.41376809]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.36411899 0.40420347 0.85486465 0.24547698 0.20556482 0.67799328\n",
      " 0.06979018 0.36689751 0.29159124 0.72520465]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.17390759 0.82391637 0.41598415 0.09538038 0.38987367 0.16153822\n",
      " 0.39302995 0.4409485  0.60831687 0.32722663]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 0. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -2.4849066497880017\n",
      "\n",
      "Action: [0.95917739 0.26264067 0.2409487  0.70942314 0.37012686 0.23058192\n",
      " 0.29468006 0.787782   0.27169068 0.30352636]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.0986122886681073\n",
      "\n",
      "Action: [0.22157786 0.12681963 0.46727366 0.53313048 0.99348727 0.1721489\n",
      " 0.64313116 0.2025099  0.20973881 0.04641137]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1. -1.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 3.4657359027997217\n",
      "\n",
      "Action: [0.18370015 0.9557406  0.47717849 0.02099686 0.94901468 0.25484147\n",
      " 0.52341448 0.31272897 0.63326429 0.53565444]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1. -1.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.98082925301172\n",
      "\n",
      "Action: [0.44048203 0.72823437 0.59777912 0.54638785 0.97306043 0.09784165\n",
      " 0.423263   0.6881032  0.02241733 0.52671744]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  0.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1. -1.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1. -1.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599472\n",
      "\n",
      "Action: [0.56954644 0.61225732 0.46953745 0.38833064 0.94401754 0.06002175\n",
      " 0.75626945 0.49202504 0.13470516 0.89800268]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  1.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1. -1.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1. -1.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -3.178053830347949\n",
      "\n",
      "Action: [0.85194252 0.48017777 0.75952232 0.70747928 0.82968766 0.57067286\n",
      " 0.47545311 0.94542979 0.51081257 0.04265789]\n",
      "[[ 0.  0. -1.  0.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  1.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1. -1.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1. -1.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599472\n",
      "\n",
      "Action: [0.01623109 0.89844469 0.82022788 0.47006739 0.78305755 0.77562396\n",
      " 0.79599361 0.7214671  0.00794562 0.18450335]\n",
      "[[ 0.  1. -1.  0.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  1.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1. -1.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1. -1.  0. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.0986122886681073\n",
      "\n",
      "Action: [0.81643066 0.38347983 0.37713597 0.00804344 0.52060375 0.9611871\n",
      " 0.26491231 0.8901777  0.33362282 0.36762005]\n",
      "[[ 0.  1. -1.  0.  0.  0.  0. -1.  1.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1. -1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1. -1.  1.  1.  0.  0.  0.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1.  0.  0.  0. -1.  1.  0.]\n",
      " [-1. -1. -1.  1.  0.  0.  1.  1.  1.  0.]\n",
      " [-1.  1. -1. -1.  1.  0.  1.  1. -1.  0.]\n",
      " [ 1.  1.  1. -1. -1.  0. -1.  1. -1.  0.]\n",
      " [ 1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: -1\n",
      "Reward: 23.475423254313654\n",
      "\n",
      "Winner: -1\n"
     ]
    }
   ],
   "source": [
    "class BoardEnv:\n",
    "\n",
    "    def __init__(self, board_size=7, win_length=4, reward_mode: str = 'sparse'):\n",
    "        \"\"\"RL environment for Connect 4. \n",
    "\n",
    "        Args:\n",
    "            board_size (int, optional): The size of the board. Defaults to 7.\n",
    "            win_length (int, optional): The minimum connected length to win. Defaults to 4.\n",
    "            reward_mode (str, optional): One of 'sparse', 'dense_stateless', 'dense_advantage'. \n",
    "                - For 'sparse', the reward is 1 if the player has attained a connect `win_length`,\n",
    "                    and is 0 otherwise.\n",
    "                - For 'dense_stateless', the reward increases linearly with the number of N-in-a-row's\n",
    "                    for all values of N from 0 to board_size weighted logarithmically by N.\n",
    "                - For 'dense_advantage', the reward is determined by the difference between the\n",
    "                    previous and current dense reward for each player individually.\n",
    "                `reward_mode` defaults to 'sparse'.\n",
    "        \"\"\"\n",
    "        self.board_size = board_size\n",
    "        self.win_length = win_length\n",
    "        self.reward_mode = reward_mode\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        self.board = Board(self.board_size, self.win_length)\n",
    "\n",
    "        if self.reward_mode == 'dense_advantage':\n",
    "            self.prev_dense_reward = [0., 0.]\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=np.zeros_like(self._make_obs()),\n",
    "            action=np.zeros(self.board_size),\n",
    "            next_obs=self._make_obs(), \n",
    "            reward=np.array(0.), \n",
    "            done=False, \n",
    "            info=dict()\n",
    "        )\n",
    "\n",
    "    def step(self, action: np.ndarray) -> NoBatchStep:\n",
    "        \"\"\"Apply agent X's action to the board and \n",
    "        returns the next agent's timestep.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): array shaped (board_size,). The arg max\n",
    "                action index is the column where next piece is placed.\n",
    "\n",
    "        Returns:\n",
    "            tuple: NoBatchStep with values:\n",
    "                obs (np.ndarray[H, W, 2]): None\n",
    "                next_obs (np.ndarray[H, W, 2]): the next board state with\n",
    "                    self's entered squares represented in channel 0 and\n",
    "                    opponent's squares represented in channel 1\n",
    "                action (np.ndarray[board_size]): None\n",
    "                reward (float): the reward for the agent\n",
    "                    If sparse_reward is True, then reward is -1, 0, or +1.\n",
    "                    If sparse_reward is False, then reward is:\n",
    "                        ego_dense_reward - opponent_dense_reward.\n",
    "                done (bool): whether the game is over\n",
    "                info (dict): extra information\n",
    "        \"\"\"\n",
    "        # Apply action\n",
    "        action_index = np.argmax(action)  # []\n",
    "        self.board.make_move(action_index)  # this flips `board.turn`\n",
    "        # Temporarily unflip `board.turn`\n",
    "        self.board.turn *= -1\n",
    "\n",
    "        # Make egocentric observation\n",
    "        obs = self._make_obs()\n",
    "\n",
    "        # Compute reward\n",
    "        # This is the lazy way to do it, but it's fast enough\n",
    "\n",
    "        # Sparse reward\n",
    "        winner = self.board.check_win()\n",
    "        sparse_reward = self.board.turn * winner\n",
    "\n",
    "        # Dense reward\n",
    "        def dense_reward_for_turn(board, turn):\n",
    "            r = 0\n",
    "            for length in range(2, self.board_size):\n",
    "                r += math.log(length) * board.num_connected(length, turn)\n",
    "            return r\n",
    "        ego_dense_reward = dense_reward_for_turn(self.board, self.board.turn)\n",
    "        opponent_dense_reward = dense_reward_for_turn(self.board, -self.board.turn)\n",
    "        dense_reward = ego_dense_reward - opponent_dense_reward  \n",
    "\n",
    "        if self.reward_mode == 'sparse':\n",
    "            reward = sparse_reward\n",
    "        elif self.reward_mode == 'dense_stateless':\n",
    "            reward = dense_reward\n",
    "        elif self.reward_mode == 'dense_advantage':\n",
    "            turn_index = (self.board.turn+1)//2\n",
    "            reward = dense_reward - self.prev_dense_reward[turn_index]\n",
    "            self.prev_dense_reward[turn_index] = dense_reward\n",
    "        else:\n",
    "            raise ValueError(f'Invalid reward_mode: {self.reward_mode}')\n",
    "\n",
    "        # Evaluate whether game is over\n",
    "        winner = self.board.check_win()\n",
    "        done = winner != 0\n",
    "\n",
    "        # Record debugging info\n",
    "        info = dict()\n",
    "\n",
    "        # Revert temporary flip on `board.turn`\n",
    "        self.board.turn *= -1\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=np.zeros_like(obs),\n",
    "            next_obs=obs, \n",
    "            action=action,\n",
    "            reward=np.array(reward), \n",
    "            done=done, \n",
    "            info=info\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "    def _make_obs(self) -> np.ndarray:\n",
    "        \"\"\"Only show ego values on first channel and opponent values on second channel\"\"\"\n",
    "        obs = np.stack([\n",
    "            self.board.turn * self.board.board, \n",
    "            -self.board.turn * self.board.board\n",
    "            ], axis=-1)  # [board_size, board_size, 2]\n",
    "        obs[obs<0] = 0  # rectify negative values\n",
    "        return obs\n",
    "\n",
    "board_size = 10\n",
    "env = BoardEnv(board_size=board_size, win_length=5, reward_mode='dense_advantage')\n",
    "\n",
    "step = env.reset()\n",
    "while not step.done:\n",
    "    action = np.random.uniform(0, 1, (board_size,))\n",
    "    step = env.step(action)\n",
    "\n",
    "    print(f'Action: {action}')\n",
    "    env.render()\n",
    "    print(f'Reward: {step.reward}\\n')\n",
    "\n",
    "print(f'Winner: {env.board.winner}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwiredConnect4Agent(Agent):\n",
    "\n",
    "    def __init__(self, board_size: int, hparams: dict):\n",
    "        self.board_size = board_size\n",
    "        self.hparams = hparams\n",
    "        super(HardwiredConnect4Agent, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        B = obs.shape[0]\n",
    "        action = np.zeros(B, dtype=np.int32)\n",
    "        for b in range(B):\n",
    "            o = obs[b]\n",
    "            ## TODO: make a greedy agent\n",
    "            action[b] = random.randint(0, self.board_size-1)\n",
    "        return action\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Concat' object has no attribute 'orig_axis_dims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_167400/121239757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         reward_mode='dense_stateless'))\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m trainer = ParallelTrainer(\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     callbacks=[\n",
      "\u001b[0;32m/tmp/ipykernel_167400/1584774949.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, agents, env, test_env, buffers, collect_driver, test_driver, histories)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtrain_trajs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0magent_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_names\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magent_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_trajs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/1294350732.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, traj)\u001b[0m\n\u001b[1;32m    103\u001b[0m             loss_T = ReduceSum(((r_T+discount_T*StopGrad(Qnext_T)) - Qnow_T)**2,\n\u001b[1;32m    104\u001b[0m                 axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mq_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0;34m'STATIC execution mode not enabled'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, t_out, output_grad)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             input_grads = t_out.reverse_grad(\n\u001b[0m\u001b[1;32m    666\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 output=t_out.val, top_grad=output_grad)\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36mreverse_grad\u001b[0;34m(self, inputs, output, top_grad)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mdY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mdXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_axis_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdXs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Concat' object has no attribute 'orig_axis_dims'"
     ]
    }
   ],
   "source": [
    "hparams = dict(\n",
    "    hidden_size=256,                # hidden layer size for RealDQN\n",
    "    categorical_hidden_size=32,     # hidden layer size for CategoricalDQN\n",
    "    activation=Relu,                # activation function for networks\n",
    "    optimizer=SGD(1e-3),            # optimizer for networks\n",
    "    epsilon_start=1.0,              # Starting value for epsilon\n",
    "    epsilon_decay=0.95,             # Decay rate for epsilon per epoch\n",
    "    min_epsilon=0.01,               # Final value for epsilon\n",
    "    discount=0.99,                  # Discount factor\n",
    "    epoch=0,                        # Current epoch\n",
    "    epochs=2,                       # Number of training epochs\n",
    "    board_size=8,                   # Board size\n",
    "    train_win_length=4,             # Number of pieces in a row needed to win in training\n",
    "    test_win_length=6,              # Number of pieces in a row needed to win in testing\n",
    "    min_steps_per_epoch=10,         # Minimum number of steps per epoch\n",
    "    batch_size=32,                  # Number of samples per training batch\n",
    "    num_steps_replay_coef=0.5,      # How much to upweight longer episodes\n",
    "    success_replay_coef=0.5,        # How much to upweight successful experience\n",
    "    age_replay_coef=0.5,            # How much to downweight older trajectories\n",
    ")\n",
    "\n",
    "encoder = Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    AxisMaxPooling(1),\n",
    "])  # [B, H, W, 2] -> [B, W, d_enc]\n",
    "agent = RealDQN(hparams['board_size'], encoder, hparams)\n",
    "\n",
    "self_play_agents = dict(\n",
    "    Bob=agent,\n",
    "    Alice=agent,\n",
    ")\n",
    "\n",
    "train_env = ParallelEnv(\n",
    "    hparams['batch_size'], \n",
    "    lambda: BoardEnv(\n",
    "        board_size=hparams['board_size'],\n",
    "        win_length=hparams['train_win_length'], \n",
    "        reward_mode='dense_stateless'))\n",
    "\n",
    "test_env = ParallelEnv(\n",
    "    hparams['batch_size'], \n",
    "    lambda: BoardEnv(\n",
    "        board_size=hparams['board_size'],\n",
    "        win_length=hparams['test_win_length'], \n",
    "        reward_mode='dense_stateless'))\n",
    "\n",
    "trainer = ParallelTrainer(\n",
    "    hparams=hparams,\n",
    "    callbacks=[\n",
    "        PrintCallback(hparams=hparams, print_hparam_keys=['epoch'], print_data_keys=['reward']),\n",
    "        QEvalCallback(eval_on_collect=True, eval_on_train=True, eval_on_test=True)\n",
    "    ]).train(self_play_agents, train_env, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_167400/2212580671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_167400/3369367143.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X_T)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_167400/3251554517.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_T)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_T\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mX_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/3369367143.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X_T)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_167400/1288851348.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_T)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# pad height\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             X_T = Concat([\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mX_T\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ts, axis)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_axis_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mEXECUTION_MODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mExecutionMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167400/993919293.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mEXECUTION_MODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mExecutionMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'val'"
     ]
    }
   ],
   "source": [
    "test_encoder(test_step.next_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_pool = dict()\n",
    "agent_pool = dict()\n",
    "env_pool = dict()\n",
    "\n",
    "Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    AxisMaxPooling(1),\n",
    "])\n",
    "\n",
    "def new_encoder() -> Sequential:\n",
    "    layers = []\n",
    "    print('\\n')\n",
    "    num_layers = int(input('Enter number of conv layers (int): ').strip())\n",
    "    for _ in range(num_layers):\n",
    "        filters = eval(input('Enter number of filters (int): ').strip())\n",
    "        kernel_size = eval(input('Enter kernel size (int|2-tuple, odd): ').strip())\n",
    "        stride = eval(input('Enter stride (int|2-tuple): ').strip())\n",
    "        activation = eval(input('Enter activation function ' \n",
    "                                '(`Relu`, `Sigm`, `Tanh`, `Linear`): ').strip())\n",
    "        layers.append(Conv2D(filters, kernel_size, stride, 'same', activation))\n",
    "    layers.append(AxisMaxPooling(1))\n",
    "    encoder = Sequential(layers)\n",
    "\n",
    "    encoder_name = input('Enter encoder name: ').strip()\n",
    "    encoder_pool[encoder_name] = encoder\n",
    "    return encoder\n",
    "\n",
    "def get_encoder() -> Sequential:\n",
    "    print('\\n')\n",
    "    print('All encoders:')\n",
    "    for name, encoder in encoder_pool.items():\n",
    "        print(f'{name}: {encoder}')\n",
    "    print('\\n')\n",
    "    encoder_name = input('Enter encoder name (or enter \"new\"): ').strip().lower()\n",
    "    if encoder_name == 'new':\n",
    "        encoder = new_encoder()\n",
    "    else:\n",
    "        encoder = encoder_pool[encoder_name]\n",
    "    return encoder\n",
    "\n",
    "def new_agent() -> Tuple[str, Agent]:\n",
    "    print('\\n')\n",
    "    agent_type = input('Enter agent type (Random, RealDQN, or CategoricalDQN): ').strip().lower()\n",
    "    if agent_type == 'Random':\n",
    "        num_actions = eval(input('Enter num_actions (int): ').strip())\n",
    "        agent = RandomAgent(num_actions=num_actions)\n",
    "    elif agent_type == 'RealDQN':\n",
    "        num_actions = eval(input('Enter num_actions (int): ').strip())\n",
    "        _, encoder = get_encoder()\n",
    "        agent = RealDQN(num_actions=num_actions, encoder=encoder, hparams=hparams)\n",
    "    elif agent_type == 'CategoricalDQN':\n",
    "        num_actions = eval(input('Enter num_actions (int): ').strip())\n",
    "        _, encoder = get_encoder()\n",
    "        agent = CategoricalDQN(num_actions=num_actions, encoder=encoder, hparams=hparams)\n",
    "\n",
    "    agent_name = input('Please name this agent: ').strip().lower()\n",
    "    agent_pool[agent_name] = agent\n",
    "    return agent_name, agent\n",
    "\n",
    "def get_agent() -> Tuple[str, Agent]:\n",
    "    print('\\n')\n",
    "    print('All agents:')\n",
    "    for name, agent in agent_pool.items():\n",
    "        print(f'{name}: {agent} (made for board size = {agent.num_actions})')\n",
    "    print('\\n')\n",
    "    agent_name = input('Enter agent name (or enter \"new\"): ').strip().lower()\n",
    "    if agent_name == 'new':\n",
    "        agent_name, agent = new_agent()\n",
    "    else:\n",
    "        agent = agent_pool[agent_name]\n",
    "    return agent_name, agent\n",
    "\n",
    "def new_env() -> Tuple[str, ParallelEnv]:\n",
    "    print('\\n')\n",
    "    \n",
    "    board_size = int(input('Enter board size (int): ').strip())\n",
    "    win_length = int(input('Enter train win length (int): ').strip())\n",
    "    reward_mode = input('Enter reward mode (`sparse`, `dense_stateless`, or `dense_advantage`): ').strip().lower()\n",
    "    batch_size = int(input('Enter number of batches (int): ').strip())\n",
    "    \n",
    "    psuedo_env = dict(\n",
    "        board_size=board_size, \n",
    "        win_length=win_length, \n",
    "        reward_mode=reward_mode,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    env_name = input('Please name this environment: ').strip().lower()\n",
    "    env_pool[env_name] = psuedo_env\n",
    "    return env_name, psuedo_env\n",
    "\n",
    "def get_env() -> Tuple[str, ParallelEnv]:\n",
    "    print('\\n')\n",
    "    print('All environments:')\n",
    "    for name, env in env_pool.items():\n",
    "        print(f'{name}: {env}')  # list psuedo_env attributes\n",
    "    print('\\n')\n",
    "    env_name = input('Enter environment name (or enter \"new\"): ').strip().lower()\n",
    "    if env_name == 'new':\n",
    "        env_name, env = new_env()\n",
    "    else:\n",
    "        env = env_pool[env_name]\n",
    "    return env_name, env\n",
    "\n",
    "def train():\n",
    "\n",
    "    train_env_name, train_env_dict = get_env()\n",
    "    test_env_name, test_env_dict = get_env()\n",
    "\n",
    "    if train_env_dict['board_size'] != test_env_dict['board_size']:\n",
    "        raise ValueError('Train and test environments must have the same board size.')\n",
    "\n",
    "    # convert psuedo env's to full parallel env\n",
    "    train_env = ParallelEnv(\n",
    "        batch_size=train_env_dict['batch_size'], \n",
    "        env_init_fn=lambda : BoardEnv(\n",
    "            board=train_env_dict['board_size'],\n",
    "            win_length=train_env_dict['win_length'],\n",
    "            reward_mode=train_env_dict['reward_mode'],\n",
    "        ))\n",
    "    test_env = ParallelEnv(\n",
    "        batch_size=test_env_dict['batch_size'], \n",
    "        env_init_fn=lambda : BoardEnv(\n",
    "            board=test_env_dict['board_size'],\n",
    "            win_length=test_env_dict['win_length'],\n",
    "            reward_mode=test_env_dict['reward_mode'],\n",
    "        ))\n",
    "\n",
    "    agent1_name, agent1 = get_agent()\n",
    "    agent2_name, agent2 = get_agent()\n",
    "\n",
    "    if agent1.num_actions != agent2.num_actions:\n",
    "        raise ValueError('Agents must have the same number of actions: '\n",
    "                         '{agent1_name} has {agent1.num_actions}, '\n",
    "                         '{agent2_name} has {agent2.num_actions}.')\n",
    "\n",
    "    trainer = ParallelTrainer(hparams,\n",
    "        callbacks=[\n",
    "            PrintCallback(hparams=hparams, print_hparam_keys=['epoch'], print_data_keys=['reward']),\n",
    "            QEvalCallback(eval_on_collect=True, eval_on_train=True, eval_on_test=True)\n",
    "        ])\n",
    "\n",
    "    train_start_time = datetime.datetime.now()\n",
    "    print(f'Training {agent1_name} against {agent2_name} with train_env={train_env_name} test_env={test_env_name}...')\n",
    "    history = trainer.train(agent1, agent2, train_env, test_env)\n",
    "    train_finish_time = datetime.datetime.now()\n",
    "    print(f'Training finished. Elapsed time = {train_finish_time-train_start_time}')\n",
    "    print(history)\n",
    "\n",
    "    print('\\n')\n",
    "    if input('Save history? (y/N):').strip().lower() == 'y':\n",
    "        fname = input('Enter filename: ').strip()\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "        print(f'Saved pickle dump to {fname}')\n",
    "\n",
    "def play():\n",
    "\n",
    "    def is_human_first(human_name, agent_name) -> bool:\n",
    "        first_agent = input(f'Who goes first ({human_name}, {agent_name})? ').strip()\n",
    "        if first_agent == human_name:\n",
    "            return True\n",
    "        elif first_agent == agent_name:\n",
    "            return False\n",
    "        else:\n",
    "            print(f'{first_agent} is not a valid option.')\n",
    "            return is_human_first(human_name, agent_name)\n",
    "\n",
    "    human_name = input('Enter your name: ').strip()\n",
    "    agent_name, agent = get_agent()\n",
    "    human_first = is_human_first(human_name, agent_name)\n",
    "\n",
    "    _, env_dict = get_env()\n",
    "    env = BoardEnv(\n",
    "        board=env_dict['board_size'],\n",
    "        win_length=env_dict['win_length'],\n",
    "        reward_mode='sparse')\n",
    "    batch_env = Batch1Env(env)  # to handle executing single actions\n",
    "\n",
    "    step = env.reset()\n",
    "    if human_first:\n",
    "        env.render()\n",
    "        step.action = int(input(f'Enter move (0-{env.board_size}): ').strip())\n",
    "        step = batch_env.step(step)\n",
    "    while True:\n",
    "        step.action = agent.act(step)\n",
    "        step = batch_env.step(step)\n",
    "        if step.done:\n",
    "            winner = agent_name if step.reward > 0 else human_name\n",
    "            break\n",
    "        env.render()\n",
    "        step.action = int(input(f'Enter move (0-{env.board_size}): ').strip())\n",
    "        step = batch_env.step(step)\n",
    "        if step.done:\n",
    "            winner = human_name if step.reward > 0 else agent_name\n",
    "            break\n",
    "\n",
    "    # announce winner\n",
    "    print(f'{winner} wins!\\n')\n",
    "\n",
    "    # play again?\n",
    "    if input('Play again? (y/N):').strip().lower() == 'y':\n",
    "        play()\n",
    "\n",
    "def main_menu():\n",
    "    print(128*'\\n')\n",
    "    print('1. Train')\n",
    "    print('2. Play')\n",
    "    print('3. Exit')\n",
    "    print('\\n')\n",
    "\n",
    "    choice = input('Enter your choice: ')\n",
    "    if choice == '1':\n",
    "        train()\n",
    "    elif choice == '2':\n",
    "        play()\n",
    "    elif choice == '3':\n",
    "        exit()\n",
    "    else:\n",
    "        print('Invalid choice')\n",
    "\n",
    "    _ = input('\\nPress enter to continue...')\n",
    "\n",
    "def main():\n",
    "    print('CSE 4309 Machine Learning Project 7++')\n",
    "    print('Copyright 2021 Jacob Valdez. Released under the MIT License')\n",
    "    print('Code at https://github.com/JacobFV/assignment-rl')\n",
    "    print('\\n')\n",
    "    while True:\n",
    "        try:\n",
    "            main_menu()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('\\n')\n",
    "            print('An error occurred. Please try again.')\n",
    "            print('Please report issues on github at https://github.com/JacobFV/assignment-rl')\n",
    "            print('\\n')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "605fe966a75bc2c3dfa708e269323e6491854b30a36f4e77953579e94649bfba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
