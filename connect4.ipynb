{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "import functools\n",
    "from typing import Tuple, List, Mapping, Optional, Union, NamedTuple, Callable\n",
    "# Many default parameters are included in jnumpy and are optional.\n",
    "# I only resort to using `Optional` in the type annotations where the\n",
    "# context does not make this clear.  \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jnumpy: Jacob's numpy library for machine learning\n",
    "# Copyright (c) 2021 Jacob F. Valdez. Released under the MIT license.\n",
    "\n",
    "\n",
    "V = np.array # V is for Value type\n",
    "Vs = Tuple[V]\n",
    "Vss = Union[V,Vs]\n",
    "\n",
    "\n",
    "class ExecutionMode:\n",
    "    EAGER=1\n",
    "    STATIC=2  # STATIC execution mode not supported\n",
    "    \n",
    "EXECUTION_MODE = ExecutionMode.EAGER\n",
    "\n",
    "\n",
    "class T:\n",
    "    \"\"\"Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, val: Optional[V] = None):\n",
    "        self.val = val\n",
    "        \n",
    "        if val is None:\n",
    "            raise 'STATIC execution mode not supported'\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return eval(\"Index\")(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        raise NotImplementedError('slice assign not yet supported')\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return eval(\"Add\")(self, other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return eval('Neg')(self)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return eval(\"Sub\")(self, other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return eval(\"Mul\")(self, other)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        return eval(\"Pow\")(self, other)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        return eval(\"MatMul\")(self, other)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.val.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.val.ndim\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.val.size\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.val.dtype\n",
    "\n",
    "    @property\n",
    "    def T(self, axes: Tuple[int] = None):\n",
    "        return eval(\"Transpose\")(self, axes=axes)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.val == other.val\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.val)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.val)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.val)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.val.__getstate__()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.val = state\n",
    "\n",
    "    def __array__(self):\n",
    "        return self.val.__array__()\n",
    "\n",
    "\n",
    "Ts = Tuple[T]\n",
    "Tss = Union[T,Ts]\n",
    "\n",
    "\n",
    "class Var(T):\n",
    "    \"\"\"Variable Tensor\"\"\"\n",
    "    def __init__(self, val: Optional[V] = None, trainable: bool = True):\n",
    "        \n",
    "        self.trainable = trainable\n",
    "        super().__init__(val=val)\n",
    "\n",
    "\n",
    "class Op(T):\n",
    "    \"\"\"Operation-backed Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, *inputs: T):\n",
    "        \"\"\"Make sure to set any variables you might need in `forward` \n",
    "        before initializing when the graph is in eager execution mode\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_ts = inputs\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.EAGER:\n",
    "            val = self.forward(tuple(i.val for i in inputs))\n",
    "        else:\n",
    "            val = None\n",
    "        \n",
    "        super().__init__(val=val)\n",
    "        \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "\n",
    "\n",
    "class Transpose(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axes: Tuple[int] = None):\n",
    "        \n",
    "        self.forward_kwargs = dict()\n",
    "        self.reverse_kwargs = dict()\n",
    "        \n",
    "        if axes is not None:\n",
    "            self.forward_kwargs['axes'] = axes\n",
    "            self.reverse_kwargs['axes'] = tuple(reversed(axes))\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.transpose(**self.forward_kwargs)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "\n",
    "        dX = dY.transpose(**self.reverse_kwargs)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Reshape(Op):\n",
    "    \n",
    "    def __init__(self, t: T, shape: Tuple[int]):\n",
    "        \n",
    "        self.reshape_shape = shape\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.reshape(self.reshape_shape)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = dY.reshape(tuple(reversed(self.reshape_shape)))\n",
    "        \n",
    "        return (dX,)\n",
    " \n",
    "\n",
    "class Concat(Op):\n",
    "    \n",
    "    def __init__(self, ts: List[T], axis: int = 0):\n",
    "        \"\"\"Concatenates input tensors along an axis\n",
    "\n",
    "        Args:\n",
    "            t (T): [description]\n",
    "            axis (int, optional): Axis to concatenate along. Defaults to 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.axis = axis\n",
    "        self.orig_axis_lens = [t.shape[axis] for t in ts]\n",
    "\n",
    "        super().__init__(*ts)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        Xs = inputs\n",
    "        \n",
    "        Y = np.concatenate(Xs, axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "        \n",
    "        dXs = np.split(dY, self.orig_axis_dims, axis=self.axis)[0]\n",
    "        \n",
    "        return dXs\n",
    "\n",
    "\n",
    "class Index(Op):\n",
    "    \n",
    "    def __init__(self, t: T, indices):\n",
    "        \"\"\"Slices a tensor along all axes.\n",
    "\n",
    "        Args:\n",
    "            t (T): The tensor to slice\n",
    "            indices (Tuple[slice]):  The partial or full indices to slice on `t`.\n",
    "                Can be an index, single slice, tuple of slices, or Ellipsis.\n",
    "                `None` is not allowed.\n",
    "        \"\"\"\n",
    "        if not isinstance(indices, tuple):\n",
    "            indices = (indices,)\n",
    "\n",
    "        self.indices = indices\n",
    "        \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X[self.indices]\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.zeros(X.shape)\n",
    "        dX[self.indices] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceSum(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.sum(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.repeat(\n",
    "            np.expand_dims(dY, axis=self.axis),\n",
    "            X.shape[self.axis],\n",
    "            axis=self.axis\n",
    "        )\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceMax(Op):\n",
    "    \"\"\"Differentiable max operator\"\"\"\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.max(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        print(X.shape, dY.shape)\n",
    "\n",
    "        dX = np.zeros_like(X)\n",
    "        dX[np.argmax(X, axis=self.axis)] = dY\n",
    "\n",
    "        print(dX.shape)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "class ReduceMin(Op):\n",
    "    \"\"\"Differentiable min operator\"\"\"\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.min(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.zeros_like(X)\n",
    "        dX[np.argmin(X, axis=self.axis)] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class NaN2Num(Op):\n",
    "    \n",
    "    def __init__(self, t: T, posinf: float = 1e3, neginf: float = -1e3):\n",
    "        self.posinf = posinf\n",
    "        self.neginf = neginf\n",
    "            \n",
    "        super().__init__(t)\n",
    "\n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.nan_to_num(X, posinf=self.posinf, neginf=self.neginf)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = np.nan_to_num(dZ, posinf=10., neginf=-10.)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Linear(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class StopGrad(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = np.zeros_like(dZ)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Neg(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = -X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = -dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Add(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X + Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Sub(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X - Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = -dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Mul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X * Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Y * dZ\n",
    "        dY = X * dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class MatMul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X @ Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]  # [A,B]\n",
    "        Y = inputs[1]  # [B,C]\n",
    "        dZ = top_grad  # [A,C]\n",
    "        \n",
    "        dX = dZ @ Y.transpose()\n",
    "        dY = X.transpose() @ dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Exp(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.exp(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Z * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Sigm(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = 1 / (1 + np.exp(-X))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Z * (1 - Z) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Tanh(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.tanh(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = ((1 - Z)**2) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Relu(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X > 0) * X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = (X > 0) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Threshold(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X >= 0)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Pow(Op):\n",
    "    \n",
    "    def __init__(self, x: T, power: int):\n",
    "        \n",
    "        self.power = power\n",
    "        \n",
    "        super().__init__(x)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        \n",
    "        Y = X ** p\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = p * X ** (p-1) * dY\n",
    "        dX = np.nan_to_num(dX, posinf=1e3, neginf=-1e3)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, lr: float = 0.001):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.debug = False\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.STATIC:\n",
    "            raise 'STATIC execution mode not enabled'\n",
    "        \n",
    "        self.bprop(t_out=t, output_grad=-np.ones_like(t.val))\n",
    "        \n",
    "    def bprop(self, t_out: T, output_grad: V):\n",
    "        \n",
    "        output_grad = np.nan_to_num(output_grad, posinf=10., neginf=-10.)\n",
    "        \n",
    "        assert isinstance(t_out, (Var, Op))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f'bp {t_out} output_grad:')\n",
    "            print(output_grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        This approach does not efficiently handle weights that are consumed by multiple nodes\n",
    "        It would be better to treat backpropagation from a spreading-network-delta perspective\n",
    "        than assume everything is a tree (That's also how I should do STATIC execution refresh)\n",
    "        This should still work though, but it's just going to set the same weight multiple times\n",
    "        for every downstream consumer.\n",
    "\n",
    "        Actually, the whole thesis of minibatch gradient descent is that we can approximate a global\n",
    "        gradient by updates on local subsets of data, so it might be sufficient to leave the code\n",
    "        as is.\n",
    "\n",
    "        However this approach will still take unnecessary walks down the tree in depth-first fashion.\n",
    "        Innefficient: Yes; Works: Yes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # iteratively called\n",
    "        if isinstance(t_out, Var):\n",
    "            if t_out.trainable:\n",
    "                #print('output_grad', output_grad.shape)\n",
    "                if self.debug:                    \n",
    "                    print('t_out.val (old)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "                \n",
    "                # yucky duct tape to handle batch size differences\n",
    "                if t_out.shape[0] == 1 and output_grad.shape[0] > 1:\n",
    "                    output_grad = np.sum(output_grad, axis=0)[None, ...]\n",
    "\n",
    "                t_out.val = t_out.val + (self.lr * output_grad)\n",
    "                if self.debug:\n",
    "                    print('t_out.val (new)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "            \n",
    "        elif isinstance(t_out, Op):\n",
    "            input_grads = t_out.reverse_grad(\n",
    "                inputs=tuple(t.val for t in t_out.input_ts),\n",
    "                output=t_out.val, top_grad=output_grad)\n",
    "            \n",
    "            for input_t, input_grad in zip(t_out.input_ts, input_grads):\n",
    "                self.bprop(t_out=input_t, output_grad=input_grad)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._built = False\n",
    "        self._loss = Var(np.zeros(()), trainable=False)\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return self._loss\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return []\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, X_T: T) -> T:\n",
    "        if not self._built:\n",
    "            self.build(X_T.shape)\n",
    "            self._built = True\n",
    "\n",
    "        # reset the regularization loss\n",
    "        self._loss = Var(0, trainable=False)\n",
    "\n",
    "        return self.forward(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.17077285, 0.91746829, 0.28856126, 0.31815334, 0.33051224],\n",
       "        [0.44030342, 0.06883121, 0.74554893, 0.27581148, 0.6131212 ],\n",
       "        [0.89694911, 0.58436521, 0.59848925, 0.70549612, 0.76714117]]),\n",
       " array([[-0.        ,  0.05412479,  0.00084676, -0.        , -0.        ,\n",
       "         -0.        , -0.        , -0.        ,  0.07431308, -0.        ],\n",
       "        [-0.        ,  0.04944821, -0.        , -0.        , -0.        ,\n",
       "         -0.        ,  0.02637271,  0.02425416,  0.10352752,  0.00586475],\n",
       "        [-0.        ,  0.09907608, -0.        , -0.        , -0.        ,\n",
       "         -0.        ,  0.00624597,  0.00575352,  0.14533493, -0.        ]]),\n",
       " Tensor(0.010286420401423798),\n",
       " [Tensor([[ 0.01895912  0.03404793 -0.03160739 -0.04358683 -0.04132869 -0.03075421\n",
       "     0.01232975 -0.01567076  0.03066721 -0.0165504 ]\n",
       "   [ 0.02291355  0.00272011  0.03967205  0.04094996 -0.04109615 -0.01771524\n",
       "    -0.03811032 -0.02313912  0.01421816 -0.03133357]\n",
       "   [-0.03383381 -0.0413763   0.02987785 -0.01045483 -0.0298578  -0.04684618\n",
       "    -0.00841894  0.03067419  0.04779576  0.04351242]\n",
       "   [-0.00635463  0.0498718   0.01871131 -0.0138359  -0.04460059  0.02192595\n",
       "    -0.01208861  0.01650669  0.04729478 -0.03601244]\n",
       "   [-0.01543119  0.03352678 -0.0241662  -0.04687095  0.03008312 -0.02002091\n",
       "    -0.01398323 -0.01296837  0.04661996 -0.04082962]]),\n",
       "  Tensor([[-0.00911047  0.03080642 -0.03674084 -0.04793586  0.01749966  0.02393288\n",
       "     0.04175138  0.01327609  0.01178378  0.01783418]])])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "        units: int, \n",
    "        activation: Op = None, \n",
    "        use_bias: bool = True,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None\n",
    "    ):\n",
    "        super(Dense, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "            \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(input_shape[-1], self.units))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.units))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # compute presynaptic input\n",
    "        Z_T = X_T @ self.W_T\n",
    "\n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply activation\n",
    "        Y_T = self.activation(Z_T)\n",
    "        \n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Dense(10, Relu, 0.1, 0.1, 0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(3, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.val, Y_T.val, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 7, 70, 5),\n",
       " (2, 7, 70, 64),\n",
       " Tensor(0.6604701136122415),\n",
       " [Tensor([[-0.00669302 -0.04148393  0.02680388 ... -0.04593632  0.0035611\n",
       "    -0.01565057]\n",
       "   [ 0.0210981  -0.01297245 -0.02408055 ... -0.0449786  -0.02102165\n",
       "     0.03065583]\n",
       "   [ 0.00032685 -0.03122991  0.01316144 ... -0.00224943  0.01421396\n",
       "     0.00495422]\n",
       "   ...\n",
       "   [ 0.00908327 -0.03973156 -0.03590723 ... -0.01658428  0.00646538\n",
       "     0.0337815 ]\n",
       "   [ 0.00995362  0.00724076 -0.00470494 ...  0.00564645  0.00468026\n",
       "     0.02973746]\n",
       "   [-0.0457778  -0.02148497 -0.00115111 ...  0.03316463 -0.03893888\n",
       "     0.02907854]])])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv2D(Layer):\n",
    "    \"\"\"Standard 2D Conv layer.\n",
    "    I.E. convolves over Tensors shaped [B, H, W, D]\n",
    "    to produce [B, H-2*kernel_size, W-2*kernel_size, filters]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        filters: int, \n",
    "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
    "        strides: Union[int, Tuple[int, int]] = 1,\n",
    "        padding: str = 'valid',  # 'valid' or 'same'\n",
    "        activation: Op = None,\n",
    "        use_bias: bool = False,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None,\n",
    "    ):\n",
    "        super(Conv2D, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1, 'kernel_size must be odd'\n",
    "        if isinstance(strides, int):\n",
    "            strides = (strides, strides)\n",
    "        assert strides[0] > 0 and strides[1] > 0, 'strides must be positive'\n",
    "        padding = padding.lower()\n",
    "        assert padding in ('valid', 'same'), 'padding must be valid or same'\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(\n",
    "            self.kernel_size[0]*self.kernel_size[1]*input_shape[-1], \n",
    "            self.filters))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.filters))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # maybe pad input\n",
    "        if self.padding == 'same':\n",
    "\n",
    "            # various padding sizes, strides, and offsets\n",
    "            # 0   1   2   3   4\n",
    "            #     0 1 2 3 4\n",
    "            #         0 1 2 3 4 5 6 7 8 9\n",
    "            #         0 1 2 3 4\n",
    "            #         0   1   2   3   4\n",
    "\n",
    "            pad_top = self.strides[0]*(self.kernel_size[0]-1)//2\n",
    "            pad_bottom = pad_top\n",
    "            pad_left = self.strides[1]*(self.kernel_size[1]-1)//2\n",
    "            pad_right = pad_left\n",
    "            B, H_orig, W_orig, C = X_T.shape\n",
    "\n",
    "            # pad height\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, pad_top, W_orig, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, pad_bottom, W_orig, C)), trainable=False),\n",
    "            ], axis=1)\n",
    "\n",
    "            # pad width\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_left, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_right, C)), trainable=False),\n",
    "            ], axis=2)\n",
    "\n",
    "        elif self.padding == 'valid':\n",
    "            pass\n",
    "\n",
    "        # stack the input tensor along the channel axis\n",
    "        # but shifted by all possible kernel shifts\n",
    "        stack = []\n",
    "        for shift in itertools.product(range(0, self.strides[0]*self.kernel_size[0], self.strides[0]),\n",
    "                                       range(0, self.strides[1]*self.kernel_size[1], self.strides[1])):\n",
    "            stack.append(X_T[\n",
    "                :,\n",
    "                shift[0]:,\n",
    "                shift[1]:,\n",
    "                :\n",
    "            ])\n",
    "\n",
    "        # clip stack to greatest common shape\n",
    "        min_shape = np.min(np.array([s.shape for s in stack]), axis=0)\n",
    "        stack = [s[:, :min_shape[1], :min_shape[2], :] for s in stack]\n",
    "\n",
    "        # stack the shifted tensors along the channel axis\n",
    "        stacked = Concat(stack, axis=3)  # [B, H-k_h//2, W-k_w//2, C*k_h*k_w]\n",
    "\n",
    "        # convolve over the stacked tensors\n",
    "        Z_T = stacked @ self.W_T  \n",
    "    \n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply the activation function\n",
    "        Y_T = self.activation(Z_T)\n",
    "\n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Conv2D(filters=64, kernel_size=5, strides=5, padding='same', weight_L2=0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 7, 70, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxisMaxPooling(Layer):\n",
    "\n",
    "    def __init__(self, axis: int):\n",
    "        super(AxisMaxPooling, self).__init__()\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        return ReduceMax(X_T, self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(Layer):\n",
    "\n",
    "    def __init__(self, fn: Callable[[T], T]):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        return self.fn(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3, 4, 5), (2, 60), Tensor(0), [])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Flatten(Layer):\n",
    "    \"\"\"Flattens all non-batch dimensions into a single axis\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return []\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        flat_dims = functools.reduce(lambda x, y: x*y, X_T.shape[1:])\n",
    "        Y_T = Reshape(X_T, (X_T.shape[0], flat_dims))\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Flatten()\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 3, 4, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[0.24706402]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        super(Sequential, self).__init__()\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        for layer in self.layers:\n",
    "            X_T = layer(X_T)\n",
    "        return X_T\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return sum(layer.loss for layer in self.layers)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        trainable_vars = []\n",
    "        for layer in self.layers:\n",
    "            trainable_vars += layer.trainable_variables\n",
    "        return trainable_vars\n",
    "\n",
    "\n",
    "net = Sequential([\n",
    "    Dense(10, Relu),\n",
    "    Dense(128, Relu),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(X_T)\n",
    "\n",
    "img_T = Var(np.random.uniform(0, 1, size=(1, 28, 28, 1)), trainable=False)\n",
    "net = Sequential([\n",
    "    Conv2D(32, 3, 2, activation=Relu),\n",
    "    Conv2D(64, 3, 2, activation=Relu),\n",
    "    Flatten(),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(img_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standards:\n",
    "- `Step`: uses batch dimension (except `done` which is always a bool)\n",
    "- `Agent` uses batch dimension\n",
    "- `Environment` doesn't use batch dimension\n",
    "\n",
    "This means you will have to use `Step.batch` and `Step.unbatch` in your training/running loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(NamedTuple):\n",
    "    \"\"\"Single step.\"\"\"\n",
    "    \n",
    "    obs: np.ndarray\n",
    "    next_obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: np.ndarray\n",
    "    done: bool\n",
    "    info: any\n",
    "\n",
    "    @staticmethod\n",
    "    def unbatch(step: Step) -> List[Step]:\n",
    "        return [\n",
    "            Step(\n",
    "                obs=step.obs[i:i+1],\n",
    "                next_obs=step.next_obs[i:i+1],\n",
    "                action=step.action[i:i+1],\n",
    "                reward=step.reward[i:i+1],\n",
    "                done=step.done,\n",
    "                info=step.info[i:i+1],\n",
    "            )\n",
    "            for i in range(step.obs.shape[0])\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def batch(steps: List[Step]) -> Step:\n",
    "        return Step(\n",
    "            obs=np.concatenate([step.obs for step in steps], axis=0),\n",
    "            next_obs=np.concatenate([step.next_obs for step in steps], axis=0),\n",
    "            action=np.concatenate([step.action for step in steps], axis=0),\n",
    "            reward=np.concatenate([step.reward for step in steps], axis=0),\n",
    "            done=any(step.done for step in steps),\n",
    "            info=[step.info for step in steps])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_no_batch_axis(step: NoBatchStep) -> Step:\n",
    "        return Step(\n",
    "            obs=step.obs[None, ...],\n",
    "            next_obs=step.next_obs[None, ...],\n",
    "            action=step.action[None, ...],\n",
    "            reward=step.reward[None, ...],\n",
    "            done=step.done,\n",
    "            info=[step.info]\n",
    "        )\n",
    "\n",
    "# dimensional type hinting\n",
    "BatchStep = Step\n",
    "NoBatchStep = Step\n",
    "\n",
    "Traj = List[BatchStep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- replace `list` with List and same for dict/mapping\n",
    "- use one-hot encodings for all actions and rewire the policies to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"RL environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        \"\"\"Resets the environment\n",
    "\n",
    "        Returns:\n",
    "            Step: Initial step. The `next_obs` attribute should be set \n",
    "                with an initial observation. `done` should be False. \n",
    "                `obs` and `action` should not be used.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Step:\n",
    "        \"\"\"Computes one logical step in the environment\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): The action to take.\n",
    "\n",
    "        Returns:\n",
    "            Step: Step resulting from taking `action`. The `next_obs` attribute\n",
    "                should be set with the observation resulting from taking the `action`\n",
    "                in the current environment state. `obs` should not be used. If the \n",
    "                environment is turn-based, then the reward should correspond to the \n",
    "                agent that just acted (not the next agent in line to act).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NoBatchEnv(Environment):\n",
    "    \"\"\"Environment with no batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "\n",
    "class BatchEnv(Environment):\n",
    "    \"\"\"Environment with batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> BatchStep:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch1Env(BatchEnv):\n",
    "    \"\"\"Adds a batch axis to all outgoing Steps and strips it off incoming Steps.\"\"\"\n",
    "\n",
    "    def __init__(self, env: NoBatchEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.reset())\n",
    "\n",
    "    def step(self, action: np.array) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.step(action[0]))\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEnv(BatchEnv):\n",
    "    \"\"\"Keeps reseting the same environment in a batch.\n",
    "    \n",
    "    Declares itself done when a total of `batch_size` individual environment\n",
    "    dones are experienced.\n",
    "\n",
    "    NOTE: Individual environments should be `NoBatchEnv`'s.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, env_init_fn: Callable[[], NoBatchEnv]):\n",
    "        self.batch_size = batch_size\n",
    "        self.env_init_fn = env_init_fn\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        self.dones = 0\n",
    "        self.envs = [self.env_init_fn() for _ in range(self.batch_size)]\n",
    "        steps = [env.reset() for env in self.envs]\n",
    "        steps = [Step.from_no_batch_axis(step) for step in steps]\n",
    "        return Step.batch(steps)\n",
    "\n",
    "    def step(self, action: np.array) -> Step:\n",
    "        steps = []\n",
    "        for i, (env, single_action) in enumerate(zip(self.envs, action)):\n",
    "            step = env.step(single_action)\n",
    "            if step.done:\n",
    "                self.envs[i] = self.env_init_fn()\n",
    "                new_step = env.reset()\n",
    "                step.next_obs = new_step.next_obs\n",
    "                self.dones += 1\n",
    "            steps.append(Step.from_no_batch_axis(step))\n",
    "\n",
    "        batched_step = Step.batch(steps)\n",
    "        batched_step.done = self.dones >= self.batch_size\n",
    "        return batched_step\n",
    "\n",
    "    def render(self):\n",
    "        for env in self.envs:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer.\n",
    "\n",
    "    Expects the following hyperparameters:\n",
    "        - `epoch`: The current epoch.\n",
    "        - `batch_size`: Number of trajectories to return at each call.\n",
    "        - `min_sample_len`: Minimum length of trajectories to sample.\n",
    "        - `max_sample_len`: Maximum length of trajectories to sample.\n",
    "        - `num_steps_replay_coef`: Sampling coefficient based on trajectory length.\n",
    "        - `success_replay_coef`: Sampling coefficient based on trajectory success.\n",
    "        - `age_replay_coef`: Sampling coefficient based on trajectory age.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, hparams: dict):\n",
    "        self.hparams = hparams\n",
    "        self.trajs = dict()\n",
    "\n",
    "    @property\n",
    "    def flat_traj(self):\n",
    "        return [step for traj in self.trajs for step in traj]\n",
    "\n",
    "    def add(self, traj: Traj, epoch: int):\n",
    "        \"\"\"Add a new trajectory to the buffer.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): the trajectory to add.\n",
    "            epoch (int): the epoch that the trajectory was experience in.\n",
    "        \"\"\"\n",
    "        if epoch not in self.trajs:\n",
    "            self.trajs[epoch] = []\n",
    "        self.trajs[epoch] += traj\n",
    "\n",
    "    def sample(self) -> Traj:\n",
    "        \"\"\"Samples a batched trajectory from the buffer stochastically based on:\n",
    "            - the number of steps in the trajectory (num_steps_replay_coef)\n",
    "            - how well the agent did in the trajectory (success_replay_coef)\n",
    "            - how long ago the trajectory was experienced (age_replay_coef)\n",
    "\n",
    "        Returns:\n",
    "            Traj: a trajectory of batched steps experienced.\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = {\n",
    "            epoch: self.hparams['num_steps_replay_coef'] * len(traj) +\n",
    "                   self.hparams['success_replay_coef'] * sum(step.reward for step in traj) +\n",
    "                   self.hparams['age_replay_coef'] * (epoch - self.hparams['epoch'])\n",
    "            for epoch, traj in self.trajs.items()\n",
    "        }\n",
    "        epochs = np.random.choice(list(weights.keys()), size=(), replace=True, p=list(weights.values()))\n",
    "        trajs = [self.trajs[epoch] for epoch in epochs]\n",
    "        batched_traj = [BatchStep.batch(steps) for steps in zip(*trajs)]\n",
    "        return batched_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, policy: Callable):\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, step: BatchStep) -> np.ndarray:\n",
    "        \"\"\"Generates an action for a given observation using `self.policy`. \n",
    "        Override if you want to give your policy more information such as\n",
    "        recurrent state or previous reward.\n",
    "\n",
    "        Args:\n",
    "            step (Step): last step output by the environment. This means the agent\n",
    "                should feed `step.next_obs`, not `step.obs` to its policy. If the \n",
    "                environment is multi-agent, then the `reward` attribute has already\n",
    "                updated to reflect the reward for this agent by the driver.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The action to take\n",
    "        \"\"\"\n",
    "        return self.policy(step.next_obs)\n",
    "\n",
    "    def reward(self, traj: Traj) -> float:\n",
    "        \"\"\"Evaluates the cumulative reward for your agent as the sum of \n",
    "        individual rewards experienced. \n",
    "        \n",
    "        If your agent uses intrinsic rewards, be sure to add them in here.\n",
    "        Do not introduce Q-values or predicted rewards here.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "\n",
    "        Returns:\n",
    "            float: Cumulative (sum) reward over the entire sequence.\n",
    "        \"\"\"\n",
    "        return sum(step.reward for step in traj)\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        raise NotImplemented('Method `train` must be implemented by subclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelDriver:\n",
    "    \"\"\"Drives batched turn-based `BatchEnv` environments with multiple agents.\n",
    "    Also supports single-agent environments as a special case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def drive(self, agents: Mapping[str, Agent], env: BatchEnv) -> Mapping[str, Traj]:\n",
    "        \"\"\"Drives a batched environment with multiple agents.\n",
    "        \n",
    "        Args:\n",
    "            agents (Mapping[str, Agent]): A dictionary of agents to drive.\n",
    "            env (BatchEnv): The environment to drive.\n",
    "\n",
    "        Returns:\n",
    "            Mapping[str, Traj]: A dictionary of trajectories for each agent.\n",
    "                Each trajectory is completely disengaged from the other agent's.\n",
    "                (i.e.: the obs, next_obs, action, reward, done attributes are\n",
    "                individual to each agent for each trajectory.)\n",
    "        \"\"\"\n",
    "\n",
    "        names_it = itertools.cycle(agents.keys())\n",
    "        trajs = {agent_name: [] for agent_name in agents}\n",
    "        prev_rewards = {agent_name: 0. for agent_name in agents}\n",
    "\n",
    "        step = env.reset()\n",
    "        while not step.done:\n",
    "            agent_name = next(names_it)\n",
    "            \n",
    "            # `Agent.forward` only looks at `step.next_obs` and `step.reward`\n",
    "            # but I'm assigning defaults just to be safe.\n",
    "            action = agents[agent_name].forward(Step(\n",
    "                obs=step.obs,  # what the previous agent saw before acting\n",
    "                action=step.action,  # what the previous agent did\n",
    "                next_obs=step.next_obs,  # what the current agent sees before acting\n",
    "                reward=prev_rewards[agent_name],  # the reward this agent experienced following its last action\n",
    "                done=step.done,  # whether the environment was done after the previous agent acted\n",
    "                info=step.info  # any extra information the environment might have output\n",
    "            )) \n",
    "\n",
    "            prev_step = step\n",
    "            step = env.step(action)  # `Environment.step` produces a Step with all fields except `step.obs` set\n",
    "            step.obs = prev_step.next_obs  # the current agent's observation is the previous agent's next observation\n",
    "            prev_rewards[agent_name] = step.reward  # the reward for the action the current agent just took\n",
    "            trajs[agent_name].append(step)  # Step completely corresponding to this agent (obs before action, obs after action, action, reward, done, info)\n",
    "        \n",
    "        # remove the agents' first trajectories since they don't carry Markovian information\n",
    "        trajs = {n: traj[1:] for n, traj in trajs.items()}  \n",
    "        return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTrainer:\n",
    "    \"\"\"Trains `BatchEnv` environments and mutliple agents \n",
    "    (with N=1 single-agent supported as a special case).\n",
    "    \n",
    "    Uses following hyperparameters:\n",
    "    - `epoch`: the current epoch. Reads and writes to this variable.\n",
    "    - `epochs`: the number of epochs to train for.\n",
    "    - `min_steps_per_epoch`: the minimum number of steps to train for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams: dict, callbacks: List[Callable]):\n",
    "        self.hparams = hparams\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "    def train(self, \n",
    "        agents: Mapping[str, Agent], \n",
    "        env: BatchEnv,\n",
    "        test_env: BatchEnv = None,\n",
    "        buffers: Mapping[str, ReplayBuffer] = None,\n",
    "        collect_driver: ParallelDriver = None,\n",
    "        test_driver: ParallelDriver = None,\n",
    "        histories: Mapping[str, Mapping[int, Mapping[str, any]]] = None,\n",
    "        ) -> Mapping[int, Mapping[str, any]]:\n",
    "\n",
    "        agent_names = list(agents.keys())\n",
    "        \n",
    "        # initialize defaults\n",
    "        if test_env is None:\n",
    "            test_env = env\n",
    "        if buffers is None:\n",
    "            buffers = dict()\n",
    "        if collect_driver is None:\n",
    "            collect_driver = ParallelDriver()\n",
    "        if test_driver is None:\n",
    "            test_driver = collect_driver\n",
    "        if histories is None:\n",
    "            histories = dict()  # {agent_name: {epoch: {...data}}}\n",
    "\n",
    "        # build uninitialized agent-specific objects\n",
    "        for agent_name in agent_names:\n",
    "            if agent_name not in buffers:\n",
    "                buffers[agent_name] = ReplayBuffer(hparams=self.hparams)\n",
    "            if agent_name not in histories:\n",
    "                histories[agent_name] = dict()\n",
    "\n",
    "        # run training loop\n",
    "        for epoch in range(self.hparams['epoch'], self.hparams['epochs']):\n",
    "            self.hparams['epoch'] = epoch\n",
    "\n",
    "            # collect trajectories\n",
    "            steps = 0\n",
    "            while steps < self.hparams['min_steps_per_epoch']:\n",
    "                collect_trajs = collect_driver.drive(agents, env)\n",
    "                steps += min(len(traj) for _, traj in collect_trajs.items())\n",
    "                for agent_name in agent_names:\n",
    "                    buffers[agent_name].add(collect_trajs[agent_name])\n",
    "\n",
    "            # train\n",
    "            train_trajs = {agent_name: buffers[agent_name].sample() for agent_name in agent_names}\n",
    "            for agent_name in agent_names:\n",
    "                agents[agent_name].train(train_trajs[agent_name])\n",
    "                \n",
    "            # test\n",
    "            test_trajs = test_driver.drive(agents, env)\n",
    "\n",
    "            # record history and run callbacks\n",
    "            for agent_name in agent_names:\n",
    "                histories[agent_name][epoch] = {\n",
    "                    'epoch': epoch,\n",
    "                    'agent': agents[agent_name],\n",
    "                    'all_agents': agents,\n",
    "                    'env': env,\n",
    "                    'test_env': test_env,\n",
    "                    'collect_traj': collect_trajs[agent_name],\n",
    "                    'train_traj': train_trajs[agent_name],\n",
    "                    'test_traj': test_trajs[agent_name],\n",
    "                    'buffer': buffers[agent_name],\n",
    "                }\n",
    "                for callback in self.callbacks:\n",
    "                    callback(histories[agent_name][epoch])\n",
    "\n",
    "        return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback:\n",
    "\n",
    "    def __init__(self, hparams: dict, print_hparam_keys: List[str] = None, print_data_keys: List[str] = None):\n",
    "        if print_hparam_keys is None:\n",
    "            print_hparam_keys = ['epoch']\n",
    "        if print_data_keys is None:\n",
    "            print_data_keys = []\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.print_hparam_keys = print_hparam_keys\n",
    "        self.print_data_keys = print_data_keys\n",
    "\n",
    "    def __call__(self, data: Mapping[str, any]):\n",
    "        for key in self.print_hparam_keys:\n",
    "            print(f'{key}: {self.hparams[key]}', end='\\t')\n",
    "        for key in self.print_data_keys:\n",
    "            print(f'{key}: {data[key]}', end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEvalCallback:\n",
    "\n",
    "    def __init__(self, \n",
    "        eval_on_collect: bool = True, \n",
    "        eval_on_train: bool = False, \n",
    "        eval_on_test: bool = False):\n",
    "\n",
    "        self.eval_on_collect = eval_on_collect\n",
    "        self.eval_on_train = eval_on_train\n",
    "        self.eval_on_test = eval_on_test\n",
    "\n",
    "    def __call__(self, data: Mapping[str, any]):\n",
    "        agent = data['agent']\n",
    "        if not hasattr(agent, 'q_eval'):\n",
    "            return\n",
    "\n",
    "        if self.eval_on_collect:\n",
    "            traj = data['collect_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_collect'] = q_val\n",
    "\n",
    "        if self.eval_on_train:\n",
    "            traj = data['train_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_train'] = q_val\n",
    "\n",
    "        if self.eval_on_test:\n",
    "            traj = data['test_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_test'] = q_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- make the reward optionally an advantage computation over last round\n",
    "- also make a recurrent DQN agent (estimate q function of a sequence of states)\n",
    "- make a simple greedy connect4 agent\n",
    "- make the preprocessor perform a columnwise mean pool before flattening\n",
    "- train the preprocessor on an auxillary objective to estimate the max connected for each length for self and for oponent\n",
    "- add padding='SAME'|'VALID' to conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomAgent(Agent):\n",
    "    \"\"\"Takes a random action on each timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int):\n",
    "        super(RandomAgent, self).__init__(policy=self._policy)\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        choices = np.random.randint(0, self.num_actions, (obs.shape[0],))\n",
    "        onehots = np.eye(self.num_actions)[choices]\n",
    "        return onehots\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "agent = RandomAgent(num_actions=5)\n",
    "obs = Var(np.array([[1, 2], [3, 4]]))\n",
    "step = Step(obs=None, next_obs=obs, action=None, reward=None, done=None, info=None)\n",
    "agent.forward(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RealDQN(Agent):\n",
    "    \"\"\"'Classic' Deep Q-learning agent.\n",
    "    Implements the approach in https://arxiv.org/pdf/1312.5602.pdf.\n",
    "    \n",
    "    NOTE: This agent expects its encoder to output a per-column vector.\n",
    "        I.E.: [B, H, W, C] --encoder--> [B, W, d_enc]\n",
    "\n",
    "    This agent uses the following hyperparameters:\n",
    "        - activation: activation function to use for the hidden layers.\n",
    "        - hidden_size: hidden layer size.\n",
    "        - discount: discount factor.\n",
    "        - optimizer: optimizer to use.\n",
    "        - epsilon_start: initial epsilon value.\n",
    "        - min_epsilon: minimum epsilon value.\n",
    "        - epsilon_decay: epsilon decay rate.\n",
    "        - epoch: current epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder  # [B, H, W, C] -> [B, W, d_enc]\n",
    "        self.neck = Flatten()\n",
    "        self.head = Sequential([\n",
    "            Dense(hparams['hidden_size'], hparams['activation']), \n",
    "            Dense(1, Linear)\n",
    "        ]) # [B, L+|A|] -> [B, 1]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(RealDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        B = obs.shape[0]\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = max(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            indeces = np.random.randint(0, self.num_actions, (B,))\n",
    "            return np.eye(self.num_actions)[indeces]\n",
    "    \n",
    "        # Otherwise take the action with the highest Q-value\n",
    "        q_vals = np.zeros((B, self.num_actions))  # [B, self.num_actions]\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "\n",
    "        for action_index in range(self.num_actions):\n",
    "\n",
    "            # prepare action\n",
    "            action_T = Var(np.repeat(\n",
    "                np.array([action_index])[None, :], \n",
    "                repeats=B, axis=0))  # [B, 1]\n",
    "\n",
    "            # run the network\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [B, W*d_enc+A]\n",
    "            q_T = self.head(cat_T)  # [B, 1]\n",
    "\n",
    "            # store q-values\n",
    "            q_vals[:, action_index] = q_T.val[:, 0]\n",
    "        \n",
    "        # select the action with the highest Q-value\n",
    "        action_indeces = np.argmax(q_vals, axis=1)  # [B]\n",
    "        onehots = np.eye(self.num_actions)[action_indeces]  # [B, self.num_actions]\n",
    "        return onehots\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_T = Var(step.action, trainable=False)  # [B, A]\n",
    "            next_obs_T = Var(step.next_obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_next_T = Var(self.policy(step.next_obs), trainable=False)  # [B, A]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [B, d_enc+A]\n",
    "            Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            next_enc_T = self.neck(self.encoder(next_obs_T))  # [B, W*d_enc]\n",
    "            next_cat_T = Concat([next_enc_T, action_next_T], axis=1)  # [B, d_enc+A]\n",
    "            Qnext_T = self.head(next_cat_T)[:, 0]  # [B]\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above eq; small updates are \n",
    "            # ensured by small SGD lr instead)\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Qnext_T)) - Qnow_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        action_T = Var(action)  # [B, A]\n",
    "        enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "        cat_T = Concat([enc_T, action_T], axis=1)  # [B, W*d_enc+A]\n",
    "        Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "\n",
    "        return Qnow_T.val\n",
    "\n",
    "\n",
    "test_encoder = Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    AxisMaxPooling(1),\n",
    "])  # [B, H, W, 2] -> [B, W, d_enc]\n",
    "\n",
    "test_batch_size = 5\n",
    "test_num_actions = 7\n",
    "\n",
    "test_step = Step(\n",
    "    obs=np.random.rand(test_batch_size, test_num_actions, test_num_actions, 2),\n",
    "    next_obs=np.random.rand(test_batch_size, test_num_actions, test_num_actions, 2),\n",
    "    action=np.random.rand(test_batch_size, test_num_actions),\n",
    "    reward=np.random.rand(test_batch_size),\n",
    "    done=False,\n",
    "    info={}\n",
    ")\n",
    "\n",
    "test_hparams = {\n",
    "    'hidden_size': 256,\n",
    "    'activation': Relu,\n",
    "    'optimizer': SGD(1e-3),\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_decay': 0.9,\n",
    "    'min_epsilon': 0.1,\n",
    "    'discount': 0.99,\n",
    "    'epoch': 0,\n",
    "}\n",
    "\n",
    "test_agent = RealDQN(num_actions=test_num_actions, encoder=test_encoder, hparams=test_hparams)\n",
    "test_agent.forward(test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00722749, -0.00724694, -0.0074713 , -0.0071368 , -0.00791581,\n",
       "        -0.00766936, -0.00773851],\n",
       "       [-0.00747899, -0.00760495, -0.00735962, -0.00726089, -0.00771944,\n",
       "        -0.00757706, -0.00789943],\n",
       "       [-0.0069688 , -0.0074345 , -0.00732423, -0.00736093, -0.00774947,\n",
       "        -0.0075314 , -0.00803977],\n",
       "       [-0.00714163, -0.00728854, -0.00701383, -0.00713212, -0.00748578,\n",
       "        -0.0078163 , -0.00767636],\n",
       "       [-0.00727814, -0.00738283, -0.00723426, -0.00712125, -0.00787802,\n",
       "        -0.00742734, -0.00776105]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CategoricalDQN(Agent):\n",
    "    \"\"\"Categorical deep Q-network agent.\n",
    "    I never read the paper for this architecture, so my implementation may\n",
    "    be different from the origonal researchers.\n",
    "    \n",
    "    NOTE: This agent expects its encoder to output a per-column vector.\n",
    "        I.E.: [B, H, W, C] --encoder--> [B, W, d_enc]\n",
    "\n",
    "    This agent uses the following hyperparameters:\n",
    "        - activation: activation function to use for the hidden layers.\n",
    "        - categorical_hidden_size: hidden layer size.\n",
    "        - discount: discount factor.\n",
    "        - optimizer: optimizer to use.\n",
    "        - epsilon_start: initial epsilon value.\n",
    "        - min_epsilon: minimum epsilon value.\n",
    "        - epsilon_decay: epsilon decay rate.\n",
    "        - epoch: current epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder  # [B, H, W, C] -> [B, W, d_enc]\n",
    "        self.neck = Flatten()\n",
    "        self.head = Sequential([\n",
    "            Dense(hparams['categorical_hidden_size'], hparams['activation']), \n",
    "            Dense(1, Linear)\n",
    "        ]) # [B, W, d_enc] -> [B, A, 1]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(CategoricalDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        B = obs.shape[0]\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = max(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            indeces = np.random.randint(0, self.num_actions, (B,))\n",
    "            return np.eye(self.num_actions)[indeces]\n",
    "        \n",
    "        # Otherwise estimate Q-values for all actions\n",
    "        obs_T = Var(obs, trainable=False)  # [B, H, W, C]\n",
    "        enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "        qvals_T = self.head(enc_T)  # [B, W, 1]\n",
    "        return qvals_T[..., 0].val  # [B, W]\n",
    "\n",
    "    \n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_indeces = np.argmax(step.action, axis=1)  # [B]\n",
    "            obs_next_T = Var(step.next_obs, trainable=False)  # [B, H, W, 2]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "            qvals_T = self.head(enc_T)[..., 0]  # [B, W]\n",
    "            Q_now_T = qvals_T[action_indeces]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            enc_next_T = self.encoder(obs_next_T)  # [B, W, d_enc]\n",
    "            qvals_T = self.head(enc_next_T)[..., 0]  # [B, W]\n",
    "            Q_next_T = ReduceMax(qvals_T, axis=1)  # [B]  \n",
    "            # equivalent to: Q_next_T = ReduceMax(qvals_T, axis=1)\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above; small updates are handled in the SGD step)\n",
    "            # but only update the targets that were actually selected for action at `step_now`.\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Q_next_T)) - Q_now_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        action_indeces = np.argmax(action, axis=1)  # [B]\n",
    "\n",
    "        enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "        qvals_T = self.head(enc_T)[..., 0]  # [B, W]\n",
    "\n",
    "        return qvals_T.val[action_indeces]  # [B]\n",
    "\n",
    "test_hparams['categorical_hidden_size'] = 32\n",
    "test_hparams['epoch'] = 100\n",
    "test_agent = CategoricalDQN(num_actions=test_num_actions, encoder=test_encoder, hparams=test_hparams)\n",
    "test_agent.forward(test_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
       "Turn: 1\n",
       "Winner: 0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Board:\n",
    "    \"\"\"Drafted by copilot with minor human edits\"\"\"\n",
    "\n",
    "    def __init__(self, size=7, win_length=4):\n",
    "        self.size = size\n",
    "        self.win_length = win_length\n",
    "        self.board = np.zeros((size, size))\n",
    "        self.turn = 1\n",
    "        self.winner = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.board}\\nTurn: {self.turn}\\nWinner: {self.winner}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.board == other.board\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.board.tostring())\n",
    "\n",
    "    def is_full(self):\n",
    "        return np.count_nonzero(self.board) == self.size**2\n",
    "\n",
    "    def is_empty(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return 0 <= col < self.size and self.is_empty(col)\n",
    "\n",
    "    def make_move(self, col):\n",
    "        if self.is_valid_move(col):\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-1]\n",
    "            self.board[highest_row, col] = self.turn\n",
    "            self.turn *= -1\n",
    "\n",
    "    def undo_move(self, col):\n",
    "        if self.is_valid_move(col) and self.board[0, col] != 0:\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-2]\n",
    "            self.board[highest_row, col] = 0\n",
    "            self.turn *= -1\n",
    "\n",
    "    def check_win(self) -> int:\n",
    "        for turn in [-1, 1]:\n",
    "            if self.num_connected(self.win_length, turn) > 0:\n",
    "                self.winner = turn\n",
    "                return True\n",
    "        return self.winner\n",
    "\n",
    "    def num_connected(self, length, turn):\n",
    "        num_connected = 0\n",
    "        # Check horizontal\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size-length+1):\n",
    "                if np.all(self.board[row, col:col+length] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check vertical\n",
    "        for col in range(self.size):\n",
    "            for row in range(self.size-length+1):\n",
    "                if np.all(self.board[row:row+length, col] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(self.size-length+1):\n",
    "                if all(self.board[row+i, col+i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        # Check anti-diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(length-1, self.size):\n",
    "                if all(self.board[row+i, col-i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        return num_connected\n",
    "\n",
    "board = Board()\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [0.16344107 0.07411118 0.34806212 0.06973554 0.33789297 0.11457544\n",
      " 0.05957138 0.64540343 0.14173163 0.63834791]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.02425668 0.01497782 0.88053876 0.70586225 0.98850431 0.50034016\n",
      " 0.4317397  0.31498178 0.27581316 0.49590597]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.  1.  0.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.77183795 0.55414832 0.59201855 0.39255701 0.28437021 0.67313047\n",
      " 0.13450246 0.60445492 0.95498174 0.3424968 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599453\n",
      "\n",
      "Action: [0.06641599 0.60075445 0.54891958 0.33895822 0.63165055 0.35127417\n",
      " 0.30126908 0.93606875 0.79217898 0.40083998]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599453\n",
      "\n",
      "Action: [0.91670845 0.31734722 0.06376391 0.32574521 0.49608087 0.41426744\n",
      " 0.18215532 0.04562018 0.41885614 0.15741747]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.58976856 0.62236781 0.54058175 0.25631273 0.45594867 0.35600889\n",
      " 0.10274527 0.99298168 0.36666445 0.79810213]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599453\n",
      "\n",
      "Action: [0.61816062 0.39753485 0.28187224 0.23329309 0.14878968 0.71494815\n",
      " 0.97476459 0.06955204 0.46620441 0.55594315]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  1.  1.  1.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.0986122886681096\n",
      "\n",
      "Action: [0.82385634 0.5667191  0.83705074 0.38470455 0.63409736 0.77657325\n",
      " 0.14100063 0.28029887 0.11867154 0.03831778]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0. -1.  0. -1.  0.  1.  1.  1.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.791759469228055\n",
      "\n",
      "Action: [0.06576966 0.53830695 0.65745353 0.02830099 0.04566849 0.5149409\n",
      " 0.14533562 0.30948232 0.21770198 0.70613627]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0. -1.  0. -1.  0.  1.  1.  1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 3.1780538303479458\n",
      "\n",
      "Action: [0.56285474 0.54229593 0.50276761 0.15595283 0.69687816 0.66006845\n",
      " 0.19796872 0.57815493 0.62335957 0.8693746 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0. -1.]\n",
      " [ 1.  0. -1.  0. -1.  0.  1.  1.  1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -3.1780538303479458\n",
      "\n",
      "Action: [0.97905179 0.86735503 0.97623178 0.435029   0.5840034  0.577565\n",
      " 0.13689489 0.25399041 0.51760356 0.88190266]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0. -1.  0. -1.]\n",
      " [ 1.  0. -1.  0. -1.  0.  1.  1.  1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599454\n",
      "\n",
      "Action: [0.0698801  0.87898663 0.26298233 0.84673247 0.05223749 0.1005807\n",
      " 0.97210707 0.26561729 0.43355546 0.2680853 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0. -1. -1.  0. -1.]\n",
      " [ 1.  0. -1.  0. -1.  0.  1.  1.  1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599445\n",
      "\n",
      "Action: [0.24719922 0.4583653  0.71240362 0.86896593 0.54254028 0.16554443\n",
      " 0.10411486 0.94784094 0.34779651 0.23877068]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0. -1. -1.  0. -1.]\n",
      " [ 1.  0. -1.  0. -1.  0.  1.  1.  1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.38629436111989\n",
      "\n",
      "Action: [0.13466326 0.03992662 0.76367723 0.35075311 0.86754971 0.70865765\n",
      " 0.53326568 0.41194718 0.60649205 0.74428734]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0. -1. -1.  0. -1.]\n",
      " [ 1.  0. -1.  0. -1.  0.  1.  1.  1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599458\n",
      "\n",
      "Action: [0.36898605 0.28517476 0.35451376 0.20887318 0.59108729 0.75721445\n",
      " 0.7308961  0.70962231 0.71277024 0.67179488]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0. -1. -1.  0. -1.]\n",
      " [ 1.  0. -1.  0. -1.  1.  1.  1.  1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 1\n",
      "Reward: 4.0943445622220995\n",
      "\n",
      "Winner: 1\n"
     ]
    }
   ],
   "source": [
    "class BoardEnv:\n",
    "\n",
    "    def __init__(self, board_size=7, win_length=4, reward_mode: str = 'sparse'):\n",
    "        \"\"\"RL environment for Connect 4. \n",
    "\n",
    "        Args:\n",
    "            board_size (int, optional): The size of the board. Defaults to 7.\n",
    "            win_length (int, optional): The minimum connected length to win. Defaults to 4.\n",
    "            reward_mode (str, optional): One of 'sparse', 'dense_stateless', 'dense_advantage'. \n",
    "                - For 'sparse', the reward is 1 if the player has attained a connect `win_length`,\n",
    "                    and is 0 otherwise.\n",
    "                - For 'dense_stateless', the reward increases linearly with the number of N-in-a-row's\n",
    "                    for all values of N from 0 to board_size weighted logarithmically by N.\n",
    "                - For 'dense_advantage', the reward is determined by the difference between the\n",
    "                    previous and current dense reward for each player individually.\n",
    "                `reward_mode` defaults to 'sparse'.\n",
    "        \"\"\"\n",
    "        self.board_size = board_size\n",
    "        self.win_length = win_length\n",
    "        self.reward_mode = reward_mode\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        self.board = Board(self.board_size, self.win_length)\n",
    "\n",
    "        if self.reward_mode == 'dense_advantage':\n",
    "            self.prev_dense_reward = [0., 0.]\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=np.zeros_like(self._make_obs()),\n",
    "            action=np.zeros(self.board_size),\n",
    "            next_obs=self._make_obs(), \n",
    "            reward=np.array(0.), \n",
    "            done=False, \n",
    "            info=dict()\n",
    "        )\n",
    "\n",
    "    def step(self, action: np.ndarray) -> NoBatchStep:\n",
    "        \"\"\"Apply agent X's action to the board and \n",
    "        returns the next agent's timestep.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): array shaped (board_size,). The arg max\n",
    "                action index is the column where next piece is placed.\n",
    "\n",
    "        Returns:\n",
    "            tuple: NoBatchStep with values:\n",
    "                obs (np.ndarray[H, W, 2]): None\n",
    "                next_obs (np.ndarray[H, W, 2]): the next board state with\n",
    "                    self's entered squares represented in channel 0 and\n",
    "                    opponent's squares represented in channel 1\n",
    "                action (np.ndarray[board_size]): None\n",
    "                reward (float): the reward for the agent\n",
    "                    If sparse_reward is True, then reward is -1, 0, or +1.\n",
    "                    If sparse_reward is False, then reward is:\n",
    "                        ego_dense_reward - opponent_dense_reward.\n",
    "                done (bool): whether the game is over\n",
    "                info (dict): extra information\n",
    "        \"\"\"\n",
    "        # Apply action\n",
    "        action_index = np.argmax(action)  # []\n",
    "        self.board.make_move(action_index)  # this flips `board.turn`\n",
    "        # Temporarily unflip `board.turn`\n",
    "        self.board.turn *= -1\n",
    "\n",
    "        # Make egocentric observation\n",
    "        obs = self._make_obs()\n",
    "\n",
    "        # Compute reward\n",
    "        # This is the lazy way to do it, but it's fast enough\n",
    "\n",
    "        # Sparse reward\n",
    "        winner = self.board.check_win()\n",
    "        sparse_reward = self.board.turn * winner\n",
    "\n",
    "        # Dense reward\n",
    "        def dense_reward_for_turn(board, turn):\n",
    "            r = 0\n",
    "            for length in range(2, self.board_size):\n",
    "                r += math.log(length) * board.num_connected(length, turn)\n",
    "            return r\n",
    "        ego_dense_reward = dense_reward_for_turn(self.board, self.board.turn)\n",
    "        opponent_dense_reward = dense_reward_for_turn(self.board, -self.board.turn)\n",
    "        dense_reward = ego_dense_reward - opponent_dense_reward  \n",
    "\n",
    "        if self.reward_mode == 'sparse':\n",
    "            reward = sparse_reward\n",
    "        elif self.reward_mode == 'dense_stateless':\n",
    "            reward = dense_reward\n",
    "        elif self.reward_mode == 'dense_advantage':\n",
    "            turn_index = (self.board.turn+1)//2\n",
    "            reward = dense_reward - self.prev_dense_reward[turn_index]\n",
    "            self.prev_dense_reward[turn_index] = dense_reward\n",
    "        else:\n",
    "            raise ValueError(f'Invalid reward_mode: {self.reward_mode}')\n",
    "\n",
    "        # Evaluate whether game is over\n",
    "        winner = self.board.check_win()\n",
    "        done = winner != 0\n",
    "\n",
    "        # Record debugging info\n",
    "        info = dict()\n",
    "\n",
    "        # Revert temporary flip on `board.turn`\n",
    "        self.board.turn *= -1\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=None,\n",
    "            next_obs=obs, \n",
    "            action=None,\n",
    "            reward=reward, \n",
    "            done=done, \n",
    "            info=info\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "    def _make_obs(self) -> np.ndarray:\n",
    "        \"\"\"Only show ego values on first channel and opponent values on second channel\"\"\"\n",
    "        obs = np.stack([\n",
    "            self.board.turn * self.board.board, \n",
    "            -self.board.turn * self.board.board\n",
    "            ], axis=-1)  # [board_size, board_size, 2]\n",
    "        obs[obs<0] = 0  # rectify negative values\n",
    "        return obs\n",
    "\n",
    "board_size = 10\n",
    "env = BoardEnv(board_size=board_size, win_length=5, reward_mode='dense_advantage')\n",
    "\n",
    "step = env.reset()\n",
    "while not step.done:\n",
    "    action = np.random.uniform(0, 1, (board_size,))\n",
    "    step = env.step(action)\n",
    "\n",
    "    print(f'Action: {action}')\n",
    "    env.render()\n",
    "    print(f'Reward: {step.reward}\\n')\n",
    "\n",
    "print(f'Winner: {env.board.winner}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwiredConnect4Agent(Agent):\n",
    "\n",
    "    def __init__(self, board_size: int, hparams: dict):\n",
    "        self.board_size = board_size\n",
    "        self.hparams = hparams\n",
    "        super(HardwiredConnect4Agent, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        B = obs.shape[0]\n",
    "        action = np.zeros(B, dtype=np.int32)\n",
    "        for b in range(B):\n",
    "            o = obs[b]\n",
    "            ## TODO: make a greedy agent\n",
    "            action[b] = random.randint(0, self.board_size-1)\n",
    "        return action\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-858f9c672013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msingle_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m trainer = ParallelTrainer(\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     callbacks=[\n",
      "\u001b[0;32m<ipython-input-195-d1db2db5fdc9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, agents, env, test_env, buffers, collect_driver, test_driver, histories)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_steps_per_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mcollect_trajs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollect_trajs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0magent_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-194-6c4e2f66d388>\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self, agents, env)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprev_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# `Environment.step` produces a Step with all fields except `step.obs` set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_obs\u001b[0m  \u001b[0;31m# the current agent's observation is the previous agent's next observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprev_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m  \u001b[0;31m# the reward for the action the current agent just took\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-191-e85249ec71b0>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_no_batch_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbatched_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-0b1ba6601b52>\u001b[0m in \u001b[0;36mfrom_no_batch_axis\u001b[0;34m(step)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_no_batch_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNoBatchStep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         return Step(\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "hparams = dict(\n",
    "    hidden_size=256,                # hidden layer size for RealDQN\n",
    "    categorical_hidden_size=32,     # hidden layer size for CategoricalDQN\n",
    "    activation=Relu,                # activation function for networks\n",
    "    optimizer=SGD(1e-3),            # optimizer for networks\n",
    "    epsilon_start=1.0,              # Starting value for epsilon\n",
    "    epsilon_decay=0.95,             # Decay rate for epsilon per epoch\n",
    "    min_epsilon=0.01,               # Final value for epsilon\n",
    "    discount=0.99,                  # Discount factor\n",
    "    epoch=0,                        # Current epoch\n",
    "    epochs=10,                      # Number of training epochs\n",
    "    board_size=8,                   # Board size\n",
    "    train_win_length=4,             # Number of pieces in a row needed to win in training\n",
    "    test_win_length=4,              # Number of pieces in a row needed to win in testing\n",
    "    min_steps_per_epoch=1000,       # Minimum number of steps per epoch\n",
    "    batch_size=32,                  # Number of samples per training batch\n",
    ")\n",
    "\n",
    "encoder = Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    AxisMaxPooling(1),\n",
    "])  # [B, H, W, 2] -> [B, W, d_enc]\n",
    "agent = RealDQN(hparams['board_size'], encoder, hparams)\n",
    "\n",
    "self_play_agents = dict(\n",
    "    Bob=agent,\n",
    "    Alice=agent,\n",
    ")\n",
    "\n",
    "single_env = BoardEnv(board_size=hparams['board_size'], win_length=hparams['win_length'], \n",
    "                      reward_mode='dense_stateless')\n",
    "env = ParallelEnv(hparams['batch_size'], lambda: single_env)\n",
    "\n",
    "trainer = ParallelTrainer(\n",
    "    hparams=hparams,\n",
    "    callbacks=[\n",
    "        PrintCallback(hparams=hparams, print_hparam_keys=['epoch'], print_data_keys=['reward']),\n",
    "        QEvalCallback(eval_on_collect=True, eval_on_train=True, eval_on_test=True)\n",
    "    ]).train(self_play_agents, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_pool = dict()\n",
    "agent_pool = dict()\n",
    "env_pool = dict()\n",
    "\n",
    "Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    AxisMaxPooling(1),\n",
    "])\n",
    "\n",
    "def new_encoder() -> Sequential:\n",
    "    layers = []\n",
    "    print('\\n')\n",
    "    num_layers = int(input('Enter number of conv layers (int): ').strip())\n",
    "    for _ in range(num_layers):\n",
    "        filters = eval(input('Enter number of filters (int): ').strip())\n",
    "        kernel_size = eval(input('Enter kernel size (int|2-tuple, odd): ').strip())\n",
    "        stride = eval(input('Enter stride (int|2-tuple): ').strip())\n",
    "        activation = eval(input('Enter activation function ' \n",
    "                                '(`Relu`, `Sigm`, `Tanh`, `Linear`): ').strip())\n",
    "        layers.append(Conv2D(filters, kernel_size, stride, 'same', activation))\n",
    "    layers.append(AxisMaxPooling(1))\n",
    "    encoder = Sequential(layers)\n",
    "\n",
    "    encoder_name = input('Enter encoder name: ').strip()\n",
    "    encoder_pool[encoder_name] = encoder\n",
    "    return encoder\n",
    "\n",
    "def get_encoder() -> Sequential:\n",
    "    print('\\n')\n",
    "    print('All encoders:')\n",
    "    for name, encoder in encoder_pool.items():\n",
    "        print(f'{name}: {encoder}')\n",
    "    print('\\n')\n",
    "    encoder_name = input('Enter encoder name (or enter \"new\"): ').strip().lower()\n",
    "    if encoder_name == 'new':\n",
    "        encoder = new_encoder()\n",
    "    else:\n",
    "        encoder = encoder_pool[encoder_name]\n",
    "    return encoder\n",
    "\n",
    "def new_agent() -> Tuple[str, Agent]:\n",
    "    print('\\n')\n",
    "    agent_type = input('Enter agent type (Random, RealDQN, or CategoricalDQN): ').strip().lower()\n",
    "    if agent_type == 'Random':\n",
    "        num_actions = eval(input('Enter num_actions (int): ').strip())\n",
    "        agent = RandomAgent(num_actions=num_actions)\n",
    "    elif agent_type == 'RealDQN':\n",
    "        num_actions = eval(input('Enter num_actions (int): ').strip())\n",
    "        _, encoder = get_encoder()\n",
    "        agent = RealDQN(num_actions=num_actions, encoder=encoder, hparams=hparams)\n",
    "    elif agent_type == 'CategoricalDQN':\n",
    "        num_actions = eval(input('Enter num_actions (int): ').strip())\n",
    "        _, encoder = get_encoder()\n",
    "        agent = CategoricalDQN(num_actions=num_actions, encoder=encoder, hparams=hparams)\n",
    "\n",
    "    agent_name = input('Please name this agent: ').strip().lower()\n",
    "    agent_pool[agent_name] = agent\n",
    "    return agent_name, agent\n",
    "\n",
    "def get_agent() -> Tuple[str, Agent]:\n",
    "    print('\\n')\n",
    "    print('All agents:')\n",
    "    for name, agent in agent_pool.items():\n",
    "        print(f'{name}: {agent} (made for board size = {agent.num_actions})')\n",
    "    print('\\n')\n",
    "    agent_name = input('Enter agent name (or enter \"new\"): ').strip().lower()\n",
    "    if agent_name == 'new':\n",
    "        agent_name, agent = new_agent()\n",
    "    else:\n",
    "        agent = agent_pool[agent_name]\n",
    "    return agent_name, agent\n",
    "\n",
    "def new_env() -> Tuple[str, ParallelEnv]:\n",
    "    print('\\n')\n",
    "    \n",
    "    board_size = int(input('Enter board size (int): ').strip())\n",
    "    win_length = int(input('Enter train win length (int): ').strip())\n",
    "    reward_mode = input('Enter reward mode (`sparse`, `dense_stateless`, or `dense_advantage`): ').strip().lower()\n",
    "    batch_size = int(input('Enter number of batches (int): ').strip())\n",
    "    \n",
    "    psuedo_env = dict(\n",
    "        board_size=board_size, \n",
    "        win_length=win_length, \n",
    "        reward_mode=reward_mode,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    env_name = input('Please name this environment: ').strip().lower()\n",
    "    env_pool[env_name] = psuedo_env\n",
    "    return env_name, psuedo_env\n",
    "\n",
    "def get_env() -> Tuple[str, ParallelEnv]:\n",
    "    print('\\n')\n",
    "    print('All environments:')\n",
    "    for name, env in env_pool.items():\n",
    "        print(f'{name}: {env}')  # list psuedo_env attributes\n",
    "    print('\\n')\n",
    "    env_name = input('Enter environment name (or enter \"new\"): ').strip().lower()\n",
    "    if env_name == 'new':\n",
    "        env_name, env = new_env()\n",
    "    else:\n",
    "        env = env_pool[env_name]\n",
    "    return env_name, env\n",
    "\n",
    "def train():\n",
    "\n",
    "    train_env_name, train_env_dict = get_env()\n",
    "    test_env_name, test_env_dict = get_env()\n",
    "\n",
    "    if train_env_dict['board_size'] != test_env_dict['board_size']:\n",
    "        raise ValueError('Train and test environments must have the same board size.')\n",
    "\n",
    "    # convert psuedo env's to full parallel env\n",
    "    train_env = ParallelEnv(\n",
    "        batch_size=train_env_dict['batch_size'], \n",
    "        env_init_fn=lambda : BoardEnv(\n",
    "            board=train_env_dict['board_size'],\n",
    "            win_length=train_env_dict['win_length'],\n",
    "            reward_mode=train_env_dict['reward_mode'],\n",
    "        ))\n",
    "    test_env = ParallelEnv(\n",
    "        batch_size=test_env_dict['batch_size'], \n",
    "        env_init_fn=lambda : BoardEnv(\n",
    "            board=test_env_dict['board_size'],\n",
    "            win_length=test_env_dict['win_length'],\n",
    "            reward_mode=test_env_dict['reward_mode'],\n",
    "        ))\n",
    "\n",
    "    agent1_name, agent1 = get_agent()\n",
    "    agent2_name, agent2 = get_agent()\n",
    "\n",
    "    if agent1.num_actions != agent2.num_actions:\n",
    "        raise ValueError('Agents must have the same number of actions: '\n",
    "                         '{agent1_name} has {agent1.num_actions}, '\n",
    "                         '{agent2_name} has {agent2.num_actions}.')\n",
    "\n",
    "    trainer = ParallelTrainer(hparams,\n",
    "        callbacks=[\n",
    "            PrintCallback(hparams=hparams, print_hparam_keys=['epoch'], print_data_keys=['reward']),\n",
    "            QEvalCallback(eval_on_collect=True, eval_on_train=True, eval_on_test=True)\n",
    "        ])\n",
    "\n",
    "    train_start_time = datetime.datetime.now()\n",
    "    print(f'Training {agent1_name} against {agent2_name} with train_env={train_env_name} test_env={test_env_name}...')\n",
    "    history = trainer.train(agent1, agent2, train_env, test_env)\n",
    "    train_finish_time = datetime.datetime.now()\n",
    "    print(f'Training finished. Elapsed time = {train_finish_time-train_start_time}')\n",
    "    print(history)\n",
    "\n",
    "    print('\\n')\n",
    "    if input('Save history? (y/N):').strip().lower() == 'y':\n",
    "        fname = input('Enter filename: ').strip()\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "        print(f'Saved pickle dump to {fname}')\n",
    "\n",
    "def play():\n",
    "\n",
    "    def is_human_first(human_name, agent_name) -> bool:\n",
    "        first_agent = input(f'Who goes first ({human_name}, {agent_name})? ').strip()\n",
    "        if first_agent == human_name:\n",
    "            return True\n",
    "        elif first_agent == agent_name:\n",
    "            return False\n",
    "        else:\n",
    "            print(f'{first_agent} is not a valid option.')\n",
    "            return is_human_first(human_name, agent_name)\n",
    "\n",
    "    human_name = input('Enter your name: ').strip()\n",
    "    agent_name, agent = get_agent()\n",
    "    human_first = is_human_first(human_name, agent_name)\n",
    "\n",
    "    _, env_dict = get_env()\n",
    "    env = BoardEnv(\n",
    "        board=env_dict['board_size'],\n",
    "        win_length=env_dict['win_length'],\n",
    "        reward_mode='sparse')\n",
    "    batch_env = Batch1Env(env)  # to handle executing single actions\n",
    "\n",
    "    step = env.reset()\n",
    "    if human_first:\n",
    "        env.render()\n",
    "        step.action = int(input(f'Enter move (0-{env.board_size}): ').strip())\n",
    "        step = batch_env.step(step)\n",
    "    while True:\n",
    "        step.action = agent.act(step)\n",
    "        step = batch_env.step(step)\n",
    "        if step.done:\n",
    "            winner = agent_name if step.reward > 0 else human_name\n",
    "            break\n",
    "        env.render()\n",
    "        step.action = int(input(f'Enter move (0-{env.board_size}): ').strip())\n",
    "        step = batch_env.step(step)\n",
    "        if step.done:\n",
    "            winner = human_name if step.reward > 0 else agent_name\n",
    "            break\n",
    "\n",
    "    # announce winner\n",
    "    print(f'{winner} wins!\\n')\n",
    "\n",
    "    # play again?\n",
    "    if input('Play again? (y/N):').strip().lower() == 'y':\n",
    "        play()\n",
    "\n",
    "\n",
    "def main_menu():\n",
    "    print(128*'\\n')\n",
    "    print('1. Train')\n",
    "    print('2. Play')\n",
    "    print('3. Exit')\n",
    "    print('\\n')\n",
    "\n",
    "    choice = input('Enter your choice: ')\n",
    "    if choice == '1':\n",
    "        train()\n",
    "    elif choice == '2':\n",
    "        play()\n",
    "    elif choice == '3':\n",
    "        exit()\n",
    "    else:\n",
    "        print('Invalid choice')\n",
    "\n",
    "    _ = input('\\nPress enter to continue...')\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('CSE 4309 Machine Learning Project 7++')\n",
    "    print('Copyright 2021 Jacob Valdez. Released under the MIT License')\n",
    "    print('Code at https://github.com/JacobFV/assignment-rl')\n",
    "    print('\\n')\n",
    "    while True:\n",
    "        try:\n",
    "            main_menu()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('\\n')\n",
    "            print('An error occurred. Please try again.')\n",
    "            print('Please report issues on github at https://github.com/JacobFV/assignment-rl')\n",
    "            print('\\n')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "605fe966a75bc2c3dfa708e269323e6491854b30a36f4e77953579e94649bfba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
