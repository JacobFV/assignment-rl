{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from typing import Tuple, Optional, Union, NamedTuple, Callable\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jnumpy: Jacob's numpy library for machine learning\n",
    "# Copyright (c) 2021 Jacob F. Valdez. Released under the MIT license.\n",
    "\n",
    "\n",
    "V = np.array # V is for Value type\n",
    "Vs = Tuple[V]\n",
    "Vss = Union[V,Vs]\n",
    "\n",
    "\n",
    "class ExecutionMode:\n",
    "    EAGER=1\n",
    "    STATIC=2  # STATIC execution mode not supported\n",
    "    \n",
    "EXECUTION_MODE = ExecutionMode.EAGER\n",
    "\n",
    "\n",
    "class T:\n",
    "    \"\"\"Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, val: Optional[V] = None):\n",
    "        self.val = val\n",
    "        \n",
    "        if val is None:\n",
    "            raise 'STATIC execution mode not supported'\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return eval(\"Add\")(self, other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return eval('Neg')(self)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return eval(\"Sub\")(self, other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return eval(\"Mul\")(self, other)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        return eval(\"Pow\")(self, other)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        return eval(\"MatMul\")(self, other)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return eval(\"Index\")(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        raise NotImplementedError('slice assign not yet supported')\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.val.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.val.ndim\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.val.size\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.val.dtype\n",
    "\n",
    "    @property\n",
    "    def T(self, axes: Optional[Tuple[int]] = None):\n",
    "        return eval(\"Transpose\")(self, axes=axes)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.val == other.val\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.val)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.val)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.val)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.val.__getstate__()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.val = state\n",
    "\n",
    "    def __array__(self):\n",
    "        return self.val.__array__()\n",
    "\n",
    "\n",
    "Ts = Tuple[T]\n",
    "Tss = Union[T,Ts]\n",
    "\n",
    "\n",
    "class Var(T):\n",
    "    \"\"\"Variable Tensor\"\"\"\n",
    "    def __init__(self, val: Optional[V] = None, trainable: bool = True):\n",
    "        \n",
    "        self.trainable = trainable\n",
    "        super().__init__(val=val)\n",
    "\n",
    "\n",
    "class Op(T):\n",
    "    \"\"\"Operation-backed Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, *inputs: T):\n",
    "        \"\"\"Make sure to set any variables you might need in `forward` \n",
    "        before initializing when the graph is in eager execution mode\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_ts = inputs\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.EAGER:\n",
    "            val = self.forward(tuple(i.val for i in inputs))[0]\n",
    "        else:\n",
    "            val = None\n",
    "        \n",
    "        super().__init__(val=val)\n",
    "        \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "\n",
    "\n",
    "class Transpose(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axes: Optional[Tuple[int]] = None):\n",
    "        \n",
    "        self.forward_kwargs = dict()\n",
    "        self.reverse_kwargs = dict()\n",
    "        \n",
    "        if axes is not None:\n",
    "            self.forward_kwargs['axes'] = axes\n",
    "            self.reverse_kwargs['axes'] = tuple(reversed(axes))\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.transpose(**self.forward_kwargs)\n",
    "        \n",
    "        return (Y,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dY = top_grads[0]\n",
    "        \n",
    "        dX = dY.transpose(**self.reverse_kwargs)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Reshape(Op):\n",
    "    \n",
    "    def __init__(self, t: T, shape: Tuple[int]):\n",
    "        \n",
    "        self.reshape_shape = shape\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.reshape(self.reshape_shape)\n",
    "        \n",
    "        return (Y,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dY = top_grads[0]\n",
    "        \n",
    "        dX = dY.reshape(tuple(reversed(self.reshape_shape)))\n",
    "        \n",
    "        return (dX,)\n",
    " \n",
    "\n",
    "class Concat(Op):\n",
    "    \n",
    "    def __init__(self, ts: list[T], axis: int=0):\n",
    "        \"\"\"Concatenates input tensors along an axis\n",
    "\n",
    "        Args:\n",
    "            t (T): [description]\n",
    "            axis (int, optional): Axis to concatenate along. Defaults to 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.axis = axis\n",
    "        self.orig_axis_lens = [t.shape[axis] for t in ts]\n",
    "\n",
    "        super().__init__(*ts)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        Xs = inputs\n",
    "        \n",
    "        Y = np.concatenate(Xs, axis=self.axis)\n",
    "        \n",
    "        return (Y,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dY = top_grads[0]\n",
    "        \n",
    "        dXs = np.split(dY, self.orig_axis_dims, axis=self.axis)[0]\n",
    "        \n",
    "        return dXs\n",
    "\n",
    "\n",
    "class Index(Op):\n",
    "    \n",
    "    def __init__(self, t: T, indices):\n",
    "        \"\"\"Slices a tensor along all axes.\n",
    "\n",
    "        Args:\n",
    "            t (T): The tensor to slice\n",
    "            indices (Tuple[slice]):  The partial or full indices to slice on `t`.\n",
    "                Can be an index, single slice, tuple of slices, or Ellipsis.\n",
    "                `None` is not allowed.\n",
    "        \"\"\"\n",
    "        if not isinstance(indices, tuple):\n",
    "            indices = (indices,)\n",
    "\n",
    "        self.indices = indices\n",
    "        \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X[self.indices]\n",
    "        \n",
    "        return (Y,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grads[0]\n",
    "        \n",
    "        dX = np.zeros(X.shape)\n",
    "        dX[self.indices] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceSum(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.sum_axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.sum(axis=self.sum_axis)\n",
    "        \n",
    "        return (Y,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grads[0]\n",
    "        \n",
    "        dX = np.repeat(\n",
    "            np.expand_dims(dY, axis=self.sum_axis),\n",
    "            X.shape[self.sum_axis],\n",
    "            axis=self.sum_axis\n",
    "        )\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class NaN2Num(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.nan_to_num(X, posinf=10., neginf=-10.)\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = np.nan_to_num(dZ, posinf=10., neginf=-10.)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Linear(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class StopGrad(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = np.zeros_like(dZ)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Neg(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = -X\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = -dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Add(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X + Y\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Sub(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X - Y\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = -dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Mul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X * Y\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = Y * dZ\n",
    "        dY = X * dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class MatMul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X @ Y\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        X = inputs[0]  # [A,B]\n",
    "        Y = inputs[1]  # [B,C]\n",
    "        dZ = top_grads[0]  # [A,C]\n",
    "        \n",
    "        dX = dZ @ Y.transpose()\n",
    "        dY = X.transpose() @ dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Exp(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.exp(X)\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        Z = outputs[0]\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = Z * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Sigm(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = 1 / (1 + np.exp(-X))\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        Z = outputs[0]\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = Z * (1 - Z) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Tanh(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.tanh(X)\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        Z = outputs[0]\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = ((1 - Z)**2) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Relu(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X > 0) * X\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = (X > 0) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Threshold(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X >= 0)\n",
    "        \n",
    "        return (Z,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        dZ = top_grads[0]\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Pow(Op):\n",
    "    \n",
    "    def __init__(self, x: T, power: int):\n",
    "        \n",
    "        self.power = power\n",
    "        \n",
    "        super().__init__(x)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        \n",
    "        Y = X ** p\n",
    "        \n",
    "        return (Y,)\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, outputs: Vs, top_grads: Vs) -> Vs:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        dY = top_grads[0]\n",
    "        \n",
    "        dY = p * X ** (p-1) * dY\n",
    "        dY = np.nan_to_num(dY, posinf=10., neginf=-10.)\n",
    "        \n",
    "        return (dY,)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, lr: float = 0.001):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.debug = False\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.STATIC:\n",
    "            raise 'STATIC execution mode not enabled'\n",
    "        \n",
    "        self.bprop(t_out=t, output_grad=-np.ones_like(t.val))\n",
    "        \n",
    "    def bprop(self, t_out: T, output_grad: V):\n",
    "        \n",
    "        output_grad = np.nan_to_num(output_grad, posinf=10., neginf=-10.)\n",
    "        \n",
    "        assert isinstance(t_out, (Var, Op))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f'bp {t_out} output_grads:')\n",
    "            print(output_grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        This approach does not efficiently handle weights that are consumed by multiple nodes\n",
    "        It would be better to treat backpropagation from a spreading-network-delta perspective\n",
    "        than assume everything is a tree (That's also how I should do STATIC execution refresh)\n",
    "        This should still work though, but it's just going to set the same weight multiple times\n",
    "        for every downstream consumer.\n",
    "\n",
    "        Actually, the whole thesis of minibatch gradient descent is that we can approximate a global\n",
    "        gradient by updates on local subsets of data, so it might be sufficient to leave the code\n",
    "        as is.\n",
    "        \"\"\"\n",
    "        \n",
    "        # iteratively called\n",
    "        if isinstance(t_out, Var):\n",
    "            if t_out.trainable:\n",
    "                #print('output_grad', output_grad.shape)\n",
    "                if self.debug:                    \n",
    "                    print('t_out.val (old)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "                \n",
    "                # yucky duct tape to handle batch size differences\n",
    "                if t_out.shape[0] == 1 and output_grad.shape[0] > 1:\n",
    "                    output_grad = np.sum(output_grad, axis=0)[None, ...]\n",
    "\n",
    "                t_out.val = t_out.val + (self.lr * output_grad)\n",
    "                if self.debug:\n",
    "                    print('t_out.val (new)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "            \n",
    "        elif isinstance(t_out, Op):\n",
    "            input_grads = t_out.reverse_grad(\n",
    "                inputs=tuple(t.val for t in t_out.input_ts),\n",
    "                outputs=(t_out.val,), \n",
    "                top_grads=(output_grad,))\n",
    "            \n",
    "            for input_t, input_grad in zip(t_out.input_ts, input_grads):\n",
    "                self.bprop(t_out=input_t, output_grad=input_grad)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._built = False\n",
    "        self._loss = Var(np.zeros(()), trainable=False)\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return self._loss\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> list[T]:\n",
    "        pass\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, X_T: T) -> T:\n",
    "        if not self._built:\n",
    "            self.build(X_T.shape)\n",
    "            self._built = True\n",
    "\n",
    "        # reset the regularization loss\n",
    "        self._loss = Var(0, trainable=False)\n",
    "\n",
    "        return self.forward(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.86864566, 0.92713307, 0.89508585, 0.75241702, 0.6668152 ],\n",
       "        [0.57758617, 0.1520879 , 0.00955446, 0.69276964, 0.02875664],\n",
       "        [0.22569341, 0.01265441, 0.03056132, 0.74256083, 0.73818295]]),\n",
       " array([[-0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "          0.10104463, -0.        , -0.        ,  0.04244365,  0.0863988 ],\n",
       "        [ 0.02917887,  0.0353168 , -0.        , -0.        ,  0.02639938,\n",
       "          0.05926357,  0.04120708,  0.00860686,  0.03574176,  0.03599362],\n",
       "        [ 0.05351079,  0.07113467, -0.        , -0.        ,  0.00068787,\n",
       "          0.01867601,  0.0282362 ,  0.00126863,  0.04392415,  0.02155162]]),\n",
       " Tensor(0.007903155590653417),\n",
       " [Tensor([[-0.02457278  0.00993248 -0.0021894  -0.01877936  0.02044519  0.04260977\n",
       "     0.0198656   0.00239152 -0.0216521   0.03488872]\n",
       "   [-0.03009463 -0.03818426 -0.01805062 -0.02704085 -0.04051698  0.03558481\n",
       "    -0.00900563  0.02351372  0.02355069  0.04047119]\n",
       "   [-0.02608792 -0.04711397  0.00038614  0.02850588 -0.0394825   0.02277936\n",
       "    -0.03860523 -0.04368405 -0.01020666  0.0072022 ]\n",
       "   [ 0.03179814 -0.00204674  0.032379   -0.02694689  0.00193482  0.04130591\n",
       "    -0.01823015 -0.01088275  0.00919188 -0.03272374]\n",
       "   [ 0.01473513  0.04944911 -0.02323207 -0.04415429 -0.0330314  -0.03265591\n",
       "    -0.00777718 -0.00247881  0.00507968  0.0069862 ]]),\n",
       "  Tensor([[ 0.02574554  0.03583339 -0.03889974 -0.04147285  0.02073938  0.00134668\n",
       "     0.04432442  0.0116773   0.03824953  0.03208749]])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "        units: int, \n",
    "        activation: Op = None, \n",
    "        use_bias: bool = True,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None\n",
    "    ):\n",
    "        super(Dense, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "            \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> list[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(input_shape[-1], self.units))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.units))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # compute presynaptic input\n",
    "        Z_T = X_T @ self.W_T\n",
    "\n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply activation\n",
    "        Y_T = self.activation(Z_T)\n",
    "        \n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Dense(10, Relu, 0.1, 0.1, 0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(3, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.val, Y_T.val, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (2, 7, 70, 5)\n",
      "padded (2, 27, 90, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 7, 70, 5),\n",
       " (2, 7, 70, 64),\n",
       " Tensor(0.6655775763224356),\n",
       " [Tensor([[ 0.04319845  0.03023083  0.00187009 ... -0.04315947  0.02107388\n",
       "     0.03148008]\n",
       "   [ 0.02245594 -0.00383444  0.02949396 ... -0.03407175  0.02240345\n",
       "     0.04264184]\n",
       "   [ 0.00967554  0.00856563 -0.01957414 ... -0.04670813  0.01688541\n",
       "    -0.00793862]\n",
       "   ...\n",
       "   [-0.02169781  0.01405197 -0.00351415 ...  0.01277215 -0.01822374\n",
       "     0.00560085]\n",
       "   [-0.03846189  0.01834757 -0.01413276 ... -0.04137921 -0.04816662\n",
       "    -0.00257595]\n",
       "   [-0.04592661  0.00412942 -0.01291353 ...  0.04912848  0.03650673\n",
       "    -0.03092112]])])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv2D(Layer):\n",
    "    \"\"\"Standard 2D Conv layer.\n",
    "    I.E. convolves over Tensors shaped [B, H, W, D]\n",
    "    to produce [B, H-2*kernel_size, W-2*kernel_size, filters]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        filters: int, \n",
    "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
    "        strides: Union[int, Tuple[int, int]] = 1,\n",
    "        padding: str = 'valid',  # 'valid' or 'same'\n",
    "        activation: Op = None,\n",
    "        use_bias: bool = False,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None,\n",
    "    ):\n",
    "        super(Conv2D, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1, 'kernel_size must be odd'\n",
    "        if isinstance(strides, int):\n",
    "            strides = (strides, strides)\n",
    "        assert strides[0] > 0 and strides[1] > 0, 'strides must be positive'\n",
    "        padding = padding.lower()\n",
    "        assert padding in ('valid', 'same'), 'padding must be valid or same'\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> list[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(\n",
    "            self.kernel_size[0]*self.kernel_size[1]*input_shape[-1], \n",
    "            self.filters))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.filters))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        print('input', X_T.shape)\n",
    "\n",
    "        # maybe pad input\n",
    "        if self.padding == 'same':\n",
    "\n",
    "            # various padding sizes, strides, and offsets\n",
    "            # 0   1   2   3   4\n",
    "            #     0 1 2 3 4\n",
    "            #         0 1 2 3 4 5 6 7 8 9\n",
    "            #         0 1 2 3 4\n",
    "            #         0   1   2   3   4\n",
    "\n",
    "            pad_top = self.strides[0]*(self.kernel_size[0]-1)//2\n",
    "            pad_bottom = pad_top\n",
    "            pad_left = self.strides[1]*(self.kernel_size[1]-1)//2\n",
    "            pad_right = pad_left\n",
    "            B, H_orig, W_orig, C = X_T.shape\n",
    "\n",
    "            # pad height\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, pad_top, W_orig, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, pad_bottom, W_orig, C)), trainable=False),\n",
    "            ], axis=1)\n",
    "\n",
    "            # pad width\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_left, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_right, C)), trainable=False),\n",
    "            ], axis=2)\n",
    "\n",
    "        elif self.padding == 'valid':\n",
    "            pass\n",
    "\n",
    "        # stack the input tensor along the channel axis\n",
    "        # but shifted by all possible kernel shifts\n",
    "\n",
    "        print('padded', X_T.shape)\n",
    "        #print((10*X_T[0, :, :, 0].val).astype(int))\n",
    "\n",
    "\n",
    "        stack = []\n",
    "        # THIS IS THE WRONG WAY TO DO THIS\n",
    "        # I SHOULD BE GOING BY KERNEL PIXEL INDICES INSTEAD OF INPUT PIXEL INDICES\n",
    "        # free_height = X_T.shape[1] - (self.strides[0]*self.kernel_size[0]-1)\n",
    "        # free_width = X_T.shape[2] - (self.strides[1]*self.kernel_size[1]-1)\n",
    "# \n",
    "        # for shift in itertools.product(range(free_height+1), range(free_width+1)):\n",
    "        #     start_height, start_width = shift\n",
    "        #     stack.append(\n",
    "        #         X_T[:, \n",
    "        #             start_height:start_height+self.kernel_size[0],\n",
    "        #             start_width:start_width+self.kernel_size[1],\n",
    "        #             :]\n",
    "        #     )\n",
    "\n",
    "        for shift in itertools.product(range(0, self.strides[0]*self.kernel_size[0], self.strides[0]),\n",
    "                                       range(0, self.strides[1]*self.kernel_size[1], self.strides[1])):\n",
    "            stack.append(X_T[\n",
    "                :,\n",
    "                shift[0]:,\n",
    "                shift[1]:,\n",
    "                :\n",
    "            ])\n",
    "            #     slice(None, None), \n",
    "            #     slice(\n",
    "            #         shift[0],\n",
    "            #         X_T.shape[1]-(1+self.kernel_size[0]//2)+shift[0], \n",
    "            #         self.strides[0]),\n",
    "            #     slice(\n",
    "            #         shift[1],\n",
    "            #         X_T.shape[2]-(1+self.kernel_size[1]//2)+shift[1], \n",
    "            #         self.strides[1]),\n",
    "            #     slice(None, None)\n",
    "            # ])\n",
    "            # print(shift, stack[-1].shape)\n",
    "\n",
    "        # clip stack to same dimensions\n",
    "        min_shape = np.min(np.array([s.shape for s in stack]), axis=0)\n",
    "        stack = [s[:, :min_shape[1], :min_shape[2], :] for s in stack]\n",
    "\n",
    "        # stack the shifted tensors along the channel axis\n",
    "        stacked = Concat(stack, axis=3)  # [B, H-k_h//2, W-k_w//2, C*k_h*k_w]\n",
    "\n",
    "        # convolve over the stacked tensors\n",
    "        Z_T = stacked @ self.W_T  \n",
    "    \n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply the activation function\n",
    "        Y_T = self.activation(Z_T)\n",
    "\n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Conv2D(filters=64, kernel_size=5, strides=5, padding='same', weight_L2=0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 7, 70, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3, 4, 5), (2, 60), <__main__.Var at 0x7fb12d828640>, [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Flatten(Layer):\n",
    "    \"\"\"Flattens all non-batch dimensions into a single axis\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> list[T]:\n",
    "        return []\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        flat_dims = functools.reduce(lambda x, y: x*y, X_T.shape[1:])\n",
    "        Y_T = Reshape(X_T, (X_T.shape[0], flat_dims))\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Flatten()\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 3, 4, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 3 is out of bounds for array of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c9a76574f836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m ])\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-00c9107ee8dc>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X_T)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-c9a76574f836>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_T)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_T\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mX_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-00c9107ee8dc>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X_T)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-1cc2dc1f0d04>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_T)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# stack the shifted tensors along the channel axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mstacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, H-k_h//2, W-k_w//2, C*k_h*k_w]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# convolve over the stacked tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8077b0fecb82>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ts, axis)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_axis_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforeward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8077b0fecb82>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mEXECUTION_MODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mExecutionMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8077b0fecb82>\u001b[0m in \u001b[0;36mforeward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 3 is out of bounds for array of dimension 3"
     ]
    }
   ],
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        super(Sequential, self).__init__()\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        for layer in self.layers:\n",
    "            X_T = layer(X_T)\n",
    "        return X_T\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return sum(layer.loss for layer in self.layers)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> list[T]:\n",
    "        trainable_vars = []\n",
    "        for layer in self.layers:\n",
    "            trainable_vars += layer.trainable_variables\n",
    "        return trainable_vars\n",
    "\n",
    "\n",
    "net = Sequential([\n",
    "    Dense(10, Relu),\n",
    "    Dense(128, Relu),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(X_T)\n",
    "\n",
    "img_T = Var(np.random.uniform(0, 1, size=(1, 28, 28, 1)), trainable=False)\n",
    "net = Sequential([\n",
    "    Conv2D(32, 3, 2, activation=Relu),\n",
    "    Conv2D(64, 3, 2, activation=Relu),\n",
    "    Flatten(),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(img_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standards:\n",
    "- `Step`: uses batch dimension (except `done` which is always a bool)\n",
    "- `Agent` uses batch dimension\n",
    "- `Environment` doesn't use batch dimension\n",
    "\n",
    "This means you will have to use `Step.batch` and `Step.unbatch` in your training/running loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(NamedTuple):\n",
    "    \"\"\"Single step.\"\"\"\n",
    "    \n",
    "    obs: np.ndarray\n",
    "    next_obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: np.ndarray\n",
    "    done: bool\n",
    "    info: any\n",
    "\n",
    "    @staticmethod\n",
    "    def unbatch(step: Step) -> list[Step]:\n",
    "        return [\n",
    "            Step(\n",
    "                obs=step.obs[i:i+1],\n",
    "                next_obs=step.next_obs[i:i+1],\n",
    "                action=step.action[i:i+1],\n",
    "                reward=step.reward[i:i+1],\n",
    "                done=step.done,\n",
    "                info=step.info[i:i+1],\n",
    "            )\n",
    "            for i in range(step.obs.shape[0])\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def batch(steps: list[Step]) -> Step:\n",
    "        return Step(\n",
    "            obs=np.concatenate([step.obs for step in steps], axis=0),\n",
    "            next_obs=np.concatenate([step.next_obs for step in steps], axis=0),\n",
    "            action=np.concatenate([step.action for step in steps], axis=0),\n",
    "            reward=np.concatenate([step.reward for step in steps], axis=0),\n",
    "            done=any(step.done for step in steps),\n",
    "            info=[step.info for step in steps])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_no_batch_axis(step: NoBatchStep) -> Step:\n",
    "        return Step(\n",
    "            obs=step.obs[None, ...],\n",
    "            next_obs=step.next_obs[None, ...],\n",
    "            action=step.action[None, ...],\n",
    "            reward=step.reward[None, ...],\n",
    "            done=step.done,\n",
    "            info=[step.info]\n",
    "        )\n",
    "\n",
    "# dimensional type hinting\n",
    "BatchStep = Step\n",
    "NoBatchStep = Step\n",
    "\n",
    "Traj = list[BatchStep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"RL environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        \"\"\"Resets the environment\n",
    "\n",
    "        Returns:\n",
    "            Step: Initial step. The `next_obs` attribute should be set \n",
    "                with an initial observation. `done` should be False. \n",
    "                `obs` and `action` should not be used.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action: int) -> Step:\n",
    "        \"\"\"Computes one logical step in the environment\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take.\n",
    "\n",
    "        Returns:\n",
    "            Step: Step resulting from taking `action`. The `next_obs` attribute\n",
    "                should be set with the observation resulting from taking the `action`\n",
    "                in the current environment state. `obs` should not be used. If the \n",
    "                environment is turn-based, then the reward should correspond to the \n",
    "                agent that just acted (not the next agent in line to act).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NoBatchEnv(Environment):\n",
    "    \"\"\"Environment with no batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: int) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "\n",
    "class BatchEnv(Environment):\n",
    "    \"\"\"Environment with batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> BatchStep:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch1Env(BatchEnv):\n",
    "    \"\"\"Adds a batch axis to all outgoing Steps and strips it off incoming Steps.\"\"\"\n",
    "\n",
    "    def __init__(self, env: NoBatchEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.reset())\n",
    "\n",
    "    def step(self, action: np.array) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.step(action[0]))\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEnv(BatchEnv):\n",
    "    \"\"\"Keeps reseting the same environment in a batch.\n",
    "    \n",
    "    Declares itself done when a total of `batch_size` individual environment\n",
    "    dones are experienced.\n",
    "\n",
    "    NOTE: Individual environments should be `NoBatchEnv`'s.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, env_init_fn: Callable[[], NoBatchEnv]):\n",
    "        self.batch_size = batch_size\n",
    "        self.env_init_fn = env_init_fn\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        self.dones = 0\n",
    "        self.envs = [self.env_init_fn() for _ in range(self.batch_size)]\n",
    "        steps = [env.reset() for env in self.envs]\n",
    "        steps = [Step.from_no_batch_axis(step) for step in steps]\n",
    "        return Step.batch(steps)\n",
    "\n",
    "    def step(self, action: np.array) -> Step:\n",
    "        steps = []\n",
    "        for i, (env, single_action) in enumerate(zip(self.envs, action)):\n",
    "            step = env.step(single_action)\n",
    "            if step.done:\n",
    "                self.envs[i] = self.env_init_fn()\n",
    "                new_step = env.reset()\n",
    "                step.next_obs = new_step.next_obs\n",
    "                self.dones += 1\n",
    "            steps.append(Step.from_no_batch_axis(step))\n",
    "\n",
    "        batched_step = Step.batch(steps)\n",
    "        batched_step.done = self.dones >= self.batch_size\n",
    "        return batched_step\n",
    "\n",
    "    def render(self):\n",
    "        for env in self.envs:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer.\n",
    "\n",
    "    Expects the following hyperparameters:\n",
    "        - `epoch`: The current epoch.\n",
    "        - `batch_size`: Number of trajectories to return at each call.\n",
    "        - `min_sample_len`: Minimum length of trajectories to sample.\n",
    "        - `max_sample_len`: Maximum length of trajectories to sample.\n",
    "        - `num_steps_replay_coef`: Sampling coefficient based on trajectory length.\n",
    "        - `success_replay_coef`: Sampling coefficient based on trajectory success.\n",
    "        - `age_replay_coef`: Sampling coefficient based on trajectory age.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, hparams: dict):\n",
    "        self.hparams = hparams\n",
    "        self.trajs = dict()\n",
    "\n",
    "    @property\n",
    "    def flat_traj(self):\n",
    "        return [step for traj in self.trajs for step in traj]\n",
    "\n",
    "    def add(self, traj: Traj, epoch: int):\n",
    "        \"\"\"Add a new trajectory to the buffer.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): the trajectory to add.\n",
    "            epoch (int): the epoch that the trajectory was experience in.\n",
    "        \"\"\"\n",
    "        if epoch not in self.trajs:\n",
    "            self.trajs[epoch] = []\n",
    "        self.trajs[epoch] += traj\n",
    "\n",
    "    def sample(self) -> Traj:\n",
    "        \"\"\"Samples a batched trajectory from the buffer stochastically based on:\n",
    "            - the number of steps in the trajectory (num_steps_replay_coef)\n",
    "            - how well the agent did in the trajectory (success_replay_coef)\n",
    "            - how long ago the trajectory was experienced (age_replay_coef)\n",
    "\n",
    "        Returns:\n",
    "            Traj: a trajectory of batched steps experienced.\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = {\n",
    "            epoch: self.hparams['num_steps_replay_coef'] * len(traj) +\n",
    "                   self.hparams['success_replay_coef'] * sum(step.reward for step in traj) +\n",
    "                   self.hparams['age_replay_coef'] * (epoch - self.hparams['epoch'])\n",
    "            for epoch, traj in self.trajs.items()\n",
    "        }\n",
    "        epochs = np.random.choice(list(weights.keys()), size=(), replace=True, p=list(weights.values()))\n",
    "        trajs = [self.trajs[epoch] for epoch in epochs]\n",
    "        batched_traj = [BatchStep.batch(steps) for steps in zip(*trajs)]\n",
    "        return batched_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, policy: Callable):\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, step: BatchStep) -> np.ndarray:\n",
    "        \"\"\"Generates an action for a given observation using `self.policy`. \n",
    "        Override if you want to give your policy more information such as\n",
    "        recurrent state or previous reward.\n",
    "\n",
    "        Args:\n",
    "            step (Step): last step output by the environment. This means the agent\n",
    "                should feed `step.next_obs`, not `step.obs` to its policy. If the \n",
    "                environment is multi-agent, then the `reward` attribute has already\n",
    "                updated to reflect the reward for this agent by the driver.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The action to take\n",
    "        \"\"\"\n",
    "        return self.policy(step.next_obs)\n",
    "\n",
    "    def reward(self, traj: Traj) -> float:\n",
    "        \"\"\"Evaluates the cumulative reward for your agent as the sum of \n",
    "        individual rewards experienced. \n",
    "        \n",
    "        If your agent uses intrinsic rewards, be sure to add them in here.\n",
    "        Do not introduce Q-values or predicted rewards here.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "\n",
    "        Returns:\n",
    "            float: Cumulative (sum) reward over the entire sequence.\n",
    "        \"\"\"\n",
    "        return sum(step.reward for step in traj)\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        raise NotImplemented('Method `train` must be implemented by subclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-897bf21b0558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mParallelDriver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"Drives batched turn-based `BatchedEnv` environments with multiple agents.\n\u001b[1;32m      3\u001b[0m     \u001b[0mAlso\u001b[0m \u001b[0msupports\u001b[0m \u001b[0msingle\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0magent\u001b[0m \u001b[0menvironments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mspecial\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-897bf21b0558>\u001b[0m in \u001b[0;36mParallelDriver\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTraj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \"\"\"Drives a batched environment with multiple agents.\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Agent' is not defined"
     ]
    }
   ],
   "source": [
    "class ParallelDriver:\n",
    "    \"\"\"Drives batched turn-based `BatchedEnv` environments with multiple agents.\n",
    "    Also supports single-agent environments as a special case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def drive(self, agents: dict[str, Agent], env: Environment) -> dict[str, Traj]:\n",
    "        \"\"\"Drives a batched environment with multiple agents.\n",
    "        \n",
    "        Args:\n",
    "            agents (dict[str, Agent]): A dictionary of agents to drive.\n",
    "            env (Environment): The environment to drive.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, Traj]: A dictionary of trajectories for each agent.\n",
    "                Each trajectory is completely disengaged from the other agent's.\n",
    "                (i.e.: the obs, next_obs, action, reward, done attributes are\n",
    "                individual to each agent for each trajectory.)\n",
    "        \"\"\"\n",
    "\n",
    "        names_it = itertools.cycle(agents.keys())\n",
    "        trajs = {agent_name: [] for agent_name in agents}\n",
    "        prev_rewards = {agent_name: 0. for agent_name in agents}\n",
    "\n",
    "        step = env.reset()\n",
    "        while not step.done:\n",
    "            agent_name = next(names_it)\n",
    "            \n",
    "            # `Agent.forward` only looks at `step.next_obs` and `step.reward`\n",
    "            # but I'm assigning defaults just to be safe.\n",
    "            action = agents[agent_name].forward(Step(\n",
    "                obs=step.obs,  # what the previous agent saw before acting\n",
    "                next_obs=step.next_obs,  # what the current agent sees before acting\n",
    "                reward=prev_rewards[agent_name],  # the reward this agent experienced following its last action\n",
    "                done=step.done,  # whether the environment was done after the previous agent acted\n",
    "                info=step.info  # any extra information the environment might have output\n",
    "            )) \n",
    "\n",
    "            prev_step = step\n",
    "            step = env.step(action)  # `Environment.step` produces a Step with all fields except `step.obs` set\n",
    "            step.obs = prev_step.next_obs  # the current agent's observation is the previous agent's next observation\n",
    "            prev_rewards[agent_name] = step.reward  # the reward for the action the current agent just took\n",
    "            trajs[agent_name].append(step)  # Step completely corresponding to this agent (obs before action, obs after action, action, reward, done, info)\n",
    "        \n",
    "        return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTrainer:\n",
    "    \"\"\"Trains `BatchedEnv` environments and mutliple agents \n",
    "    (with N=1 single-agent supported as a special case).\n",
    "    \n",
    "    Uses following hyperparameters:\n",
    "    - `epoch`: the current epoch. Reads and writes to this variable.\n",
    "    - `epochs`: the number of epochs to train for.\n",
    "    - `min_steps_per_epoch`: the minimum number of steps to train for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams: dict, callbacks: list[Callable]):\n",
    "        self.hparams = hparams\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "    def train(self, \n",
    "        agents: dict[str, Agent], \n",
    "        env: Environment,\n",
    "        test_env: Environment = None,\n",
    "        buffers: dict[str, ReplayBuffer] = None,\n",
    "        collect_driver: ParallelDriver = None,\n",
    "        test_driver: ParallelDriver = None,\n",
    "        histories: dict[str, dict[int, dict[str, any]]] = None,\n",
    "        ) -> dict[int, dict[str, any]]:\n",
    "\n",
    "        agent_names = list(agents.keys())\n",
    "        \n",
    "        # initialize defaults\n",
    "        if test_env is None:\n",
    "            test_env = env\n",
    "        if buffers is None:\n",
    "            buffers = dict()\n",
    "        if collect_driver is None:\n",
    "            collect_driver = ParallelDriver()\n",
    "        if test_driver is None:\n",
    "            test_driver = collect_driver\n",
    "        if histories is None:\n",
    "            histories = dict()  # {agent_name: {epoch: {...data}}}\n",
    "\n",
    "        # build uninitialized agent-specific objects\n",
    "        for agent_name in agent_names:\n",
    "            if agent_name not in buffers:\n",
    "                buffers[agent_name] = ReplayBuffer()\n",
    "            if agent_name not in histories:\n",
    "                histories[agent_name] = dict()\n",
    "\n",
    "        # run training loop\n",
    "        for epoch in range(self.hparams['epoch'], self.hparams['epochs']):\n",
    "            self.hparams['epoch'] = epoch\n",
    "\n",
    "            # collect trajectories\n",
    "            steps = 0\n",
    "            while steps < self.hparams['steps_per_epoch']:\n",
    "                collect_trajs = collect_driver.drive(agents, env)\n",
    "                steps += min(len(traj) for _, traj in collect_trajs.items())\n",
    "                for agent_name in agent_names:\n",
    "                    buffers[agent_name].add(collect_trajs[agent_name])\n",
    "\n",
    "            # train\n",
    "            train_trajs = {agent_name: buffers[agent_name].sample() for agent_name in agent_names}\n",
    "            for agent_name in agent_names:\n",
    "                agents[agent_name].train(train_trajs[agent_name])\n",
    "                \n",
    "            # test\n",
    "            test_trajs = test_driver.drive(agents, env)\n",
    "\n",
    "            # record history and run callbacks\n",
    "            for agent_name in agent_names:\n",
    "                histories[agent_name][epoch] = {\n",
    "                    'epoch': epoch,\n",
    "                    'agent': agents[agent_name],\n",
    "                    'all_agents': agents,\n",
    "                    'env': env,\n",
    "                    'test_env': test_env,\n",
    "                    'collect_traj': collect_trajs[agent_name],\n",
    "                    'train_traj': train_trajs[agent_name],\n",
    "                    'test_traj': test_trajs[agent_name],\n",
    "                    'buffer': buffers[agent_name],\n",
    "                }\n",
    "                for callback in self.callbacks:\n",
    "                    callback(histories[agent_name][epoch])\n",
    "\n",
    "        return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback:\n",
    "\n",
    "    def __init__(self, hparams: dict, print_hparam_keys: list[str] = None, print_data_keys: list[str] = None):\n",
    "        if print_hparam_keys is None:\n",
    "            print_hparam_keys = ['epoch']\n",
    "        if print_data_keys is None:\n",
    "            print_data_keys = []\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.print_hparam_keys = print_hparam_keys\n",
    "        self.print_data_keys = print_data_keys\n",
    "\n",
    "    def __call__(self, data: dict[str, any]):\n",
    "        for key in self.print_hparam_keys:\n",
    "            print(f'{key}: {self.hparams[key]}', end='\\t')\n",
    "        for key in self.print_data_keys:\n",
    "            print(f'{key}: {data[key]}', end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-afd6d6bb8bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mQEvalCallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self, \n\u001b[1;32m      4\u001b[0m         \u001b[0meval_on_collect\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0meval_on_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-afd6d6bb8bac>\u001b[0m in \u001b[0;36mQEvalCallback\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_on_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'agent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q_eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "class QEvalCallback:\n",
    "\n",
    "    def __init__(self, \n",
    "        eval_on_collect: bool = True, \n",
    "        eval_on_train: bool = False, \n",
    "        eval_on_test: bool = False):\n",
    "\n",
    "        self.eval_on_collect = eval_on_collect\n",
    "        self.eval_on_train = eval_on_train\n",
    "        self.eval_on_test = eval_on_test\n",
    "\n",
    "    def __call__(self, data: dict[str, any]):\n",
    "        agent = data['agent']\n",
    "        if not hasattr(agent, 'q_eval'):\n",
    "            return\n",
    "\n",
    "        if self.eval_on_collect:\n",
    "            traj = data['collect_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_collect'] = q_val\n",
    "\n",
    "        if self.eval_on_train:\n",
    "            traj = data['train_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_train'] = q_val\n",
    "\n",
    "        if self.eval_on_test:\n",
    "            traj = data['test_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_test'] = q_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- make the reward optionally an advantage computation over last round\n",
    "- also make a recurrent DQN agent (estimate q function of a sequence of states)\n",
    "- make a simple greedy connect4 agent\n",
    "- make the preprocessor perform a columnwise mean pool before flattening\n",
    "- train the preprocessor on an auxillary objective to estimate the max connected for each length for self and for oponent\n",
    "- add padding='SAME'|'VALID' to conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    \"\"\"Takes a random action on each timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int):\n",
    "        self.policy = lambda obs : np.random.randint(0, num_actions, (obs.shape[0],))\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HPARAMS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6d271b87613f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m ])  # [B, H, W, 1] -> [B, L]\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRealDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'board_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHPARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HPARAMS' is not defined"
     ]
    }
   ],
   "source": [
    "class RealDQN(Agent):\n",
    "    \"\"\"'Classic' Deep Q-learning agent.\n",
    "    Implements the approach in https://arxiv.org/pdf/1312.5602.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder\n",
    "        self.head = Sequential([\n",
    "            Dense(512, Sigm), \n",
    "            Dense(1, lambda x: x)\n",
    "        ]) # [B, L+|A|] -> [B, 1]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(RealDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = min(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            return np.random.rand(self.num_actions)\n",
    "        \n",
    "        # Otherwise take the action with the highest Q-value\n",
    "        actions = np.arange(self.num_actions)  # [self.num_actions]\n",
    "        q_vals = np.zeros((self.num_actions,))  # [self.num_actions]\n",
    "        for i, action in enumerate(actions):\n",
    "            \n",
    "            # prepare inputs\n",
    "            obs_T = Var(obs[None, ...])  # [1, H, W, 2]\n",
    "            action_T = Var(action[None, None], trainable=False)  # [1, 1]\n",
    "\n",
    "            # run the network\n",
    "            enc_T = self.encoder(obs_T)  # [1, d_enc]\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [1, d_enc+1]\n",
    "            q_T = self.head(cat_T)  # [1, 1]\n",
    "\n",
    "            # store q-value\n",
    "            q_vals[i] = q_T.val[0,0]  # []\n",
    "        \n",
    "        # select the action with the highest Q-value\n",
    "        action = actions[q_vals.argmax()]\n",
    "        return action\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_T = Var(step.action[None], trainable=False)  # [B, 1]\n",
    "            action_next_T = Var(self.policy(step.next_obs), trainable=False)  # [B, 1]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [B, d_enc+1]\n",
    "            Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "            cat_T = Concat([enc_T, action_next_T], axis=1)  # [B, d_enc+1]\n",
    "            Qnext_T = self.head(cat_T)[:, 0]  # [B]\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above; small updates are handled in the SGD step)\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Qnext_T)) - Qnow_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        action_T = Var(action)  # [B, 1]\n",
    "        enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "        cat_T = Concat([enc_T, action_T], axis=1)  # [B, d_enc+1]\n",
    "        Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "\n",
    "        return Qnow_T.val\n",
    "\n",
    "\n",
    "encoder = Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    Flatten(),\n",
    "])  # [B, H, W, 1] -> [B, L]\n",
    "\n",
    "agent = RealDQN(num_actions=HPARAMS['board_size'], encoder=encoder, hparams=HPARAMS)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CategoricalDQN at 0x7fe1ec8b5cd0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CategoricalDQN(Agent):\n",
    "    \"\"\"Categorical deep Q-network agent.\n",
    "    I never read the paper for this architecture, so my implementation\n",
    "    might be different from the origonal researchers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder  # [B, H, W, C] -> [B, d_enc]\n",
    "        self.head = Sequential([\n",
    "            Dense(2*hparams['board_size']*num_actions, Tanh),\n",
    "            Dense(num_actions, Linear)\n",
    "        ]) # [B, d_enc] -> [B, num_actions]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(CategoricalDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = min(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            return np.random.rand(self.num_actions)\n",
    "        \n",
    "        # Otherwise take the action with the highest Q-value\n",
    "        # compute q-values for all actions\n",
    "        obs_T = Var(obs, trainable=False)  # [B, H, W, C]\n",
    "        enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "        qvals_T = self.head(enc_T)  # [B, A]\n",
    "        return np.argmax(qvals_T.val, axis=1)  # [B]\n",
    "\n",
    "    \n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            obs_next_T = Var(step.next_obs, trainable=False)  # [B, H, W, 2]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "            qvals_T = self.head(enc_T)  # [B, A]\n",
    "            Q_now_T = qvals_T[step.action]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            enc_T = self.encoder(obs_next_T)  # [B, d_enc]\n",
    "            qvals_T = self.head(enc_T)  # [B, A]\n",
    "            Q_next_T = np.max(qvals_T.val, axis=1)  # [B]\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above; small updates are handled in the SGD step)\n",
    "            # but only update the targets that were actually selected for action at `step_now`.\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Q_next_T)) - Q_now_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "        qvals_T = self.head(enc_T)  # [B, A]\n",
    "\n",
    "        return qvals_T.val[action]\n",
    "\n",
    "\n",
    "agent = CategoricalDQN(num_actions=HPARAMS['board_size'], encoder=encoder, hparams=HPARAMS)\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwiredConnect4Agent(Agent):\n",
    "\n",
    "    def __init__(self, board_size: int, hparams: dict):\n",
    "        self.board_size = board_size\n",
    "        self.hparams = hparams\n",
    "        super(HardwiredConnect4Agent, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        B = obs.shape[0]\n",
    "        action = np.zeros(B, dtype=np.int32)\n",
    "        for b in range(B):\n",
    "            o = obs[b]\n",
    "            ## TODO: make a greedy agent\n",
    "            action[b] = random.randint(0, self.board_size-1)\n",
    "        return action\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
       "Turn: 1\n",
       "Winner: 0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Board:\n",
    "    \"\"\"Drafted by copilot with minor human edits\"\"\"\n",
    "\n",
    "    def __init__(self, size=7, win_length=4):\n",
    "        self.size = size\n",
    "        self.win_length = win_length\n",
    "        self.board = np.zeros((size, size))\n",
    "        self.turn = 1\n",
    "        self.winner = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.board}\\nTurn: {self.turn}\\nWinner: {self.winner}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.board == other.board\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.board.tostring())\n",
    "\n",
    "    def is_full(self):\n",
    "        return np.count_nonzero(self.board) == self.size**2\n",
    "\n",
    "    def is_empty(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return 0 <= col < self.size and self.is_empty(col)\n",
    "\n",
    "    def make_move(self, col):\n",
    "        if self.is_valid_move(col):\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-1]\n",
    "            self.board[highest_row, col] = self.turn\n",
    "            self.turn *= -1\n",
    "\n",
    "    def undo_move(self, col):\n",
    "        if self.is_valid_move(col) and self.board[0, col] != 0:\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-2]\n",
    "            self.board[highest_row, col] = 0\n",
    "            self.turn *= -1\n",
    "\n",
    "    def check_win(self) -> int:\n",
    "        for turn in [-1, 1]:\n",
    "            if self.num_connected(self.win_length, turn) > 0:\n",
    "                self.winner = turn\n",
    "                return True\n",
    "        return self.winner\n",
    "\n",
    "    def num_connected(self, length, turn):\n",
    "        num_connected = 0\n",
    "        # Check horizontal\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size-length+1):\n",
    "                if np.all(self.board[row, col:col+length] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check vertical\n",
    "        for col in range(self.size):\n",
    "            for row in range(self.size-length+1):\n",
    "                if np.all(self.board[row:row+length, col] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(self.size-length+1):\n",
    "                if all(self.board[row+i, col+i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        # Check anti-diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(length-1, self.size):\n",
    "                if all(self.board[row+i, col-i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        return num_connected\n",
    "\n",
    "board = Board()\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 9\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: 3\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: 9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599453\n",
      "\n",
      "Action: 3\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: 9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.791759469228055\n",
      "\n",
      "Action: 8\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.791759469228055\n",
      "\n",
      "Action: 3\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.791759469228055\n",
      "\n",
      "Action: 2\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.4054651081081646\n",
      "\n",
      "Action: 8\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -2.4849066497880004\n",
      "\n",
      "Action: 2\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.4054651081081646\n",
      "\n",
      "Action: 8\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -2.484906649788001\n",
      "\n",
      "Action: 5\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 2.484906649788001\n",
      "\n",
      "Action: 0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -2.484906649788001\n",
      "\n",
      "Action: 5\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.7917594692280554\n",
      "\n",
      "Action: 3\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -2.484906649788001\n",
      "\n",
      "Action: 9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 2.484906649788001\n",
      "\n",
      "Action: 0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -3.178053830347946\n",
      "\n",
      "Action: 0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 3.178053830347946\n",
      "\n",
      "Action: 1\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -4.564348191467835\n",
      "\n",
      "Action: 9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 3.87120101090789\n",
      "\n",
      "Action: 0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -3.87120101090789\n",
      "\n",
      "Action: 0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 3.87120101090789\n",
      "\n",
      "Action: 5\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1.  0. -1.  0.  0. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -3.87120101090789\n",
      "\n",
      "Action: 4\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0.  0. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -2.4849066497880017\n",
      "\n",
      "Action: 7\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599454\n",
      "\n",
      "Action: 7\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.3862943611198908\n",
      "\n",
      "Action: 6\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599454\n",
      "\n",
      "Action: 0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.3862943611198908\n",
      "\n",
      "Action: 9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1.  0. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.3862943611198908\n",
      "\n",
      "Action: 4\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  0. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -8.435549202375729\n",
      "\n",
      "Action: 1\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 6.356107660695892\n",
      "\n",
      "Action: 2\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -9.939626599152001\n",
      "\n",
      "Action: 5\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  1.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 9.246479418592056\n",
      "\n",
      "Action: 9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  1.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  1.  0.  1.  0.  0.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -9.246479418592056\n",
      "\n",
      "Action: 7\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  1.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  1.  0.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 5.662960480135947\n",
      "\n",
      "Action: 0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  1.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  1.  0.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -7.4547199493640015\n",
      "\n",
      "Action: 4\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  1.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 3.5835189384561126\n",
      "\n",
      "Action: 9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  0.  1.  0.  1.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -4.27666611901606\n",
      "\n",
      "Action: 2\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 2.8903717578961654\n",
      "\n",
      "Action: 8\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0.  0. -1. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -4.276666119016056\n",
      "\n",
      "Action: 5\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  1.  0.  0.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0.  0. -1. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 2.4849066497880017\n",
      "\n",
      "Action: 7\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  1.  0.  0.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -4.276666119016056\n",
      "\n",
      "Action: 7\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0.  0.  0.  1.  0.  1.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 4.276666119016056\n",
      "\n",
      "Action: 3\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0. -1.  0.  1.  0.  1.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -4.276666119016056\n",
      "\n",
      "Action: 9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0.  0. -1.  0.  1.  0.  1.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 4.276666119016056\n",
      "\n",
      "Action: 2\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [-1.  0. -1. -1.  0.  1.  0.  1.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -4.969813299576003\n",
      "\n",
      "Action: 5\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  1.  0.  0.  0.  1.]\n",
      " [-1.  0. -1. -1.  0.  1.  0.  1.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1.  0. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.791759469228058\n",
      "\n",
      "Action: 1\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  1.  0.  0.  0.  1.]\n",
      " [-1.  0. -1. -1.  0.  1.  0.  1.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1. -1. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -6.068425588244107\n",
      "\n",
      "Action: 2\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  1.  0.  0.  1.  0.  0.  0.  1.]\n",
      " [-1.  0. -1. -1.  0.  1.  0.  1.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1. -1. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  0. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 6.068425588244107\n",
      "\n",
      "Action: 6\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [-1.  0.  1.  0.  0.  1.  0.  0.  0.  1.]\n",
      " [-1.  0. -1. -1.  0.  1.  0.  1.  0. -1.]\n",
      " [ 1.  0.  1.  1.  0.  1.  0. -1. -1. -1.]\n",
      " [-1. -1. -1.  1.  1.  1.  0.  1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1. -1. -1.  1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1. -1.  1.]]\n",
      "Turn: 1\n",
      "Winner: -1\n",
      "Reward: -18.1283157235962\n",
      "\n",
      "Winner: -1\n"
     ]
    }
   ],
   "source": [
    "class BoardEnv:\n",
    "\n",
    "    def __init__(self, board_size=7, win_length=4, sparse_reward=False):\n",
    "        self.board_size = board_size\n",
    "        self.win_length = win_length\n",
    "        self.sparse_reward = sparse_reward\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = Board(self.board_size, self.win_length)\n",
    "\n",
    "        return self.board.board\n",
    "\n",
    "    def step(self, action: int):\n",
    "        \"\"\"Apply agent X's action to the board and \n",
    "        returns the next agent's timestep.\n",
    "\n",
    "        Args:\n",
    "            action (int): integer in [0, board_size)\n",
    "\n",
    "        Returns:\n",
    "            tuple: NoBatchStep with values:\n",
    "                obs (np.ndarray[H, W, 2]): 0's tensor (`None` substitute)\n",
    "                next_obs (np.ndarray[H, W, 2]): the next board state with\n",
    "                    self's entered squares represented in channel 0 and\n",
    "                    opponent's squares represented in channel 1\n",
    "                reward (float): the reward for the agent\n",
    "                    If sparse_reward is True, then reward is -1, 0, or +1.\n",
    "                    If sparse_reward is False, then reward is:\n",
    "                        ego_dense_reward - opponent_dense_reward.\n",
    "                done (bool): whether the game is over\n",
    "                info (dict): extra information\n",
    "        \"\"\"\n",
    "        # Apply action\n",
    "        self.board.make_move(action)  # this flips `board.turn`\n",
    "\n",
    "        # Make egocentric observation\n",
    "        obs = (self.board.turn * self.board.board)[..., None]  # [H, W, 1]\n",
    "\n",
    "        # Compute reward\n",
    "        if self.sparse_reward:\n",
    "            winner = self.board.check_win()\n",
    "            reward = self.board.turn * winner\n",
    "        else:\n",
    "            def dense_reward(board, turn):\n",
    "                r = 0\n",
    "                for length in range(2, self.board_size):\n",
    "                    r += math.log(length) * board.num_connected(length, turn)\n",
    "                return r\n",
    "            ego_dense_reward = dense_reward(self.board, self.board.turn)\n",
    "            opponent_dense_reward = dense_reward(self.board, -self.board.turn)\n",
    "            reward = ego_dense_reward - opponent_dense_reward\n",
    "\n",
    "        # Evaluate whether game is over\n",
    "        winner = self.board.check_win()\n",
    "        done = winner != 0\n",
    "\n",
    "        # Record debugging info\n",
    "        info = dict()\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=np.zeros_like(obs),\n",
    "            next_obs=obs, \n",
    "            reward=reward, \n",
    "            done=done, \n",
    "            info=info\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "board_size = 10\n",
    "env = BoardEnv(board_size=board_size, win_length=5, sparse_reward=False)\n",
    "obs = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = random.randint(0, board_size-1)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    print(f'Action: {action}')\n",
    "    env.render()\n",
    "    print(f'Reward: {reward}\\n')\n",
    "\n",
    "print(f'Winner: {env.board.winner}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_HPARAMS = dict(\n",
    "    board_size=8,           # Board size\n",
    "    discount=0.99,          # Discount factor\n",
    "    learning_rate=0.001,    # Learning rate\n",
    "    batch_size=32,          # Number of samples per training batch\n",
    "    train_freq=10000,       # Number of timesteps between training steps\n",
    "    epoch=0,                # Current epoch\n",
    "    epochs=10,              # Number of training epochs\n",
    "    epsilon_start=1.0,      # Starting value for epsilon\n",
    "    min_epsilon=0.01,       # Final value for epsilon\n",
    "    epsilon_decay=0.95,     # Decay rate for epsilon per epoch\n",
    ")\n",
    "\n",
    "INITIAL_HPARAMS['optimizer'] = \\\n",
    "    SGD(INITIAL_HPARAMS['learning_rate'])   # Optimizer\n",
    "\n",
    "\n",
    "HPARAMS = INITIAL_HPARAMS.copy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "605fe966a75bc2c3dfa708e269323e6491854b30a36f4e77953579e94649bfba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
