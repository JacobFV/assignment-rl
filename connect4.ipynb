{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "import functools\n",
    "from typing import Tuple, List, Mapping, Optional, Union, NamedTuple, Callable\n",
    "# Many default parameters are included in jnumpy and are optional.\n",
    "# I only resort to using `Optional` in the type annotations where the\n",
    "# context does not make this clear.  \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jnumpy: Jacob's numpy library for machine learning\n",
    "# Copyright (c) 2021 Jacob F. Valdez. Released under the MIT license.\n",
    "\n",
    "\n",
    "V = np.array # V is for Value type\n",
    "Vs = Tuple[V]\n",
    "Vss = Union[V,Vs]\n",
    "\n",
    "\n",
    "class ExecutionMode:\n",
    "    EAGER=1\n",
    "    STATIC=2  # STATIC execution mode not supported\n",
    "    \n",
    "EXECUTION_MODE = ExecutionMode.EAGER\n",
    "\n",
    "\n",
    "class T:\n",
    "    \"\"\"Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, val: Optional[V] = None):\n",
    "        self.val = val\n",
    "        \n",
    "        if val is None:\n",
    "            raise 'STATIC execution mode not supported'\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return eval(\"Index\")(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        raise NotImplementedError('slice assign not yet supported')\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return eval(\"Add\")(self, other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return eval('Neg')(self)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return eval(\"Sub\")(self, other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return eval(\"Mul\")(self, other)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        return eval(\"Pow\")(self, other)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        return eval(\"MatMul\")(self, other)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.val.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.val.ndim\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.val.size\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.val.dtype\n",
    "\n",
    "    @property\n",
    "    def T(self, axes: Tuple[int] = None):\n",
    "        return eval(\"Transpose\")(self, axes=axes)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.val == other.val\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.val)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.val)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.val)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.val.__getstate__()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.val = state\n",
    "\n",
    "    def __array__(self):\n",
    "        return self.val.__array__()\n",
    "\n",
    "\n",
    "Ts = Tuple[T]\n",
    "Tss = Union[T,Ts]\n",
    "\n",
    "\n",
    "class Var(T):\n",
    "    \"\"\"Variable Tensor\"\"\"\n",
    "    def __init__(self, val: Optional[V] = None, trainable: bool = True):\n",
    "        \n",
    "        self.trainable = trainable\n",
    "        super().__init__(val=val)\n",
    "\n",
    "\n",
    "class Op(T):\n",
    "    \"\"\"Operation-backed Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, *inputs: T):\n",
    "        \"\"\"Make sure to set any variables you might need in `forward` \n",
    "        before initializing when the graph is in eager execution mode\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_ts = inputs\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.EAGER:\n",
    "            val = self.forward(tuple(i.val for i in inputs))\n",
    "        else:\n",
    "            val = None\n",
    "        \n",
    "        super().__init__(val=val)\n",
    "        \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "\n",
    "\n",
    "class Transpose(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axes: Tuple[int] = None):\n",
    "        \n",
    "        self.forward_kwargs = dict()\n",
    "        self.reverse_kwargs = dict()\n",
    "        \n",
    "        if axes is not None:\n",
    "            self.forward_kwargs['axes'] = axes\n",
    "            self.reverse_kwargs['axes'] = tuple(reversed(axes))\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.transpose(**self.forward_kwargs)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "\n",
    "        dX = dY.transpose(**self.reverse_kwargs)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Reshape(Op):\n",
    "    \n",
    "    def __init__(self, t: T, shape: Tuple[int]):\n",
    "        \n",
    "        self.reshape_shape = shape\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.reshape(self.reshape_shape)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = dY.reshape(tuple(reversed(self.reshape_shape)))\n",
    "        \n",
    "        return (dX,)\n",
    " \n",
    "\n",
    "class Concat(Op):\n",
    "    \n",
    "    def __init__(self, ts: List[T], axis: int = 0):\n",
    "        \"\"\"Concatenates input tensors along an axis\n",
    "\n",
    "        Args:\n",
    "            t (T): [description]\n",
    "            axis (int, optional): Axis to concatenate along. Defaults to 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.axis = axis\n",
    "        self.orig_axis_lens = [t.shape[axis] for t in ts]\n",
    "\n",
    "        super().__init__(*ts)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        Xs = inputs\n",
    "        \n",
    "        Y = np.concatenate(Xs, axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "        \n",
    "        dXs = np.split(dY, self.orig_axis_dims, axis=self.axis)[0]\n",
    "        \n",
    "        return dXs\n",
    "\n",
    "\n",
    "class Index(Op):\n",
    "    \n",
    "    def __init__(self, t: T, indices):\n",
    "        \"\"\"Slices a tensor along all axes.\n",
    "\n",
    "        Args:\n",
    "            t (T): The tensor to slice\n",
    "            indices (Tuple[slice]):  The partial or full indices to slice on `t`.\n",
    "                Can be an index, single slice, tuple of slices, or Ellipsis.\n",
    "                `None` is not allowed.\n",
    "        \"\"\"\n",
    "        if not isinstance(indices, tuple):\n",
    "            indices = (indices,)\n",
    "\n",
    "        self.indices = indices\n",
    "        \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X[self.indices]\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.zeros(X.shape)\n",
    "        dX[self.indices] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceSum(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.sum(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.repeat(\n",
    "            np.expand_dims(dY, axis=self.axis),\n",
    "            X.shape[self.axis],\n",
    "            axis=self.axis\n",
    "        )\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceMax(Op):\n",
    "    \"\"\"Differentiable max operator\"\"\"\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.max(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        print(X.shape, dY.shape)\n",
    "\n",
    "        dX = np.zeros_like(X)\n",
    "        dX[np.argmax(X, axis=self.axis)] = dY\n",
    "\n",
    "        print(dX.shape)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "class ReduceMin(Op):\n",
    "    \"\"\"Differentiable min operator\"\"\"\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.min(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.zeros_like(X)\n",
    "        dX[np.argmin(X, axis=self.axis)] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class NaN2Num(Op):\n",
    "    \n",
    "    def __init__(self, t: T, posinf: float = 1e3, neginf: float = -1e3):\n",
    "        self.posinf = posinf\n",
    "        self.neginf = neginf\n",
    "            \n",
    "        super().__init__(t)\n",
    "\n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.nan_to_num(X, posinf=self.posinf, neginf=self.neginf)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = np.nan_to_num(dZ, posinf=10., neginf=-10.)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Linear(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class StopGrad(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = np.zeros_like(dZ)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Neg(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = -X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = -dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Add(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X + Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Sub(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X - Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = -dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Mul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X * Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Y * dZ\n",
    "        dY = X * dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class MatMul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X @ Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]  # [A,B]\n",
    "        Y = inputs[1]  # [B,C]\n",
    "        dZ = top_grad  # [A,C]\n",
    "        \n",
    "        dX = dZ @ Y.transpose()\n",
    "        dY = X.transpose() @ dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Exp(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.exp(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Z * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Sigm(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = 1 / (1 + np.exp(-X))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Z * (1 - Z) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Tanh(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.tanh(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = ((1 - Z)**2) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Relu(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X > 0) * X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = (X > 0) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Threshold(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X >= 0)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Pow(Op):\n",
    "    \n",
    "    def __init__(self, x: T, power: int):\n",
    "        \n",
    "        self.power = power\n",
    "        \n",
    "        super().__init__(x)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        \n",
    "        Y = X ** p\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = p * X ** (p-1) * dY\n",
    "        dX = np.nan_to_num(dX, posinf=1e3, neginf=-1e3)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, lr: float = 0.001):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.debug = False\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.STATIC:\n",
    "            raise 'STATIC execution mode not enabled'\n",
    "        \n",
    "        self.bprop(t_out=t, output_grad=-np.ones_like(t.val))\n",
    "        \n",
    "    def bprop(self, t_out: T, output_grad: V):\n",
    "        \n",
    "        output_grad = np.nan_to_num(output_grad, posinf=10., neginf=-10.)\n",
    "        \n",
    "        assert isinstance(t_out, (Var, Op))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f'bp {t_out} output_grad:')\n",
    "            print(output_grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        This approach does not efficiently handle weights that are consumed by multiple nodes\n",
    "        It would be better to treat backpropagation from a spreading-network-delta perspective\n",
    "        than assume everything is a tree (That's also how I should do STATIC execution refresh)\n",
    "        This should still work though, but it's just going to set the same weight multiple times\n",
    "        for every downstream consumer.\n",
    "\n",
    "        Actually, the whole thesis of minibatch gradient descent is that we can approximate a global\n",
    "        gradient by updates on local subsets of data, so it might be sufficient to leave the code\n",
    "        as is.\n",
    "\n",
    "        However this approach will still take unnecessary walks down the tree in depth-first fashion.\n",
    "        Innefficient: Yes; Works: Yes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # iteratively called\n",
    "        if isinstance(t_out, Var):\n",
    "            if t_out.trainable:\n",
    "                #print('output_grad', output_grad.shape)\n",
    "                if self.debug:                    \n",
    "                    print('t_out.val (old)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "                \n",
    "                # yucky duct tape to handle batch size differences\n",
    "                if t_out.shape[0] == 1 and output_grad.shape[0] > 1:\n",
    "                    output_grad = np.sum(output_grad, axis=0)[None, ...]\n",
    "\n",
    "                t_out.val = t_out.val + (self.lr * output_grad)\n",
    "                if self.debug:\n",
    "                    print('t_out.val (new)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "            \n",
    "        elif isinstance(t_out, Op):\n",
    "            input_grads = t_out.reverse_grad(\n",
    "                inputs=tuple(t.val for t in t_out.input_ts),\n",
    "                output=t_out.val, top_grad=output_grad)\n",
    "            \n",
    "            for input_t, input_grad in zip(t_out.input_ts, input_grads):\n",
    "                self.bprop(t_out=input_t, output_grad=input_grad)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._built = False\n",
    "        self._loss = Var(np.zeros(()), trainable=False)\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return self._loss\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        pass\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, X_T: T) -> T:\n",
    "        if not self._built:\n",
    "            self.build(X_T.shape)\n",
    "            self._built = True\n",
    "\n",
    "        # reset the regularization loss\n",
    "        self._loss = Var(0, trainable=False)\n",
    "\n",
    "        return self.forward(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.77489121, 0.77122773, 0.57652188, 0.4192426 , 0.96431482],\n",
       "        [0.31910157, 0.79432582, 0.41088662, 0.77494032, 0.14994998],\n",
       "        [0.61753794, 0.98515798, 0.88242563, 0.90604408, 0.35057589]]),\n",
       " array([[-0.        , -0.        , -0.        ,  0.00262531, -0.        ,\n",
       "         -0.        , -0.        ,  0.06937857, -0.        , -0.        ],\n",
       "        [-0.        ,  0.02030562, -0.        ,  0.01614764, -0.        ,\n",
       "         -0.        , -0.        ,  0.03847847, -0.        , -0.        ],\n",
       "        [-0.        ,  0.01156763, -0.        ,  0.02133865, -0.        ,\n",
       "         -0.        , -0.        ,  0.05267905, -0.        , -0.        ]]),\n",
       " Tensor(0.004767513934823068),\n",
       " [Tensor([[-0.04720139 -0.02048058  0.00204957  0.00587311 -0.02101643  0.04068757\n",
       "    -0.02559744  0.03319881 -0.04795947  0.03194449]\n",
       "   [-0.0102456  -0.01168267 -0.04813369  0.02581874 -0.02411819 -0.04077488\n",
       "     0.02897201 -0.00573817 -0.03563188 -0.03216296]\n",
       "   [-0.0282607   0.01285564 -0.01693905 -0.0170603  -0.04267333  0.0190482\n",
       "    -0.02586956 -0.00589674 -0.01052299 -0.0126536 ]\n",
       "   [ 0.03360538 -0.00431359  0.01587181  0.04436255  0.00338115 -0.00823899\n",
       "    -0.01348483  0.0186335  -0.036786   -0.00414352]\n",
       "   [-0.02587539 -0.02937219  0.031105    0.00368701  0.0280022   0.02657352\n",
       "    -0.0483625   0.02853817 -0.02937223 -0.02762562]]),\n",
       "  Tensor([[-0.01042861  0.03858577  0.00300786 -0.03415632 -0.04771203 -0.03415129\n",
       "     0.00975901  0.01614641 -0.02998992 -0.03582536]])])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "        units: int, \n",
    "        activation: Op = None, \n",
    "        use_bias: bool = True,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None\n",
    "    ):\n",
    "        super(Dense, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "            \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(input_shape[-1], self.units))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.units))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # compute presynaptic input\n",
    "        Z_T = X_T @ self.W_T\n",
    "\n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply activation\n",
    "        Y_T = self.activation(Z_T)\n",
    "        \n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Dense(10, Relu, 0.1, 0.1, 0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(3, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.val, Y_T.val, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 7, 70, 5),\n",
       " (2, 7, 70, 64),\n",
       " Tensor(0.6650852939497751),\n",
       " [Tensor([[-0.04690391  0.03219781  0.03133849 ... -0.01250094 -0.02040534\n",
       "     0.02855088]\n",
       "   [-0.04776165  0.02981423 -0.04664671 ...  0.01683485 -0.02068035\n",
       "     0.0417308 ]\n",
       "   [-0.04366009  0.00428372  0.01177435 ... -0.04615517 -0.02045305\n",
       "    -0.019969  ]\n",
       "   ...\n",
       "   [ 0.01918657 -0.03537107  0.00954096 ...  0.00848717  0.01232981\n",
       "    -0.00966575]\n",
       "   [-0.04994427  0.00382309 -0.01930179 ... -0.04597156 -0.00578884\n",
       "    -0.00101414]\n",
       "   [-0.03620059 -0.00242729  0.02865895 ...  0.04867044 -0.0362522\n",
       "     0.04736328]])])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv2D(Layer):\n",
    "    \"\"\"Standard 2D Conv layer.\n",
    "    I.E. convolves over Tensors shaped [B, H, W, D]\n",
    "    to produce [B, H-2*kernel_size, W-2*kernel_size, filters]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        filters: int, \n",
    "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
    "        strides: Union[int, Tuple[int, int]] = 1,\n",
    "        padding: str = 'valid',  # 'valid' or 'same'\n",
    "        activation: Op = None,\n",
    "        use_bias: bool = False,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None,\n",
    "    ):\n",
    "        super(Conv2D, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1, 'kernel_size must be odd'\n",
    "        if isinstance(strides, int):\n",
    "            strides = (strides, strides)\n",
    "        assert strides[0] > 0 and strides[1] > 0, 'strides must be positive'\n",
    "        padding = padding.lower()\n",
    "        assert padding in ('valid', 'same'), 'padding must be valid or same'\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(\n",
    "            self.kernel_size[0]*self.kernel_size[1]*input_shape[-1], \n",
    "            self.filters))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.filters))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # maybe pad input\n",
    "        if self.padding == 'same':\n",
    "\n",
    "            # various padding sizes, strides, and offsets\n",
    "            # 0   1   2   3   4\n",
    "            #     0 1 2 3 4\n",
    "            #         0 1 2 3 4 5 6 7 8 9\n",
    "            #         0 1 2 3 4\n",
    "            #         0   1   2   3   4\n",
    "\n",
    "            pad_top = self.strides[0]*(self.kernel_size[0]-1)//2\n",
    "            pad_bottom = pad_top\n",
    "            pad_left = self.strides[1]*(self.kernel_size[1]-1)//2\n",
    "            pad_right = pad_left\n",
    "            B, H_orig, W_orig, C = X_T.shape\n",
    "\n",
    "            # pad height\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, pad_top, W_orig, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, pad_bottom, W_orig, C)), trainable=False),\n",
    "            ], axis=1)\n",
    "\n",
    "            # pad width\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_left, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_right, C)), trainable=False),\n",
    "            ], axis=2)\n",
    "\n",
    "        elif self.padding == 'valid':\n",
    "            pass\n",
    "\n",
    "        # stack the input tensor along the channel axis\n",
    "        # but shifted by all possible kernel shifts\n",
    "        stack = []\n",
    "        for shift in itertools.product(range(0, self.strides[0]*self.kernel_size[0], self.strides[0]),\n",
    "                                       range(0, self.strides[1]*self.kernel_size[1], self.strides[1])):\n",
    "            stack.append(X_T[\n",
    "                :,\n",
    "                shift[0]:,\n",
    "                shift[1]:,\n",
    "                :\n",
    "            ])\n",
    "\n",
    "        # clip stack to greatest common shape\n",
    "        min_shape = np.min(np.array([s.shape for s in stack]), axis=0)\n",
    "        stack = [s[:, :min_shape[1], :min_shape[2], :] for s in stack]\n",
    "\n",
    "        # stack the shifted tensors along the channel axis\n",
    "        stacked = Concat(stack, axis=3)  # [B, H-k_h//2, W-k_w//2, C*k_h*k_w]\n",
    "\n",
    "        # convolve over the stacked tensors\n",
    "        Z_T = stacked @ self.W_T  \n",
    "    \n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply the activation function\n",
    "        Y_T = self.activation(Z_T)\n",
    "\n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Conv2D(filters=64, kernel_size=5, strides=5, padding='same', weight_L2=0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 7, 70, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3, 4, 5), (2, 60), Tensor(0), [])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Flatten(Layer):\n",
    "    \"\"\"Flattens all non-batch dimensions into a single axis\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return []\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        flat_dims = functools.reduce(lambda x, y: x*y, X_T.shape[1:])\n",
    "        Y_T = Reshape(X_T, (X_T.shape[0], flat_dims))\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Flatten()\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 3, 4, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[0.3228232]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        super(Sequential, self).__init__()\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        for layer in self.layers:\n",
    "            X_T = layer(X_T)\n",
    "        return X_T\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return sum(layer.loss for layer in self.layers)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        trainable_vars = []\n",
    "        for layer in self.layers:\n",
    "            trainable_vars += layer.trainable_variables\n",
    "        return trainable_vars\n",
    "\n",
    "\n",
    "net = Sequential([\n",
    "    Dense(10, Relu),\n",
    "    Dense(128, Relu),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(X_T)\n",
    "\n",
    "img_T = Var(np.random.uniform(0, 1, size=(1, 28, 28, 1)), trainable=False)\n",
    "net = Sequential([\n",
    "    Conv2D(32, 3, 2, activation=Relu),\n",
    "    Conv2D(64, 3, 2, activation=Relu),\n",
    "    Flatten(),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(img_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standards:\n",
    "- `Step`: uses batch dimension (except `done` which is always a bool)\n",
    "- `Agent` uses batch dimension\n",
    "- `Environment` doesn't use batch dimension\n",
    "\n",
    "This means you will have to use `Step.batch` and `Step.unbatch` in your training/running loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(NamedTuple):\n",
    "    \"\"\"Single step.\"\"\"\n",
    "    \n",
    "    obs: np.ndarray\n",
    "    next_obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: np.ndarray\n",
    "    done: bool\n",
    "    info: any\n",
    "\n",
    "    @staticmethod\n",
    "    def unbatch(step: Step) -> List[Step]:\n",
    "        return [\n",
    "            Step(\n",
    "                obs=step.obs[i:i+1],\n",
    "                next_obs=step.next_obs[i:i+1],\n",
    "                action=step.action[i:i+1],\n",
    "                reward=step.reward[i:i+1],\n",
    "                done=step.done,\n",
    "                info=step.info[i:i+1],\n",
    "            )\n",
    "            for i in range(step.obs.shape[0])\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def batch(steps: List[Step]) -> Step:\n",
    "        return Step(\n",
    "            obs=np.concatenate([step.obs for step in steps], axis=0),\n",
    "            next_obs=np.concatenate([step.next_obs for step in steps], axis=0),\n",
    "            action=np.concatenate([step.action for step in steps], axis=0),\n",
    "            reward=np.concatenate([step.reward for step in steps], axis=0),\n",
    "            done=any(step.done for step in steps),\n",
    "            info=[step.info for step in steps])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_no_batch_axis(step: NoBatchStep) -> Step:\n",
    "        return Step(\n",
    "            obs=step.obs[None, ...],\n",
    "            next_obs=step.next_obs[None, ...],\n",
    "            action=step.action[None, ...],\n",
    "            reward=step.reward[None, ...],\n",
    "            done=step.done,\n",
    "            info=[step.info]\n",
    "        )\n",
    "\n",
    "# dimensional type hinting\n",
    "BatchStep = Step\n",
    "NoBatchStep = Step\n",
    "\n",
    "Traj = List[BatchStep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- replace `list` with List and same for dict/mapping\n",
    "- use one-hot encodings for all actions and rewire the policies to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"RL environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        \"\"\"Resets the environment\n",
    "\n",
    "        Returns:\n",
    "            Step: Initial step. The `next_obs` attribute should be set \n",
    "                with an initial observation. `done` should be False. \n",
    "                `obs` and `action` should not be used.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Step:\n",
    "        \"\"\"Computes one logical step in the environment\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): The action to take.\n",
    "\n",
    "        Returns:\n",
    "            Step: Step resulting from taking `action`. The `next_obs` attribute\n",
    "                should be set with the observation resulting from taking the `action`\n",
    "                in the current environment state. `obs` should not be used. If the \n",
    "                environment is turn-based, then the reward should correspond to the \n",
    "                agent that just acted (not the next agent in line to act).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NoBatchEnv(Environment):\n",
    "    \"\"\"Environment with no batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "\n",
    "class BatchEnv(Environment):\n",
    "    \"\"\"Environment with batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> BatchStep:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch1Env(BatchEnv):\n",
    "    \"\"\"Adds a batch axis to all outgoing Steps and strips it off incoming Steps.\"\"\"\n",
    "\n",
    "    def __init__(self, env: NoBatchEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.reset())\n",
    "\n",
    "    def step(self, action: np.array) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.step(action[0]))\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEnv(BatchEnv):\n",
    "    \"\"\"Keeps reseting the same environment in a batch.\n",
    "    \n",
    "    Declares itself done when a total of `batch_size` individual environment\n",
    "    dones are experienced.\n",
    "\n",
    "    NOTE: Individual environments should be `NoBatchEnv`'s.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, env_init_fn: Callable[[], NoBatchEnv]):\n",
    "        self.batch_size = batch_size\n",
    "        self.env_init_fn = env_init_fn\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        self.dones = 0\n",
    "        self.envs = [self.env_init_fn() for _ in range(self.batch_size)]\n",
    "        steps = [env.reset() for env in self.envs]\n",
    "        steps = [Step.from_no_batch_axis(step) for step in steps]\n",
    "        return Step.batch(steps)\n",
    "\n",
    "    def step(self, action: np.array) -> Step:\n",
    "        steps = []\n",
    "        for i, (env, single_action) in enumerate(zip(self.envs, action)):\n",
    "            step = env.step(single_action)\n",
    "            if step.done:\n",
    "                self.envs[i] = self.env_init_fn()\n",
    "                new_step = env.reset()\n",
    "                step.next_obs = new_step.next_obs\n",
    "                self.dones += 1\n",
    "            steps.append(Step.from_no_batch_axis(step))\n",
    "\n",
    "        batched_step = Step.batch(steps)\n",
    "        batched_step.done = self.dones >= self.batch_size\n",
    "        return batched_step\n",
    "\n",
    "    def render(self):\n",
    "        for env in self.envs:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer.\n",
    "\n",
    "    Expects the following hyperparameters:\n",
    "        - `epoch`: The current epoch.\n",
    "        - `batch_size`: Number of trajectories to return at each call.\n",
    "        - `min_sample_len`: Minimum length of trajectories to sample.\n",
    "        - `max_sample_len`: Maximum length of trajectories to sample.\n",
    "        - `num_steps_replay_coef`: Sampling coefficient based on trajectory length.\n",
    "        - `success_replay_coef`: Sampling coefficient based on trajectory success.\n",
    "        - `age_replay_coef`: Sampling coefficient based on trajectory age.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, hparams: dict):\n",
    "        self.hparams = hparams\n",
    "        self.trajs = dict()\n",
    "\n",
    "    @property\n",
    "    def flat_traj(self):\n",
    "        return [step for traj in self.trajs for step in traj]\n",
    "\n",
    "    def add(self, traj: Traj, epoch: int):\n",
    "        \"\"\"Add a new trajectory to the buffer.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): the trajectory to add.\n",
    "            epoch (int): the epoch that the trajectory was experience in.\n",
    "        \"\"\"\n",
    "        if epoch not in self.trajs:\n",
    "            self.trajs[epoch] = []\n",
    "        self.trajs[epoch] += traj\n",
    "\n",
    "    def sample(self) -> Traj:\n",
    "        \"\"\"Samples a batched trajectory from the buffer stochastically based on:\n",
    "            - the number of steps in the trajectory (num_steps_replay_coef)\n",
    "            - how well the agent did in the trajectory (success_replay_coef)\n",
    "            - how long ago the trajectory was experienced (age_replay_coef)\n",
    "\n",
    "        Returns:\n",
    "            Traj: a trajectory of batched steps experienced.\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = {\n",
    "            epoch: self.hparams['num_steps_replay_coef'] * len(traj) +\n",
    "                   self.hparams['success_replay_coef'] * sum(step.reward for step in traj) +\n",
    "                   self.hparams['age_replay_coef'] * (epoch - self.hparams['epoch'])\n",
    "            for epoch, traj in self.trajs.items()\n",
    "        }\n",
    "        epochs = np.random.choice(list(weights.keys()), size=(), replace=True, p=list(weights.values()))\n",
    "        trajs = [self.trajs[epoch] for epoch in epochs]\n",
    "        batched_traj = [BatchStep.batch(steps) for steps in zip(*trajs)]\n",
    "        return batched_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, policy: Callable):\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, step: BatchStep) -> np.ndarray:\n",
    "        \"\"\"Generates an action for a given observation using `self.policy`. \n",
    "        Override if you want to give your policy more information such as\n",
    "        recurrent state or previous reward.\n",
    "\n",
    "        Args:\n",
    "            step (Step): last step output by the environment. This means the agent\n",
    "                should feed `step.next_obs`, not `step.obs` to its policy. If the \n",
    "                environment is multi-agent, then the `reward` attribute has already\n",
    "                updated to reflect the reward for this agent by the driver.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The action to take\n",
    "        \"\"\"\n",
    "        return self.policy(step.next_obs)\n",
    "\n",
    "    def reward(self, traj: Traj) -> float:\n",
    "        \"\"\"Evaluates the cumulative reward for your agent as the sum of \n",
    "        individual rewards experienced. \n",
    "        \n",
    "        If your agent uses intrinsic rewards, be sure to add them in here.\n",
    "        Do not introduce Q-values or predicted rewards here.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "\n",
    "        Returns:\n",
    "            float: Cumulative (sum) reward over the entire sequence.\n",
    "        \"\"\"\n",
    "        return sum(step.reward for step in traj)\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        raise NotImplemented('Method `train` must be implemented by subclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelDriver:\n",
    "    \"\"\"Drives batched turn-based `BatchedEnv` environments with multiple agents.\n",
    "    Also supports single-agent environments as a special case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def drive(self, agents: Mapping[str, Agent], env: Environment) -> Mapping[str, Traj]:\n",
    "        \"\"\"Drives a batched environment with multiple agents.\n",
    "        \n",
    "        Args:\n",
    "            agents (Mapping[str, Agent]): A dictionary of agents to drive.\n",
    "            env (Environment): The environment to drive.\n",
    "\n",
    "        Returns:\n",
    "            Mapping[str, Traj]: A dictionary of trajectories for each agent.\n",
    "                Each trajectory is completely disengaged from the other agent's.\n",
    "                (i.e.: the obs, next_obs, action, reward, done attributes are\n",
    "                individual to each agent for each trajectory.)\n",
    "        \"\"\"\n",
    "\n",
    "        names_it = itertools.cycle(agents.keys())\n",
    "        trajs = {agent_name: [] for agent_name in agents}\n",
    "        prev_rewards = {agent_name: 0. for agent_name in agents}\n",
    "\n",
    "        step = env.reset()\n",
    "        while not step.done:\n",
    "            agent_name = next(names_it)\n",
    "            \n",
    "            # `Agent.forward` only looks at `step.next_obs` and `step.reward`\n",
    "            # but I'm assigning defaults just to be safe.\n",
    "            action = agents[agent_name].forward(Step(\n",
    "                obs=step.obs,  # what the previous agent saw before acting\n",
    "                next_obs=step.next_obs,  # what the current agent sees before acting\n",
    "                reward=prev_rewards[agent_name],  # the reward this agent experienced following its last action\n",
    "                done=step.done,  # whether the environment was done after the previous agent acted\n",
    "                info=step.info  # any extra information the environment might have output\n",
    "            )) \n",
    "\n",
    "            prev_step = step\n",
    "            step = env.step(action)  # `Environment.step` produces a Step with all fields except `step.obs` set\n",
    "            step.obs = prev_step.next_obs  # the current agent's observation is the previous agent's next observation\n",
    "            prev_rewards[agent_name] = step.reward  # the reward for the action the current agent just took\n",
    "            trajs[agent_name].append(step)  # Step completely corresponding to this agent (obs before action, obs after action, action, reward, done, info)\n",
    "        \n",
    "        return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTrainer:\n",
    "    \"\"\"Trains `BatchedEnv` environments and mutliple agents \n",
    "    (with N=1 single-agent supported as a special case).\n",
    "    \n",
    "    Uses following hyperparameters:\n",
    "    - `epoch`: the current epoch. Reads and writes to this variable.\n",
    "    - `epochs`: the number of epochs to train for.\n",
    "    - `min_steps_per_epoch`: the minimum number of steps to train for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams: dict, callbacks: List[Callable]):\n",
    "        self.hparams = hparams\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "    def train(self, \n",
    "        agents: Mapping[str, Agent], \n",
    "        env: Environment,\n",
    "        test_env: Environment = None,\n",
    "        buffers: Mapping[str, ReplayBuffer] = None,\n",
    "        collect_driver: ParallelDriver = None,\n",
    "        test_driver: ParallelDriver = None,\n",
    "        histories: Mapping[str, Mapping[int, Mapping[str, any]]] = None,\n",
    "        ) -> Mapping[int, Mapping[str, any]]:\n",
    "\n",
    "        agent_names = list(agents.keys())\n",
    "        \n",
    "        # initialize defaults\n",
    "        if test_env is None:\n",
    "            test_env = env\n",
    "        if buffers is None:\n",
    "            buffers = dict()\n",
    "        if collect_driver is None:\n",
    "            collect_driver = ParallelDriver()\n",
    "        if test_driver is None:\n",
    "            test_driver = collect_driver\n",
    "        if histories is None:\n",
    "            histories = dict()  # {agent_name: {epoch: {...data}}}\n",
    "\n",
    "        # build uninitialized agent-specific objects\n",
    "        for agent_name in agent_names:\n",
    "            if agent_name not in buffers:\n",
    "                buffers[agent_name] = ReplayBuffer()\n",
    "            if agent_name not in histories:\n",
    "                histories[agent_name] = dict()\n",
    "\n",
    "        # run training loop\n",
    "        for epoch in range(self.hparams['epoch'], self.hparams['epochs']):\n",
    "            self.hparams['epoch'] = epoch\n",
    "\n",
    "            # collect trajectories\n",
    "            steps = 0\n",
    "            while steps < self.hparams['steps_per_epoch']:\n",
    "                collect_trajs = collect_driver.drive(agents, env)\n",
    "                steps += min(len(traj) for _, traj in collect_trajs.items())\n",
    "                for agent_name in agent_names:\n",
    "                    buffers[agent_name].add(collect_trajs[agent_name])\n",
    "\n",
    "            # train\n",
    "            train_trajs = {agent_name: buffers[agent_name].sample() for agent_name in agent_names}\n",
    "            for agent_name in agent_names:\n",
    "                agents[agent_name].train(train_trajs[agent_name])\n",
    "                \n",
    "            # test\n",
    "            test_trajs = test_driver.drive(agents, env)\n",
    "\n",
    "            # record history and run callbacks\n",
    "            for agent_name in agent_names:\n",
    "                histories[agent_name][epoch] = {\n",
    "                    'epoch': epoch,\n",
    "                    'agent': agents[agent_name],\n",
    "                    'all_agents': agents,\n",
    "                    'env': env,\n",
    "                    'test_env': test_env,\n",
    "                    'collect_traj': collect_trajs[agent_name],\n",
    "                    'train_traj': train_trajs[agent_name],\n",
    "                    'test_traj': test_trajs[agent_name],\n",
    "                    'buffer': buffers[agent_name],\n",
    "                }\n",
    "                for callback in self.callbacks:\n",
    "                    callback(histories[agent_name][epoch])\n",
    "\n",
    "        return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback:\n",
    "\n",
    "    def __init__(self, hparams: dict, print_hparam_keys: List[str] = None, print_data_keys: List[str] = None):\n",
    "        if print_hparam_keys is None:\n",
    "            print_hparam_keys = ['epoch']\n",
    "        if print_data_keys is None:\n",
    "            print_data_keys = []\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.print_hparam_keys = print_hparam_keys\n",
    "        self.print_data_keys = print_data_keys\n",
    "\n",
    "    def __call__(self, data: Mapping[str, any]):\n",
    "        for key in self.print_hparam_keys:\n",
    "            print(f'{key}: {self.hparams[key]}', end='\\t')\n",
    "        for key in self.print_data_keys:\n",
    "            print(f'{key}: {data[key]}', end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEvalCallback:\n",
    "\n",
    "    def __init__(self, \n",
    "        eval_on_collect: bool = True, \n",
    "        eval_on_train: bool = False, \n",
    "        eval_on_test: bool = False):\n",
    "\n",
    "        self.eval_on_collect = eval_on_collect\n",
    "        self.eval_on_train = eval_on_train\n",
    "        self.eval_on_test = eval_on_test\n",
    "\n",
    "    def __call__(self, data: Mapping[str, any]):\n",
    "        agent = data['agent']\n",
    "        if not hasattr(agent, 'q_eval'):\n",
    "            return\n",
    "\n",
    "        if self.eval_on_collect:\n",
    "            traj = data['collect_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_collect'] = q_val\n",
    "\n",
    "        if self.eval_on_train:\n",
    "            traj = data['train_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_train'] = q_val\n",
    "\n",
    "        if self.eval_on_test:\n",
    "            traj = data['test_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_test'] = q_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- make the reward optionally an advantage computation over last round\n",
    "- also make a recurrent DQN agent (estimate q function of a sequence of states)\n",
    "- make a simple greedy connect4 agent\n",
    "- make the preprocessor perform a columnwise mean pool before flattening\n",
    "- train the preprocessor on an auxillary objective to estimate the max connected for each length for self and for oponent\n",
    "- add padding='SAME'|'VALID' to conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomAgent(Agent):\n",
    "    \"\"\"Takes a random action on each timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int):\n",
    "        super(RandomAgent, self).__init__(policy=self._policy)\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        choices = np.random.randint(0, self.num_actions, (obs.shape[0],))\n",
    "        onehots = np.eye(self.num_actions)[choices]\n",
    "        return onehots\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "agent = RandomAgent(num_actions=5)\n",
    "obs = Var(np.array([[1, 2], [3, 4]]))\n",
    "step = Step(obs=None, next_obs=obs, action=None, reward=None, done=None, info=None)\n",
    "agent.forward(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HPARAMS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6d271b87613f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m ])  # [B, H, W, 1] -> [B, L]\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRealDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'board_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHPARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HPARAMS' is not defined"
     ]
    }
   ],
   "source": [
    "class RealDQN(Agent):\n",
    "    \"\"\"'Classic' Deep Q-learning agent.\n",
    "    Implements the approach in https://arxiv.org/pdf/1312.5602.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "        ####### TODO TODO TODO #######\n",
    "        # make encoder perform columnwise max pooling\n",
    "        # make the qfunction column-specific \n",
    "        # (no X-row connections outside conv layers)\n",
    "        # output max q value over all rows\n",
    "        # implement corresponding changes in categorical agent\n",
    "        # Actually the categorical agent just a per-column MLP\n",
    "        # with no max Q pooling.\n",
    "        ####### TODO TODO TODO #######\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder\n",
    "        self.head = Sequential([\n",
    "            Dense(512, Sigm), \n",
    "            Dense(1, lambda x: x)\n",
    "        ]) # [B, L+|A|] -> [B, 1]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(RealDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = min(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            return np.random.rand(self.num_actions)\n",
    "        \n",
    "        # Otherwise take the action with the highest Q-value\n",
    "        actions = np.arange(self.num_actions)  # [self.num_actions]\n",
    "        q_vals = np.zeros((self.num_actions,))  # [self.num_actions]\n",
    "        for i, action in enumerate(actions):\n",
    "            \n",
    "            # prepare inputs\n",
    "            obs_T = Var(obs[None, ...])  # [1, H, W, 2]\n",
    "            action_T = Var(action[None, None], trainable=False)  # [1, 1]\n",
    "\n",
    "            # run the network\n",
    "            enc_T = self.encoder(obs_T)  # [1, d_enc]\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [1, d_enc+1]\n",
    "            q_T = self.head(cat_T)  # [1, 1]\n",
    "\n",
    "            # store q-value\n",
    "            q_vals[i] = q_T.val[0,0]  # []\n",
    "        \n",
    "        # select the action with the highest Q-value\n",
    "        action = actions[q_vals.argmax()]\n",
    "        return action\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_T = Var(step.action[None], trainable=False)  # [B, 1]\n",
    "            action_next_T = Var(self.policy(step.next_obs), trainable=False)  # [B, 1]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [B, d_enc+1]\n",
    "            Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "            cat_T = Concat([enc_T, action_next_T], axis=1)  # [B, d_enc+1]\n",
    "            Qnext_T = self.head(cat_T)[:, 0]  # [B]\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above; small updates are handled in the SGD step)\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Qnext_T)) - Qnow_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        action_T = Var(action)  # [B, 1]\n",
    "        enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "        cat_T = Concat([enc_T, action_T], axis=1)  # [B, d_enc+1]\n",
    "        Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "\n",
    "        return Qnow_T.val\n",
    "\n",
    "\n",
    "encoder = Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    Flatten(),\n",
    "])  # [B, H, W, 1] -> [B, L]\n",
    "\n",
    "agent = RealDQN(num_actions=HPARAMS['board_size'], encoder=encoder, hparams=HPARAMS)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CategoricalDQN at 0x7fe1ec8b5cd0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CategoricalDQN(Agent):\n",
    "    \"\"\"Categorical deep Q-network agent.\n",
    "    I never read the paper for this architecture, so my implementation\n",
    "    might be different from the origonal researchers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder  # [B, H, W, C] -> [B, d_enc]\n",
    "        self.head = Sequential([\n",
    "            Dense(2*hparams['board_size']*num_actions, Tanh),\n",
    "            Dense(num_actions, Linear)\n",
    "        ]) # [B, d_enc] -> [B, num_actions]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(CategoricalDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = min(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            return np.random.rand(self.num_actions)\n",
    "        \n",
    "        # Otherwise take the action with the highest Q-value\n",
    "        # compute q-values for all actions\n",
    "        obs_T = Var(obs, trainable=False)  # [B, H, W, C]\n",
    "        enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "        qvals_T = self.head(enc_T)  # [B, A]\n",
    "        return np.argmax(qvals_T.val, axis=1)  # [B]\n",
    "\n",
    "    \n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            obs_next_T = Var(step.next_obs, trainable=False)  # [B, H, W, 2]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "            qvals_T = self.head(enc_T)  # [B, A]\n",
    "            Q_now_T = qvals_T[step.action]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            enc_T = self.encoder(obs_next_T)  # [B, d_enc]\n",
    "            qvals_T = self.head(enc_T)  # [B, A]\n",
    "            Q_next_T = np.max(qvals_T.val, axis=1)  # [B]\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above; small updates are handled in the SGD step)\n",
    "            # but only update the targets that were actually selected for action at `step_now`.\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Q_next_T)) - Q_now_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        enc_T = self.encoder(obs_T)  # [B, d_enc]\n",
    "        qvals_T = self.head(enc_T)  # [B, A]\n",
    "\n",
    "        return qvals_T.val[action]\n",
    "\n",
    "\n",
    "agent = CategoricalDQN(num_actions=HPARAMS['board_size'], encoder=encoder, hparams=HPARAMS)\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwiredConnect4Agent(Agent):\n",
    "\n",
    "    def __init__(self, board_size: int, hparams: dict):\n",
    "        self.board_size = board_size\n",
    "        self.hparams = hparams\n",
    "        super(HardwiredConnect4Agent, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        B = obs.shape[0]\n",
    "        action = np.zeros(B, dtype=np.int32)\n",
    "        for b in range(B):\n",
    "            o = obs[b]\n",
    "            ## TODO: make a greedy agent\n",
    "            action[b] = random.randint(0, self.board_size-1)\n",
    "        return action\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
       "Turn: 1\n",
       "Winner: 0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Board:\n",
    "    \"\"\"Drafted by copilot with minor human edits\"\"\"\n",
    "\n",
    "    def __init__(self, size=7, win_length=4):\n",
    "        self.size = size\n",
    "        self.win_length = win_length\n",
    "        self.board = np.zeros((size, size))\n",
    "        self.turn = 1\n",
    "        self.winner = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.board}\\nTurn: {self.turn}\\nWinner: {self.winner}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.board == other.board\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.board.tostring())\n",
    "\n",
    "    def is_full(self):\n",
    "        return np.count_nonzero(self.board) == self.size**2\n",
    "\n",
    "    def is_empty(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return 0 <= col < self.size and self.is_empty(col)\n",
    "\n",
    "    def make_move(self, col):\n",
    "        if self.is_valid_move(col):\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-1]\n",
    "            self.board[highest_row, col] = self.turn\n",
    "            self.turn *= -1\n",
    "\n",
    "    def undo_move(self, col):\n",
    "        if self.is_valid_move(col) and self.board[0, col] != 0:\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-2]\n",
    "            self.board[highest_row, col] = 0\n",
    "            self.turn *= -1\n",
    "\n",
    "    def check_win(self) -> int:\n",
    "        for turn in [-1, 1]:\n",
    "            if self.num_connected(self.win_length, turn) > 0:\n",
    "                self.winner = turn\n",
    "                return True\n",
    "        return self.winner\n",
    "\n",
    "    def num_connected(self, length, turn):\n",
    "        num_connected = 0\n",
    "        # Check horizontal\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size-length+1):\n",
    "                if np.all(self.board[row, col:col+length] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check vertical\n",
    "        for col in range(self.size):\n",
    "            for row in range(self.size-length+1):\n",
    "                if np.all(self.board[row:row+length, col] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(self.size-length+1):\n",
    "                if all(self.board[row+i, col+i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        # Check anti-diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(length-1, self.size):\n",
    "                if all(self.board[row+i, col-i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        return num_connected\n",
    "\n",
    "board = Board()\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() missing 1 required positional argument: 'action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-0741f16c541f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mboard_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoardEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboard_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dense_advantage'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-0741f16c541f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, board_size, win_length, reward_mode)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoBatchStep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-0741f16c541f>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_dense_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         return NoBatchStep(\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() missing 1 required positional argument: 'action'"
     ]
    }
   ],
   "source": [
    "class BoardEnv:\n",
    "\n",
    "    def __init__(self, board_size=7, win_length=4, reward_mode: str = 'sparse'):\n",
    "        \"\"\"RL environment for Connect 4. \n",
    "\n",
    "        Args:\n",
    "            board_size (int, optional): The size of the board. Defaults to 7.\n",
    "            win_length (int, optional): The minimum connected length to win. Defaults to 4.\n",
    "            reward_mode (str, optional): One of 'sparse', 'dense_stateless', 'dense_advantage'. \n",
    "                - For 'sparse', the reward is 1 if the player has attained a connect `win_length`,\n",
    "                    and is 0 otherwise.\n",
    "                - For 'dense_stateless', the reward increases linearly with the number of N-in-a-row's\n",
    "                    for all values of N from 0 to board_size weighted logarithmically by N.\n",
    "                - For 'dense_advantage', the reward is determined by the difference between the\n",
    "                    previous and current dense reward for each player individually.\n",
    "                `reward_mode` defaults to 'sparse'.\n",
    "        \"\"\"\n",
    "        self.board_size = board_size\n",
    "        self.win_length = win_length\n",
    "        self.reward_mode = reward_mode\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        self.board = Board(self.board_size, self.win_length)\n",
    "\n",
    "        if self.reward_mode == 'dense_advantage':\n",
    "            self.prev_dense_reward = [0., 0.]\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=np.zeros_like(obs),\n",
    "            next_obs=self._make_obs(), \n",
    "            reward=0., \n",
    "            done=False, \n",
    "            info=dict()\n",
    "        )\n",
    "\n",
    "    def step(self, action: np.ndarray) -> NoBatchStep:\n",
    "        \"\"\"Apply agent X's action to the board and \n",
    "        returns the next agent's timestep.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): array shaped (board_size,). The arg max\n",
    "                action index is the column where next piece is placed.\n",
    "\n",
    "        Returns:\n",
    "            tuple: NoBatchStep with values:\n",
    "                obs (np.ndarray[H, W, 2]): None\n",
    "                next_obs (np.ndarray[H, W, 2]): the next board state with\n",
    "                    self's entered squares represented in channel 0 and\n",
    "                    opponent's squares represented in channel 1\n",
    "                action (np.ndarray[board_size]): None\n",
    "                reward (float): the reward for the agent\n",
    "                    If sparse_reward is True, then reward is -1, 0, or +1.\n",
    "                    If sparse_reward is False, then reward is:\n",
    "                        ego_dense_reward - opponent_dense_reward.\n",
    "                done (bool): whether the game is over\n",
    "                info (dict): extra information\n",
    "        \"\"\"\n",
    "        # Apply action\n",
    "        action_index = np.argmax(action)  # []\n",
    "        self.board.make_move(action_index)  # this flips `board.turn`\n",
    "        # Temporarily unflip `board.turn`\n",
    "        self.board.turn *= -1\n",
    "\n",
    "        # Make egocentric observation\n",
    "        obs = self._make_obs()\n",
    "\n",
    "        # Compute reward\n",
    "        # This is the lazy way to do it, but it's fast enough\n",
    "\n",
    "        # Sparse reward\n",
    "        winner = self.board.check_win()\n",
    "        sparse_reward = self.board.turn * winner\n",
    "\n",
    "        # Dense reward\n",
    "        def dense_reward_for_turn(board, turn):\n",
    "            r = 0\n",
    "            for length in range(2, self.board_size):\n",
    "                r += math.log(length) * board.num_connected(length, turn)\n",
    "            return r\n",
    "        ego_dense_reward = dense_reward_for_turn(self.board, self.board.turn)\n",
    "        opponent_dense_reward = dense_reward_for_turn(self.board, -self.board.turn)\n",
    "        dense_reward = ego_dense_reward - opponent_dense_reward  \n",
    "\n",
    "        if self.reward_mode == 'sparse':\n",
    "            reward = sparse_reward\n",
    "        elif self.reward_mode == 'dense_stateless':\n",
    "            reward = dense_reward\n",
    "        elif self.reward_mode == 'dense_advantage':\n",
    "            turn_index = (self.board.turn+1)//2\n",
    "            reward = dense_reward - self.prev_dense_reward[turn_index]\n",
    "            self.prev_dense_reward[turn_index] = dense_reward\n",
    "        else:\n",
    "            raise ValueError(f'Invalid reward_mode: {self.reward_mode}')\n",
    "\n",
    "        # Evaluate whether game is over\n",
    "        winner = self.board.check_win()\n",
    "        done = winner != 0\n",
    "\n",
    "        # Record debugging info\n",
    "        info = dict()\n",
    "\n",
    "        # Revert temporary flip on `board.turn`\n",
    "        self.board.turn *= -1\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=None,\n",
    "            next_obs=obs, \n",
    "            action=None,\n",
    "            reward=reward, \n",
    "            done=done, \n",
    "            info=info\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "    def _make_obs(self) -> np.ndarray:\n",
    "        \"\"\"Only show ego values on first channel and opponent values on second channel\"\"\"\n",
    "        obs = np.stack([\n",
    "            self.board.turn * self.board.board, \n",
    "            -self.board.turn * self.board.board\n",
    "            ], axis=-1)  # [board_size, board_size, 2]\n",
    "        obs[obs<0] = 0  # rectify negative values\n",
    "        return obs\n",
    "\n",
    "board_size = 10\n",
    "env = BoardEnv(board_size=board_size, win_length=5, reward_mode='dense_advantage')\n",
    "\n",
    "step = env.reset()\n",
    "while not step.done:\n",
    "    action = np.random.uniform(0, 1, (board_size,))\n",
    "    step = env.step(action)\n",
    "\n",
    "    print(f'Action: {action}')\n",
    "    env.render()\n",
    "    print(f'Reward: {step.reward}\\n')\n",
    "\n",
    "print(f'Winner: {env.board.winner}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_HPARAMS = dict(\n",
    "    board_size=8,           # Board size\n",
    "    discount=0.99,          # Discount factor\n",
    "    learning_rate=0.001,    # Learning rate\n",
    "    batch_size=32,          # Number of samples per training batch\n",
    "    train_freq=10000,       # Number of timesteps between training steps\n",
    "    epoch=0,                # Current epoch\n",
    "    epochs=10,              # Number of training epochs\n",
    "    epsilon_start=1.0,      # Starting value for epsilon\n",
    "    min_epsilon=0.01,       # Final value for epsilon\n",
    "    epsilon_decay=0.95,     # Decay rate for epsilon per epoch\n",
    ")\n",
    "\n",
    "INITIAL_HPARAMS['optimizer'] = \\\n",
    "    SGD(INITIAL_HPARAMS['learning_rate'])   # Optimizer\n",
    "\n",
    "\n",
    "HPARAMS = INITIAL_HPARAMS.copy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "605fe966a75bc2c3dfa708e269323e6491854b30a36f4e77953579e94649bfba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
