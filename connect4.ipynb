{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "import functools\n",
    "from typing import Tuple, List, Mapping, Optional, Union, NamedTuple, Callable\n",
    "# Many default parameters are included in jnumpy and are optional.\n",
    "# I only resort to using `Optional` in the type annotations where the\n",
    "# context does not make this clear.  \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jnumpy: Jacob's numpy library for machine learning\n",
    "# Copyright (c) 2021 Jacob F. Valdez. Released under the MIT license.\n",
    "\n",
    "\n",
    "V = np.array # V is for Value type\n",
    "Vs = Tuple[V]\n",
    "Vss = Union[V,Vs]\n",
    "\n",
    "\n",
    "class ExecutionMode:\n",
    "    EAGER=1\n",
    "    STATIC=2  # STATIC execution mode not supported\n",
    "    \n",
    "EXECUTION_MODE = ExecutionMode.EAGER\n",
    "\n",
    "\n",
    "class T:\n",
    "    \"\"\"Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, val: Optional[V] = None):\n",
    "        self.val = val\n",
    "        \n",
    "        if val is None:\n",
    "            raise 'STATIC execution mode not supported'\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return eval(\"Index\")(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        raise NotImplementedError('slice assign not yet supported')\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return eval(\"Add\")(self, other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return eval('Neg')(self)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return eval(\"Sub\")(self, other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return eval(\"Mul\")(self, other)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        return eval(\"Pow\")(self, other)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        return eval(\"MatMul\")(self, other)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.val.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.val.ndim\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.val.size\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.val.dtype\n",
    "\n",
    "    @property\n",
    "    def T(self, axes: Tuple[int] = None):\n",
    "        return eval(\"Transpose\")(self, axes=axes)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Tensor({self.val})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.val == other.val\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.val)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.val)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.val)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.val.__getstate__()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.val = state\n",
    "\n",
    "    def __array__(self):\n",
    "        return self.val.__array__()\n",
    "\n",
    "\n",
    "Ts = Tuple[T]\n",
    "Tss = Union[T,Ts]\n",
    "\n",
    "\n",
    "class Var(T):\n",
    "    \"\"\"Variable Tensor\"\"\"\n",
    "    def __init__(self, val: Optional[V] = None, trainable: bool = True):\n",
    "        \n",
    "        self.trainable = trainable\n",
    "        super().__init__(val=val)\n",
    "\n",
    "\n",
    "class Op(T):\n",
    "    \"\"\"Operation-backed Tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, *inputs: T):\n",
    "        \"\"\"Make sure to set any variables you might need in `forward` \n",
    "        before initializing when the graph is in eager execution mode\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_ts = inputs\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.EAGER:\n",
    "            val = self.forward(tuple(i.val for i in inputs))\n",
    "        else:\n",
    "            val = None\n",
    "        \n",
    "        super().__init__(val=val)\n",
    "        \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        raise NotImplementedError('subclasses should implement this method')\n",
    "\n",
    "\n",
    "class Transpose(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axes: Tuple[int] = None):\n",
    "        \n",
    "        self.forward_kwargs = dict()\n",
    "        self.reverse_kwargs = dict()\n",
    "        \n",
    "        if axes is not None:\n",
    "            self.forward_kwargs['axes'] = axes\n",
    "            self.reverse_kwargs['axes'] = tuple(reversed(axes))\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.transpose(**self.forward_kwargs)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "\n",
    "        dX = dY.transpose(**self.reverse_kwargs)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Reshape(Op):\n",
    "    \n",
    "    def __init__(self, t: T, shape: Tuple[int]):\n",
    "        \n",
    "        self.reshape_shape = shape\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.reshape(self.reshape_shape)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = dY.reshape(tuple(reversed(self.reshape_shape)))\n",
    "        \n",
    "        return (dX,)\n",
    " \n",
    "\n",
    "class Concat(Op):\n",
    "    \n",
    "    def __init__(self, ts: List[T], axis: int = 0):\n",
    "        \"\"\"Concatenates input tensors along an axis\n",
    "\n",
    "        Args:\n",
    "            t (T): [description]\n",
    "            axis (int, optional): Axis to concatenate along. Defaults to 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.axis = axis\n",
    "        self.orig_axis_lens = [t.shape[axis] for t in ts]\n",
    "\n",
    "        super().__init__(*ts)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        Xs = inputs\n",
    "        \n",
    "        Y = np.concatenate(Xs, axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dY = top_grad\n",
    "        \n",
    "        dXs = np.split(dY, self.orig_axis_dims, axis=self.axis)[0]\n",
    "        \n",
    "        return dXs\n",
    "\n",
    "\n",
    "class Index(Op):\n",
    "    \n",
    "    def __init__(self, t: T, indices):\n",
    "        \"\"\"Slices a tensor along all axes.\n",
    "\n",
    "        Args:\n",
    "            t (T): The tensor to slice\n",
    "            indices (Tuple[slice]):  The partial or full indices to slice on `t`.\n",
    "                Can be an index, single slice, tuple of slices, or Ellipsis.\n",
    "                `None` is not allowed.\n",
    "        \"\"\"\n",
    "        if not isinstance(indices, tuple):\n",
    "            indices = (indices,)\n",
    "\n",
    "        self.indices = indices\n",
    "        \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X[self.indices]\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.zeros(X.shape)\n",
    "        dX[self.indices] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceSum(Op):\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.sum(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.repeat(\n",
    "            np.expand_dims(dY, axis=self.axis),\n",
    "            X.shape[self.axis],\n",
    "            axis=self.axis\n",
    "        )\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class ReduceMax(Op):\n",
    "    \"\"\"Differentiable max operator\"\"\"\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.max(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        print(X.shape, dY.shape)\n",
    "\n",
    "        dX = np.zeros_like(X)\n",
    "        dX[np.argmax(X, axis=self.axis)] = dY\n",
    "\n",
    "        print(dX.shape)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "class ReduceMin(Op):\n",
    "    \"\"\"Differentiable min operator\"\"\"\n",
    "    \n",
    "    def __init__(self, t: T, axis: int):\n",
    "        self.axis = axis\n",
    "            \n",
    "        super().__init__(t)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Y = X.min(axis=self.axis)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = np.zeros_like(X)\n",
    "        dX[np.argmin(X, axis=self.axis)] = dY\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class NaN2Num(Op):\n",
    "    \n",
    "    def __init__(self, t: T, posinf: float = 1e3, neginf: float = -1e3):\n",
    "        self.posinf = posinf\n",
    "        self.neginf = neginf\n",
    "            \n",
    "        super().__init__(t)\n",
    "\n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.nan_to_num(X, posinf=self.posinf, neginf=self.neginf)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = np.nan_to_num(dZ, posinf=10., neginf=-10.)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Linear(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class StopGrad(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = np.zeros_like(dZ)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Neg(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = -X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = -dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Add(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X + Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Sub(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X - Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        dY = -dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Mul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X * Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Y * dZ\n",
    "        dY = X * dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class MatMul(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        Y = inputs[1]\n",
    "        \n",
    "        Z = X @ Y\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]  # [A,B]\n",
    "        Y = inputs[1]  # [B,C]\n",
    "        dZ = top_grad  # [A,C]\n",
    "        \n",
    "        dX = dZ @ Y.transpose()\n",
    "        dY = X.transpose() @ dZ\n",
    "        \n",
    "        return dX, dY\n",
    "\n",
    "\n",
    "class Exp(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.exp(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Z * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Sigm(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = 1 / (1 + np.exp(-X))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = Z * (1 - Z) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Tanh(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = np.tanh(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        Z = output\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = ((1 - Z)**2) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Relu(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X > 0) * X\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = (X > 0) * dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Threshold(Op):\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        \n",
    "        Z = (X >= 0)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        dZ = top_grad\n",
    "        \n",
    "        dX = dZ\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Pow(Op):\n",
    "    \n",
    "    def __init__(self, x: T, power: int):\n",
    "        \n",
    "        self.power = power\n",
    "        \n",
    "        super().__init__(x)\n",
    "    \n",
    "    def forward(self, inputs: Vs) -> V:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        \n",
    "        Y = X ** p\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def reverse_grad(self, inputs: Vs, output: V, top_grad: V) -> Vs:\n",
    "        X = inputs[0]\n",
    "        p = self.power\n",
    "        dY = top_grad\n",
    "        \n",
    "        dX = p * X ** (p-1) * dY\n",
    "        dX = np.nan_to_num(dX, posinf=1e3, neginf=-1e3)\n",
    "        \n",
    "        return (dX,)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, lr: float = 0.001):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.debug = False\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    def minimize(self, t: T):\n",
    "        \n",
    "        if EXECUTION_MODE == ExecutionMode.STATIC:\n",
    "            raise 'STATIC execution mode not enabled'\n",
    "        \n",
    "        self.bprop(t_out=t, output_grad=-np.ones_like(t.val))\n",
    "        \n",
    "    def bprop(self, t_out: T, output_grad: V):\n",
    "        \n",
    "        output_grad = np.nan_to_num(output_grad, posinf=10., neginf=-10.)\n",
    "        \n",
    "        assert isinstance(t_out, (Var, Op))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f'bp {t_out} output_grad:')\n",
    "            print(output_grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        This approach does not efficiently handle weights that are consumed by multiple nodes\n",
    "        It would be better to treat backpropagation from a spreading-network-delta perspective\n",
    "        than assume everything is a tree (That's also how I should do STATIC execution refresh)\n",
    "        This should still work though, but it's just going to set the same weight multiple times\n",
    "        for every downstream consumer.\n",
    "\n",
    "        Actually, the whole thesis of minibatch gradient descent is that we can approximate a global\n",
    "        gradient by updates on local subsets of data, so it might be sufficient to leave the code\n",
    "        as is.\n",
    "\n",
    "        However this approach will still take unnecessary walks down the tree in depth-first fashion.\n",
    "        Innefficient: Yes; Works: Yes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # iteratively called\n",
    "        if isinstance(t_out, Var):\n",
    "            if t_out.trainable:\n",
    "                #print('output_grad', output_grad.shape)\n",
    "                if self.debug:                    \n",
    "                    print('t_out.val (old)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "                \n",
    "                # yucky duct tape to handle batch size differences\n",
    "                if t_out.shape[0] == 1 and output_grad.shape[0] > 1:\n",
    "                    output_grad = np.sum(output_grad, axis=0)[None, ...]\n",
    "\n",
    "                t_out.val = t_out.val + (self.lr * output_grad)\n",
    "                if self.debug:\n",
    "                    print('t_out.val (new)', t_out.shape)\n",
    "                    print(t_out.val)\n",
    "            \n",
    "        elif isinstance(t_out, Op):\n",
    "            input_grads = t_out.reverse_grad(\n",
    "                inputs=tuple(t.val for t in t_out.input_ts),\n",
    "                output=t_out.val, top_grad=output_grad)\n",
    "            \n",
    "            for input_t, input_grad in zip(t_out.input_ts, input_grads):\n",
    "                self.bprop(t_out=input_t, output_grad=input_grad)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._built = False\n",
    "        self._loss = Var(np.zeros(()), trainable=False)\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return self._loss\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return []\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, X_T: T) -> T:\n",
    "        if not self._built:\n",
    "            self.build(X_T.shape)\n",
    "            self._built = True\n",
    "\n",
    "        # reset the regularization loss\n",
    "        self._loss = Var(0, trainable=False)\n",
    "\n",
    "        return self.forward(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.3072007 , 0.79667958, 0.32161908, 0.04560883, 0.54219024],\n",
       "        [0.57148128, 0.90783496, 0.92876637, 0.62582609, 0.06817921],\n",
       "        [0.08946016, 0.97032181, 0.19728185, 0.19829606, 0.19574553]]),\n",
       " array([[ 0.0060564 , -0.        , -0.        , -0.        , -0.        ,\n",
       "          0.0918947 ,  0.02878251,  0.00487777, -0.        ,  0.02379378],\n",
       "        [ 0.00571611,  0.01224815,  0.01458366, -0.        , -0.        ,\n",
       "          0.08963543,  0.04480869,  0.03639306, -0.        , -0.        ],\n",
       "        [ 0.01158474, -0.        , -0.        , -0.        , -0.        ,\n",
       "          0.08972044,  0.02324845,  0.00316236, -0.        ,  0.01159636]]),\n",
       " Tensor(0.006613686518620811),\n",
       " [Tensor([[ 0.03371712  0.02604198  0.03234285 -0.01653742  0.008084   -0.02990792\n",
       "    -0.02552615  0.02240365  0.02087141 -0.02666338]\n",
       "   [-0.01957707 -0.01262434 -0.04861429 -0.02491815 -0.04081946  0.03385342\n",
       "     0.00111092 -0.01957252 -0.03142139 -0.00374834]\n",
       "   [-0.03198259 -0.01708203  0.02188513  0.00849423 -0.01634186  0.030193\n",
       "     0.04824725  0.00955953  0.0058895  -0.02888293]\n",
       "   [-0.01205441  0.03250964 -0.0239538   0.04597536 -0.01679718 -0.00446642\n",
       "     0.00158388  0.0306008   0.01785719 -0.0469996 ]\n",
       "   [-0.04079515 -0.01265156 -0.02832084 -0.01002949 -0.01340822  0.02923621\n",
       "     0.01595623 -0.00888351 -0.04267873  0.03973861]]),\n",
       "  Tensor([[ 0.04424986  0.00520888  0.03682971 -0.03313113 -0.03435441  0.04875358\n",
       "     0.01149833  0.01393473 -0.01500351  0.02485803]])])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "        units: int, \n",
    "        activation: Op = None, \n",
    "        use_bias: bool = True,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None\n",
    "    ):\n",
    "        super(Dense, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "            \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(input_shape[-1], self.units))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.units))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # compute presynaptic input\n",
    "        Z_T = X_T @ self.W_T\n",
    "\n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply activation\n",
    "        Y_T = self.activation(Z_T)\n",
    "        \n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Dense(10, Relu, 0.1, 0.1, 0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(3, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.val, Y_T.val, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 7, 70, 5),\n",
       " (2, 7, 70, 64),\n",
       " Tensor(0.6682137699398806),\n",
       " [Tensor([[ 0.04065261 -0.01437801 -0.04597341 ... -0.00153266 -0.02785666\n",
       "     0.00320357]\n",
       "   [ 0.00062456  0.03526321  0.03966993 ... -0.00608235 -0.02152944\n",
       "    -0.01573131]\n",
       "   [-0.01041489  0.01313308  0.00083359 ...  0.04932316 -0.00020939\n",
       "     0.0044856 ]\n",
       "   ...\n",
       "   [-0.02714797  0.0121087   0.03520785 ... -0.03685715  0.02308696\n",
       "     0.02900663]\n",
       "   [-0.02597731  0.03333598  0.01159071 ...  0.01315679  0.03064069\n",
       "     0.04843758]\n",
       "   [-0.00487441 -0.03882254 -0.026726   ...  0.01779648 -0.04461619\n",
       "     0.00332976]])])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv2D(Layer):\n",
    "    \"\"\"Standard 2D Conv layer.\n",
    "    I.E. convolves over Tensors shaped [B, H, W, D]\n",
    "    to produce [B, H-2*kernel_size, W-2*kernel_size, filters]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        filters: int, \n",
    "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
    "        strides: Union[int, Tuple[int, int]] = 1,\n",
    "        padding: str = 'valid',  # 'valid' or 'same'\n",
    "        activation: Op = None,\n",
    "        use_bias: bool = False,\n",
    "        activity_L2: float = None, \n",
    "        weight_L2: float = None,  \n",
    "        bias_L2: float = None,\n",
    "    ):\n",
    "        super(Conv2D, self).__init__()\n",
    "\n",
    "        if activation is None:\n",
    "            activation = Linear\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        assert kernel_size[0] % 2 == 1 and kernel_size[1] % 2 == 1, 'kernel_size must be odd'\n",
    "        if isinstance(strides, int):\n",
    "            strides = (strides, strides)\n",
    "        assert strides[0] > 0 and strides[1] > 0, 'strides must be positive'\n",
    "        padding = padding.lower()\n",
    "        assert padding in ('valid', 'same'), 'padding must be valid or same'\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.activity_L2 = Var(activity_L2, trainable=False) if activity_L2 is not None else None\n",
    "        self.weight_L2 = Var(weight_L2, trainable=False) if weight_L2 is not None else None\n",
    "        self.bias_L2 = Var(bias_L2, trainable=False) if bias_L2 is not None else None\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return [self.W_T] + ([self.B_T] if self.use_bias else [])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        W = np.random.uniform(low=-0.05, high=0.05, size=(\n",
    "            self.kernel_size[0]*self.kernel_size[1]*input_shape[-1], \n",
    "            self.filters))\n",
    "        self.W_T = Var(val=W, trainable=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            B = np.random.uniform(low=-0.05, high=0.05, size=(1, self.filters))\n",
    "            self.B_T = Var(val=B, trainable=True)\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "\n",
    "        # maybe pad input\n",
    "        if self.padding == 'same':\n",
    "\n",
    "            # various padding sizes, strides, and offsets\n",
    "            # 0   1   2   3   4\n",
    "            #     0 1 2 3 4\n",
    "            #         0 1 2 3 4 5 6 7 8 9\n",
    "            #         0 1 2 3 4\n",
    "            #         0   1   2   3   4\n",
    "\n",
    "            pad_top = self.strides[0]*(self.kernel_size[0]-1)//2\n",
    "            pad_bottom = pad_top\n",
    "            pad_left = self.strides[1]*(self.kernel_size[1]-1)//2\n",
    "            pad_right = pad_left\n",
    "            B, H_orig, W_orig, C = X_T.shape\n",
    "\n",
    "            # pad height\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, pad_top, W_orig, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, pad_bottom, W_orig, C)), trainable=False),\n",
    "            ], axis=1)\n",
    "\n",
    "            # pad width\n",
    "            X_T = Concat([\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_left, C)), trainable=False),\n",
    "                X_T,\n",
    "                Var(np.zeros((B, H_orig+pad_top+pad_bottom, pad_right, C)), trainable=False),\n",
    "            ], axis=2)\n",
    "\n",
    "        elif self.padding == 'valid':\n",
    "            pass\n",
    "\n",
    "        # stack the input tensor along the channel axis\n",
    "        # but shifted by all possible kernel shifts\n",
    "        stack = []\n",
    "        for shift in itertools.product(range(0, self.strides[0]*self.kernel_size[0], self.strides[0]),\n",
    "                                       range(0, self.strides[1]*self.kernel_size[1], self.strides[1])):\n",
    "            stack.append(X_T[\n",
    "                :,\n",
    "                shift[0]:,\n",
    "                shift[1]:,\n",
    "                :\n",
    "            ])\n",
    "\n",
    "        # clip stack to greatest common shape\n",
    "        min_shape = np.min(np.array([s.shape for s in stack]), axis=0)\n",
    "        stack = [s[:, :min_shape[1], :min_shape[2], :] for s in stack]\n",
    "\n",
    "        # stack the shifted tensors along the channel axis\n",
    "        stacked = Concat(stack, axis=3)  # [B, H-k_h//2, W-k_w//2, C*k_h*k_w]\n",
    "\n",
    "        # convolve over the stacked tensors\n",
    "        Z_T = stacked @ self.W_T  \n",
    "    \n",
    "        # maybe add bias\n",
    "        if self.use_bias:\n",
    "            Z_T = Z_T + self.B_T\n",
    "\n",
    "        # apply the activation function\n",
    "        Y_T = self.activation(Z_T)\n",
    "\n",
    "        # track regularization losses\n",
    "        if self.activity_L2 is not None:\n",
    "            self._loss += self.activity_L2 * ReduceSum(ReduceSum(Y_T**2, 1), 0)\n",
    "        if self.weight_L2 is not None:\n",
    "            self._loss += self.weight_L2 * ReduceSum(ReduceSum(self.W_T**2, 1), 0)\n",
    "        if self.use_bias and self.bias_L2 is not None:\n",
    "            self._loss += self.bias_L2 * ReduceSum(ReduceSum(self.B_T**2, 1), 0)\n",
    "\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Conv2D(filters=64, kernel_size=5, strides=5, padding='same', weight_L2=0.1)\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 7, 70, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxisMaxPooling(Layer):\n",
    "\n",
    "    def __init__(self, axis: int):\n",
    "        super(AxisMaxPooling, self).__init__()\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        return ReduceMax(X_T, self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(Layer):\n",
    "\n",
    "    def __init__(self, fn: Callable[[T], T]):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        return self.fn(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3, 4, 5), (2, 60), Tensor(0), [])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Flatten(Layer):\n",
    "    \"\"\"Flattens all non-batch dimensions into a single axis\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        return []\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        flat_dims = functools.reduce(lambda x, y: x*y, X_T.shape[1:])\n",
    "        Y_T = Reshape(X_T, (X_T.shape[0], flat_dims))\n",
    "        return Y_T\n",
    "\n",
    "\n",
    "layer = Flatten()\n",
    "X_T = Var(np.random.uniform(0, 1, size=(2, 3, 4, 5)), trainable=False)\n",
    "Y_T = layer(X_T)\n",
    "X_T.shape, Y_T.shape, layer.loss, layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[0.26675477]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        super(Sequential, self).__init__()\n",
    "\n",
    "    def forward(self, X_T: T) -> T:\n",
    "        for layer in self.layers:\n",
    "            X_T = layer(X_T)\n",
    "        return X_T\n",
    "\n",
    "    @property\n",
    "    def loss(self) -> T:\n",
    "        return sum(layer.loss for layer in self.layers)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self) -> List[T]:\n",
    "        trainable_vars = []\n",
    "        for layer in self.layers:\n",
    "            trainable_vars += layer.trainable_variables\n",
    "        return trainable_vars\n",
    "\n",
    "\n",
    "net = Sequential([\n",
    "    Dense(10, Relu),\n",
    "    Dense(128, Relu),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(X_T)\n",
    "\n",
    "img_T = Var(np.random.uniform(0, 1, size=(1, 28, 28, 1)), trainable=False)\n",
    "net = Sequential([\n",
    "    Conv2D(32, 3, 2, activation=Relu),\n",
    "    Conv2D(64, 3, 2, activation=Relu),\n",
    "    Flatten(),\n",
    "    Dense(512, Sigm),\n",
    "    Dense(1, lambda x: x)\n",
    "])\n",
    "net(img_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standards:\n",
    "- `Step`: uses batch dimension (except `done` which is always a bool)\n",
    "- `Agent` uses batch dimension\n",
    "- `Environment` doesn't use batch dimension\n",
    "\n",
    "This means you will have to use `Step.batch` and `Step.unbatch` in your training/running loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(NamedTuple):\n",
    "    \"\"\"Single step.\"\"\"\n",
    "    \n",
    "    obs: np.ndarray\n",
    "    next_obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: np.ndarray\n",
    "    done: bool\n",
    "    info: any\n",
    "\n",
    "    @staticmethod\n",
    "    def unbatch(step: Step) -> List[Step]:\n",
    "        return [\n",
    "            Step(\n",
    "                obs=step.obs[i:i+1],\n",
    "                next_obs=step.next_obs[i:i+1],\n",
    "                action=step.action[i:i+1],\n",
    "                reward=step.reward[i:i+1],\n",
    "                done=step.done,\n",
    "                info=step.info[i:i+1],\n",
    "            )\n",
    "            for i in range(step.obs.shape[0])\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def batch(steps: List[Step]) -> Step:\n",
    "        return Step(\n",
    "            obs=np.concatenate([step.obs for step in steps], axis=0),\n",
    "            next_obs=np.concatenate([step.next_obs for step in steps], axis=0),\n",
    "            action=np.concatenate([step.action for step in steps], axis=0),\n",
    "            reward=np.concatenate([step.reward for step in steps], axis=0),\n",
    "            done=any(step.done for step in steps),\n",
    "            info=[step.info for step in steps])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_no_batch_axis(step: NoBatchStep) -> Step:\n",
    "        return Step(\n",
    "            obs=step.obs[None, ...],\n",
    "            next_obs=step.next_obs[None, ...],\n",
    "            action=step.action[None, ...],\n",
    "            reward=step.reward[None, ...],\n",
    "            done=step.done,\n",
    "            info=[step.info]\n",
    "        )\n",
    "\n",
    "# dimensional type hinting\n",
    "BatchStep = Step\n",
    "NoBatchStep = Step\n",
    "\n",
    "Traj = List[BatchStep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- replace `list` with List and same for dict/mapping\n",
    "- use one-hot encodings for all actions and rewire the policies to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"RL environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        \"\"\"Resets the environment\n",
    "\n",
    "        Returns:\n",
    "            Step: Initial step. The `next_obs` attribute should be set \n",
    "                with an initial observation. `done` should be False. \n",
    "                `obs` and `action` should not be used.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Step:\n",
    "        \"\"\"Computes one logical step in the environment\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): The action to take.\n",
    "\n",
    "        Returns:\n",
    "            Step: Step resulting from taking `action`. The `next_obs` attribute\n",
    "                should be set with the observation resulting from taking the `action`\n",
    "                in the current environment state. `obs` should not be used. If the \n",
    "                environment is turn-based, then the reward should correspond to the \n",
    "                agent that just acted (not the next agent in line to act).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NoBatchEnv(Environment):\n",
    "    \"\"\"Environment with no batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> NoBatchStep:\n",
    "        pass\n",
    "\n",
    "\n",
    "class BatchEnv(Environment):\n",
    "    \"\"\"Environment with batch dimension.\"\"\"\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> BatchStep:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch1Env(BatchEnv):\n",
    "    \"\"\"Adds a batch axis to all outgoing Steps and strips it off incoming Steps.\"\"\"\n",
    "\n",
    "    def __init__(self, env: NoBatchEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.reset())\n",
    "\n",
    "    def step(self, action: np.array) -> BatchStep:\n",
    "        return Step.from_no_batch_axis(self.env.step(action[0]))\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEnv(BatchEnv):\n",
    "    \"\"\"Keeps reseting the same environment in a batch.\n",
    "    \n",
    "    Declares itself done when a total of `batch_size` individual environment\n",
    "    dones are experienced.\n",
    "\n",
    "    NOTE: Individual environments should be `NoBatchEnv`'s.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, env_init_fn: Callable[[], NoBatchEnv]):\n",
    "        self.batch_size = batch_size\n",
    "        self.env_init_fn = env_init_fn\n",
    "\n",
    "    def reset(self) -> Step:\n",
    "        self.dones = 0\n",
    "        self.envs = [self.env_init_fn() for _ in range(self.batch_size)]\n",
    "        steps = [env.reset() for env in self.envs]\n",
    "        steps = [Step.from_no_batch_axis(step) for step in steps]\n",
    "        return Step.batch(steps)\n",
    "\n",
    "    def step(self, action: np.array) -> Step:\n",
    "        steps = []\n",
    "        for i, (env, single_action) in enumerate(zip(self.envs, action)):\n",
    "            step = env.step(single_action)\n",
    "            if step.done:\n",
    "                self.envs[i] = self.env_init_fn()\n",
    "                new_step = env.reset()\n",
    "                step.next_obs = new_step.next_obs\n",
    "                self.dones += 1\n",
    "            steps.append(Step.from_no_batch_axis(step))\n",
    "\n",
    "        batched_step = Step.batch(steps)\n",
    "        batched_step.done = self.dones >= self.batch_size\n",
    "        return batched_step\n",
    "\n",
    "    def render(self):\n",
    "        for env in self.envs:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer.\n",
    "\n",
    "    Expects the following hyperparameters:\n",
    "        - `epoch`: The current epoch.\n",
    "        - `batch_size`: Number of trajectories to return at each call.\n",
    "        - `min_sample_len`: Minimum length of trajectories to sample.\n",
    "        - `max_sample_len`: Maximum length of trajectories to sample.\n",
    "        - `num_steps_replay_coef`: Sampling coefficient based on trajectory length.\n",
    "        - `success_replay_coef`: Sampling coefficient based on trajectory success.\n",
    "        - `age_replay_coef`: Sampling coefficient based on trajectory age.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, hparams: dict):\n",
    "        self.hparams = hparams\n",
    "        self.trajs = dict()\n",
    "\n",
    "    @property\n",
    "    def flat_traj(self):\n",
    "        return [step for traj in self.trajs for step in traj]\n",
    "\n",
    "    def add(self, traj: Traj, epoch: int):\n",
    "        \"\"\"Add a new trajectory to the buffer.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): the trajectory to add.\n",
    "            epoch (int): the epoch that the trajectory was experience in.\n",
    "        \"\"\"\n",
    "        if epoch not in self.trajs:\n",
    "            self.trajs[epoch] = []\n",
    "        self.trajs[epoch] += traj\n",
    "\n",
    "    def sample(self) -> Traj:\n",
    "        \"\"\"Samples a batched trajectory from the buffer stochastically based on:\n",
    "            - the number of steps in the trajectory (num_steps_replay_coef)\n",
    "            - how well the agent did in the trajectory (success_replay_coef)\n",
    "            - how long ago the trajectory was experienced (age_replay_coef)\n",
    "\n",
    "        Returns:\n",
    "            Traj: a trajectory of batched steps experienced.\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = {\n",
    "            epoch: self.hparams['num_steps_replay_coef'] * len(traj) +\n",
    "                   self.hparams['success_replay_coef'] * sum(step.reward for step in traj) +\n",
    "                   self.hparams['age_replay_coef'] * (epoch - self.hparams['epoch'])\n",
    "            for epoch, traj in self.trajs.items()\n",
    "        }\n",
    "        epochs = np.random.choice(list(weights.keys()), size=(), replace=True, p=list(weights.values()))\n",
    "        trajs = [self.trajs[epoch] for epoch in epochs]\n",
    "        batched_traj = [BatchStep.batch(steps) for steps in zip(*trajs)]\n",
    "        return batched_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, policy: Callable):\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, step: BatchStep) -> np.ndarray:\n",
    "        \"\"\"Generates an action for a given observation using `self.policy`. \n",
    "        Override if you want to give your policy more information such as\n",
    "        recurrent state or previous reward.\n",
    "\n",
    "        Args:\n",
    "            step (Step): last step output by the environment. This means the agent\n",
    "                should feed `step.next_obs`, not `step.obs` to its policy. If the \n",
    "                environment is multi-agent, then the `reward` attribute has already\n",
    "                updated to reflect the reward for this agent by the driver.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The action to take\n",
    "        \"\"\"\n",
    "        return self.policy(step.next_obs)\n",
    "\n",
    "    def reward(self, traj: Traj) -> float:\n",
    "        \"\"\"Evaluates the cumulative reward for your agent as the sum of \n",
    "        individual rewards experienced. \n",
    "        \n",
    "        If your agent uses intrinsic rewards, be sure to add them in here.\n",
    "        Do not introduce Q-values or predicted rewards here.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "\n",
    "        Returns:\n",
    "            float: Cumulative (sum) reward over the entire sequence.\n",
    "        \"\"\"\n",
    "        return sum(step.reward for step in traj)\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        raise NotImplemented('Method `train` must be implemented by subclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelDriver:\n",
    "    \"\"\"Drives batched turn-based `BatchedEnv` environments with multiple agents.\n",
    "    Also supports single-agent environments as a special case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def drive(self, agents: Mapping[str, Agent], env: Environment) -> Mapping[str, Traj]:\n",
    "        \"\"\"Drives a batched environment with multiple agents.\n",
    "        \n",
    "        Args:\n",
    "            agents (Mapping[str, Agent]): A dictionary of agents to drive.\n",
    "            env (Environment): The environment to drive.\n",
    "\n",
    "        Returns:\n",
    "            Mapping[str, Traj]: A dictionary of trajectories for each agent.\n",
    "                Each trajectory is completely disengaged from the other agent's.\n",
    "                (i.e.: the obs, next_obs, action, reward, done attributes are\n",
    "                individual to each agent for each trajectory.)\n",
    "        \"\"\"\n",
    "\n",
    "        names_it = itertools.cycle(agents.keys())\n",
    "        trajs = {agent_name: [] for agent_name in agents}\n",
    "        prev_rewards = {agent_name: 0. for agent_name in agents}\n",
    "\n",
    "        step = env.reset()\n",
    "        while not step.done:\n",
    "            agent_name = next(names_it)\n",
    "            \n",
    "            # `Agent.forward` only looks at `step.next_obs` and `step.reward`\n",
    "            # but I'm assigning defaults just to be safe.\n",
    "            action = agents[agent_name].forward(Step(\n",
    "                obs=step.obs,  # what the previous agent saw before acting\n",
    "                next_obs=step.next_obs,  # what the current agent sees before acting\n",
    "                reward=prev_rewards[agent_name],  # the reward this agent experienced following its last action\n",
    "                done=step.done,  # whether the environment was done after the previous agent acted\n",
    "                info=step.info  # any extra information the environment might have output\n",
    "            )) \n",
    "\n",
    "            prev_step = step\n",
    "            step = env.step(action)  # `Environment.step` produces a Step with all fields except `step.obs` set\n",
    "            step.obs = prev_step.next_obs  # the current agent's observation is the previous agent's next observation\n",
    "            prev_rewards[agent_name] = step.reward  # the reward for the action the current agent just took\n",
    "            trajs[agent_name].append(step)  # Step completely corresponding to this agent (obs before action, obs after action, action, reward, done, info)\n",
    "        \n",
    "        return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTrainer:\n",
    "    \"\"\"Trains `BatchedEnv` environments and mutliple agents \n",
    "    (with N=1 single-agent supported as a special case).\n",
    "    \n",
    "    Uses following hyperparameters:\n",
    "    - `epoch`: the current epoch. Reads and writes to this variable.\n",
    "    - `epochs`: the number of epochs to train for.\n",
    "    - `min_steps_per_epoch`: the minimum number of steps to train for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams: dict, callbacks: List[Callable]):\n",
    "        self.hparams = hparams\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "    def train(self, \n",
    "        agents: Mapping[str, Agent], \n",
    "        env: Environment,\n",
    "        test_env: Environment = None,\n",
    "        buffers: Mapping[str, ReplayBuffer] = None,\n",
    "        collect_driver: ParallelDriver = None,\n",
    "        test_driver: ParallelDriver = None,\n",
    "        histories: Mapping[str, Mapping[int, Mapping[str, any]]] = None,\n",
    "        ) -> Mapping[int, Mapping[str, any]]:\n",
    "\n",
    "        agent_names = list(agents.keys())\n",
    "        \n",
    "        # initialize defaults\n",
    "        if test_env is None:\n",
    "            test_env = env\n",
    "        if buffers is None:\n",
    "            buffers = dict()\n",
    "        if collect_driver is None:\n",
    "            collect_driver = ParallelDriver()\n",
    "        if test_driver is None:\n",
    "            test_driver = collect_driver\n",
    "        if histories is None:\n",
    "            histories = dict()  # {agent_name: {epoch: {...data}}}\n",
    "\n",
    "        # build uninitialized agent-specific objects\n",
    "        for agent_name in agent_names:\n",
    "            if agent_name not in buffers:\n",
    "                buffers[agent_name] = ReplayBuffer()\n",
    "            if agent_name not in histories:\n",
    "                histories[agent_name] = dict()\n",
    "\n",
    "        # run training loop\n",
    "        for epoch in range(self.hparams['epoch'], self.hparams['epochs']):\n",
    "            self.hparams['epoch'] = epoch\n",
    "\n",
    "            # collect trajectories\n",
    "            steps = 0\n",
    "            while steps < self.hparams['steps_per_epoch']:\n",
    "                collect_trajs = collect_driver.drive(agents, env)\n",
    "                steps += min(len(traj) for _, traj in collect_trajs.items())\n",
    "                for agent_name in agent_names:\n",
    "                    buffers[agent_name].add(collect_trajs[agent_name])\n",
    "\n",
    "            # train\n",
    "            train_trajs = {agent_name: buffers[agent_name].sample() for agent_name in agent_names}\n",
    "            for agent_name in agent_names:\n",
    "                agents[agent_name].train(train_trajs[agent_name])\n",
    "                \n",
    "            # test\n",
    "            test_trajs = test_driver.drive(agents, env)\n",
    "\n",
    "            # record history and run callbacks\n",
    "            for agent_name in agent_names:\n",
    "                histories[agent_name][epoch] = {\n",
    "                    'epoch': epoch,\n",
    "                    'agent': agents[agent_name],\n",
    "                    'all_agents': agents,\n",
    "                    'env': env,\n",
    "                    'test_env': test_env,\n",
    "                    'collect_traj': collect_trajs[agent_name],\n",
    "                    'train_traj': train_trajs[agent_name],\n",
    "                    'test_traj': test_trajs[agent_name],\n",
    "                    'buffer': buffers[agent_name],\n",
    "                }\n",
    "                for callback in self.callbacks:\n",
    "                    callback(histories[agent_name][epoch])\n",
    "\n",
    "        return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback:\n",
    "\n",
    "    def __init__(self, hparams: dict, print_hparam_keys: List[str] = None, print_data_keys: List[str] = None):\n",
    "        if print_hparam_keys is None:\n",
    "            print_hparam_keys = ['epoch']\n",
    "        if print_data_keys is None:\n",
    "            print_data_keys = []\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.print_hparam_keys = print_hparam_keys\n",
    "        self.print_data_keys = print_data_keys\n",
    "\n",
    "    def __call__(self, data: Mapping[str, any]):\n",
    "        for key in self.print_hparam_keys:\n",
    "            print(f'{key}: {self.hparams[key]}', end='\\t')\n",
    "        for key in self.print_data_keys:\n",
    "            print(f'{key}: {data[key]}', end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEvalCallback:\n",
    "\n",
    "    def __init__(self, \n",
    "        eval_on_collect: bool = True, \n",
    "        eval_on_train: bool = False, \n",
    "        eval_on_test: bool = False):\n",
    "\n",
    "        self.eval_on_collect = eval_on_collect\n",
    "        self.eval_on_train = eval_on_train\n",
    "        self.eval_on_test = eval_on_test\n",
    "\n",
    "    def __call__(self, data: Mapping[str, any]):\n",
    "        agent = data['agent']\n",
    "        if not hasattr(agent, 'q_eval'):\n",
    "            return\n",
    "\n",
    "        if self.eval_on_collect:\n",
    "            traj = data['collect_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_collect'] = q_val\n",
    "\n",
    "        if self.eval_on_train:\n",
    "            traj = data['train_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_train'] = q_val\n",
    "\n",
    "        if self.eval_on_test:\n",
    "            traj = data['test_traj']\n",
    "            q_val = agent.q_eval(traj)\n",
    "            data['q_test'] = q_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- make the reward optionally an advantage computation over last round\n",
    "- also make a recurrent DQN agent (estimate q function of a sequence of states)\n",
    "- make a simple greedy connect4 agent\n",
    "- make the preprocessor perform a columnwise mean pool before flattening\n",
    "- train the preprocessor on an auxillary objective to estimate the max connected for each length for self and for oponent\n",
    "- add padding='SAME'|'VALID' to conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomAgent(Agent):\n",
    "    \"\"\"Takes a random action on each timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int):\n",
    "        super(RandomAgent, self).__init__(policy=self._policy)\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        choices = np.random.randint(0, self.num_actions, (obs.shape[0],))\n",
    "        onehots = np.eye(self.num_actions)[choices]\n",
    "        return onehots\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): timestep trajectory.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "agent = RandomAgent(num_actions=5)\n",
    "obs = Var(np.array([[1, 2], [3, 4]]))\n",
    "step = Step(obs=None, next_obs=obs, action=None, reward=None, done=None, info=None)\n",
    "agent.forward(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RealDQN(Agent):\n",
    "    \"\"\"'Classic' Deep Q-learning agent.\n",
    "    Implements the approach in https://arxiv.org/pdf/1312.5602.pdf.\n",
    "    \n",
    "    NOTE: This agent expects its encoder to output a per-column vector.\n",
    "        I.E.: [B, H, W, C] --encoder--> [B, W, d_enc]\n",
    "\n",
    "    This agent uses the following hyperparameters:\n",
    "        - activation: activation function to use for the hidden layers.\n",
    "        - hidden_size: hidden layer size.\n",
    "        - discount: discount factor.\n",
    "        - optimizer: optimizer to use.\n",
    "        - epsilon_start: initial epsilon value.\n",
    "        - min_epsilon: minimum epsilon value.\n",
    "        - epsilon_decay: epsilon decay rate.\n",
    "        - epoch: current epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder  # [B, H, W, C] -> [B, W, d_enc]\n",
    "        self.neck = Flatten()\n",
    "        self.head = Sequential([\n",
    "            Dense(hparams['hidden_size'], hparams['activation']), \n",
    "            Dense(1, Linear)\n",
    "        ]) # [B, L+|A|] -> [B, 1]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(RealDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        B = obs.shape[0]\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = max(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            indeces = np.random.randint(0, self.num_actions, (B,))\n",
    "            return np.eye(self.num_actions)[indeces]\n",
    "    \n",
    "        # Otherwise take the action with the highest Q-value\n",
    "        q_vals = np.zeros((B, self.num_actions))  # [B, self.num_actions]\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "\n",
    "        for action_index in range(self.num_actions):\n",
    "\n",
    "            # prepare action\n",
    "            action_T = Var(np.repeat(\n",
    "                np.array([action_index])[None, :], \n",
    "                repeats=B, axis=0))  # [B, 1]\n",
    "\n",
    "            # run the network\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [B, W*d_enc+A]\n",
    "            q_T = self.head(cat_T)  # [B, 1]\n",
    "\n",
    "            # store q-values\n",
    "            q_vals[:, action_index] = q_T.val[:, 0]\n",
    "        \n",
    "        # select the action with the highest Q-value\n",
    "        action_indeces = np.argmax(q_vals, axis=1)  # [B]\n",
    "        onehots = np.eye(self.num_actions)[action_indeces]  # [B, self.num_actions]\n",
    "        return onehots\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_T = Var(step.action, trainable=False)  # [B, A]\n",
    "            next_obs_T = Var(step.next_obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_next_T = Var(self.policy(step.next_obs), trainable=False)  # [B, A]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "            cat_T = Concat([enc_T, action_T], axis=1)  # [B, d_enc+A]\n",
    "            Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            next_enc_T = self.neck(self.encoder(next_obs_T))  # [B, W*d_enc]\n",
    "            next_cat_T = Concat([next_enc_T, action_next_T], axis=1)  # [B, d_enc+A]\n",
    "            Qnext_T = self.head(next_cat_T)[:, 0]  # [B]\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above eq; small updates are \n",
    "            # ensured by small SGD lr instead)\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Qnext_T)) - Qnow_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        action_T = Var(action)  # [B, A]\n",
    "        enc_T = self.neck(self.encoder(obs_T))  # [B, W*d_enc]\n",
    "        cat_T = Concat([enc_T, action_T], axis=1)  # [B, W*d_enc+A]\n",
    "        Qnow_T = self.head(cat_T)[:, 0]  # [B]\n",
    "\n",
    "        return Qnow_T.val\n",
    "\n",
    "\n",
    "test_encoder = Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    AxisMaxPooling(1),\n",
    "])  # [B, H, W, 2] -> [B, W, d_enc]\n",
    "\n",
    "test_batch_size = 5\n",
    "test_num_actions = 7\n",
    "\n",
    "test_step = Step(\n",
    "    obs=np.random.rand(test_batch_size, test_num_actions, test_num_actions, 2),\n",
    "    next_obs=np.random.rand(test_batch_size, test_num_actions, test_num_actions, 2),\n",
    "    action=np.random.rand(test_batch_size, test_num_actions),\n",
    "    reward=np.random.rand(test_batch_size),\n",
    "    done=False,\n",
    "    info={}\n",
    ")\n",
    "\n",
    "test_hparams = {\n",
    "    'hidden_size': 256,\n",
    "    'activation': Relu,\n",
    "    'optimizer': SGD(1e-3),\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_decay': 0.9,\n",
    "    'min_epsilon': 0.1,\n",
    "    'discount': 0.99,\n",
    "    'epoch': 0,\n",
    "}\n",
    "\n",
    "test_agent = RealDQN(num_actions=test_num_actions, encoder=test_encoder, hparams=test_hparams)\n",
    "test_agent.forward(test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01377991, 0.01392456, 0.01374394, 0.01349929, 0.01379792,\n",
       "        0.01335696, 0.01345566],\n",
       "       [0.01376041, 0.0137251 , 0.01350533, 0.01346597, 0.0135998 ,\n",
       "        0.01348645, 0.0133151 ],\n",
       "       [0.01363546, 0.01402088, 0.01349636, 0.01343496, 0.0136607 ,\n",
       "        0.01344927, 0.01360759],\n",
       "       [0.01365713, 0.01390815, 0.01360794, 0.01366415, 0.0133655 ,\n",
       "        0.01352175, 0.01323169],\n",
       "       [0.01382336, 0.01378832, 0.01344049, 0.01340312, 0.01361403,\n",
       "        0.01345332, 0.01339135]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CategoricalDQN(Agent):\n",
    "    \"\"\"Categorical deep Q-network agent.\n",
    "    I never read the paper for this architecture, so my implementation may\n",
    "    be different from the origonal researchers.\n",
    "    \n",
    "    NOTE: This agent expects its encoder to output a per-column vector.\n",
    "        I.E.: [B, H, W, C] --encoder--> [B, W, d_enc]\n",
    "\n",
    "    This agent uses the following hyperparameters:\n",
    "        - activation: activation function to use for the hidden layers.\n",
    "        - categorical_hidden_size: hidden layer size.\n",
    "        - discount: discount factor.\n",
    "        - optimizer: optimizer to use.\n",
    "        - epsilon_start: initial epsilon value.\n",
    "        - min_epsilon: minimum epsilon value.\n",
    "        - epsilon_decay: epsilon decay rate.\n",
    "        - epoch: current epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, encoder: Layer, hparams: dict):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.encoder = encoder  # [B, H, W, C] -> [B, W, d_enc]\n",
    "        self.neck = Flatten()\n",
    "        self.head = Sequential([\n",
    "            Dense(hparams['categorical_hidden_size'], hparams['activation']), \n",
    "            Dense(1, Linear)\n",
    "        ]) # [B, W, d_enc] -> [B, A, 1]\n",
    "        self.hparams = hparams\n",
    "\n",
    "        super(CategoricalDQN, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        B = obs.shape[0]\n",
    "\n",
    "        # Maybe take greedy step\n",
    "        epsilon = self.hparams['epsilon_start'] * self.hparams['epsilon_decay']**self.hparams['epoch']\n",
    "        epsilon = max(epsilon, self.hparams['min_epsilon'])\n",
    "        if random.random() < epsilon:\n",
    "            indeces = np.random.randint(0, self.num_actions, (B,))\n",
    "            return np.eye(self.num_actions)[indeces]\n",
    "        \n",
    "        # Otherwise estimate Q-values for all actions\n",
    "        obs_T = Var(obs, trainable=False)  # [B, H, W, C]\n",
    "        enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "        qvals_T = self.head(enc_T)  # [B, W, 1]\n",
    "        return qvals_T[..., 0].val  # [B, W]\n",
    "\n",
    "    \n",
    "    def train(self, traj: Traj):\n",
    "        \"\"\"Train your agent on a sequence of Timestep's.\n",
    "\n",
    "        Args:\n",
    "            traj (Traj): batched timestep trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = self.hparams['optimizer']\n",
    "        discount_T = Var(self.hparams['discount'], trainable=False)  # []\n",
    "        for step in traj:\n",
    "\n",
    "            obs_T = Var(step.obs, trainable=False)  # [B, H, W, 2]\n",
    "            action_indeces = np.argmax(step.action, axis=1)  # [B]\n",
    "            obs_next_T = Var(step.next_obs, trainable=False)  # [B, H, W, 2]\n",
    "            r_T = Var(step.reward, trainable=False)  # [B]\n",
    "\n",
    "            # compute previous Q value using the actual (not necesarily optimal) action selected\n",
    "            enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "            qvals_T = self.head(enc_T)[..., 0]  # [B, W]\n",
    "            Q_now_T = qvals_T[action_indeces]  # [B]\n",
    "            reg_loss_now_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # compute the maximum possible next step Q-value\n",
    "            enc_next_T = self.encoder(obs_next_T)  # [B, W, d_enc]\n",
    "            qvals_T = self.head(enc_next_T)[..., 0]  # [B, W]\n",
    "            Q_next_T = ReduceMax(qvals_T, axis=1)  # [B]  \n",
    "            # equivalent to: Q_next_T = ReduceMax(qvals_T, axis=1)\n",
    "            reg_loss_next_T = self.encoder.loss + self.head.loss  # []\n",
    "\n",
    "            # train the current policy to estimate new Q-value \n",
    "            # i.e.: approx Qnew_T <= (1-lr)*Qnow_T + lr*(r_T+discount_T*Qnext_T)  # [B, 1]\n",
    "            # using gradient descent (let lr=1 in the above; small updates are handled in the SGD step)\n",
    "            # but only update the targets that were actually selected for action at `step_now`.\n",
    "            loss_T = ReduceSum(((r_T+discount_T*StopGrad(Q_next_T)) - Q_now_T)**2,\n",
    "                axis=0) + reg_loss_now_T + reg_loss_next_T  # []\n",
    "            optimizer.minimize(loss_T)\n",
    "\n",
    "\n",
    "    def q_eval(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Evaluate the Q-value of a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): observation.\n",
    "            action (np.ndarray): action.\n",
    "\n",
    "        Returns:\n",
    "            q_val (np.ndarray): Q-value of the given state-action pair.\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare inputs\n",
    "        obs_T = Var(obs)  # [B, H, W, 2]\n",
    "        action_indeces = np.argmax(action, axis=1)  # [B]\n",
    "\n",
    "        enc_T = self.encoder(obs_T)  # [B, W, d_enc]\n",
    "        qvals_T = self.head(enc_T)[..., 0]  # [B, W]\n",
    "\n",
    "        return qvals_T.val[action_indeces]  # [B]\n",
    "\n",
    "test_hparams['categorical_hidden_size'] = 32\n",
    "test_hparams['epoch'] = 100\n",
    "test_agent = CategoricalDQN(num_actions=test_num_actions, encoder=test_encoder, hparams=test_hparams)\n",
    "test_agent.forward(test_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwiredConnect4Agent(Agent):\n",
    "\n",
    "    def __init__(self, board_size: int, hparams: dict):\n",
    "        self.board_size = board_size\n",
    "        self.hparams = hparams\n",
    "        super(HardwiredConnect4Agent, self).__init__(policy=self._policy)\n",
    "\n",
    "    def _policy(self, obs: np.ndarray) -> np.ndarray:\n",
    "        B = obs.shape[0]\n",
    "        action = np.zeros(B, dtype=np.int32)\n",
    "        for b in range(B):\n",
    "            o = obs[b]\n",
    "            ## TODO: make a greedy agent\n",
    "            action[b] = random.randint(0, self.board_size-1)\n",
    "        return action\n",
    "\n",
    "    def train(self, traj: Traj):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]\n",
       " [-1.  0.  0.  0.  0.  0.  0.]\n",
       " [ 1.  0.  0.  0.  0.  0.  0.]]\n",
       "Turn: 1\n",
       "Winner: 0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Board:\n",
    "    \"\"\"Drafted by copilot with minor human edits\"\"\"\n",
    "\n",
    "    def __init__(self, size=7, win_length=4):\n",
    "        self.size = size\n",
    "        self.win_length = win_length\n",
    "        self.board = np.zeros((size, size))\n",
    "        self.turn = 1\n",
    "        self.winner = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.board}\\nTurn: {self.turn}\\nWinner: {self.winner}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.board == other.board\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.board.tostring())\n",
    "\n",
    "    def is_full(self):\n",
    "        return np.count_nonzero(self.board) == self.size**2\n",
    "\n",
    "    def is_empty(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return 0 <= col < self.size and self.is_empty(col)\n",
    "\n",
    "    def make_move(self, col):\n",
    "        if self.is_valid_move(col):\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-1]\n",
    "            self.board[highest_row, col] = self.turn\n",
    "            self.turn *= -1\n",
    "\n",
    "    def undo_move(self, col):\n",
    "        if self.is_valid_move(col) and self.board[0, col] != 0:\n",
    "            highest_row = np.where(self.board[:, col] == 0)[0][-2]\n",
    "            self.board[highest_row, col] = 0\n",
    "            self.turn *= -1\n",
    "\n",
    "    def check_win(self) -> int:\n",
    "        for turn in [-1, 1]:\n",
    "            if self.num_connected(self.win_length, turn) > 0:\n",
    "                self.winner = turn\n",
    "                return True\n",
    "        return self.winner\n",
    "\n",
    "    def num_connected(self, length, turn):\n",
    "        num_connected = 0\n",
    "        # Check horizontal\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size-length+1):\n",
    "                if np.all(self.board[row, col:col+length] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check vertical\n",
    "        for col in range(self.size):\n",
    "            for row in range(self.size-length+1):\n",
    "                if np.all(self.board[row:row+length, col] == turn):\n",
    "                    num_connected += 1\n",
    "        # Check diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(self.size-length+1):\n",
    "                if all(self.board[row+i, col+i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        # Check anti-diagonal\n",
    "        for row in range(self.size-length+1):\n",
    "            for col in range(length-1, self.size):\n",
    "                if all(self.board[row+i, col-i] == turn for i in range(length)):\n",
    "                    num_connected += 1\n",
    "        return num_connected\n",
    "\n",
    "board = Board()\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board.make_move(0)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [0.16932953 0.99895812 0.27073135 0.0236947  0.53996092 0.25891625\n",
      " 0.22873497 0.96096692 0.07179285 0.85949869]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.56739632 0.38746584 0.57229289 0.6962179  0.4002677  0.9528083\n",
      " 0.70463487 0.85074523 0.60315655 0.60174327]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0. -1.  0.  0.  0.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.26388332 0.70330738 0.72605748 0.23912332 0.14797825 0.96929218\n",
      " 0.38475469 0.59391951 0.37943118 0.29967008]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0. -1.  0.  0.  0.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.93840749 0.86779823 0.13269115 0.03886775 0.84117635 0.57467481\n",
      " 0.45661289 0.29190151 0.30911583 0.73921479]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  0.  0.  0. -1.  0.  0.  0.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.27114254 0.34123184 0.95232445 0.65485001 0.96334997 0.51471432\n",
      " 0.14231776 0.3500175  0.7447475  0.70510938]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  0.  0.  1. -1.  0.  0.  0.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599453\n",
      "\n",
      "Action: [0.10095573 0.25321754 0.21233761 0.85932404 0.28156529 0.5461739\n",
      " 0.41520378 0.68244997 0.71343452 0.35645245]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  0. -1.  1. -1.  0.  0.  0.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599453\n",
      "\n",
      "Action: [0.33074891 0.10846269 0.91004089 0.81701538 0.46493505 0.77722089\n",
      " 0.87861039 0.57136028 0.14696845 0.22641182]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1.  0.  0.  0.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599453\n",
      "\n",
      "Action: [0.33020988 0.96224112 0.17100865 0.8625647  0.61942773 0.65860491\n",
      " 0.29536559 0.50567994 0.57519009 0.90219503]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1.  0.  0.  0.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.27654924 0.11165341 0.8019749  0.33791734 0.45189808 0.65178449\n",
      " 0.15832426 0.9582341  0.56989524 0.24168502]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1.  0.  1.  0.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599453\n",
      "\n",
      "Action: [0.28067792 0.6461333  0.08153059 0.18523698 0.62101939 0.47402538\n",
      " 0.91141462 0.20870137 0.83353018 0.2552223 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  0.  0.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599453\n",
      "\n",
      "Action: [0.2872128  0.37963838 0.28689223 0.87708269 0.62238433 0.69929568\n",
      " 0.67408427 0.31651258 0.33722633 0.48272372]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  0.  0.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599453\n",
      "\n",
      "Action: [0.5888952  0.02627093 0.01944686 0.34992654 0.39855675 0.04006723\n",
      " 0.35543005 0.85078708 0.87740967 0.95403411]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  0. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -1.3862943611198906\n",
      "\n",
      "Action: [0.89157652 0.35320884 0.44145183 0.32069202 0.39542856 0.40328696\n",
      " 0.41503819 0.32190965 0.0845546  0.58789767]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  1.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  0. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599456\n",
      "\n",
      "Action: [0.9823461  0.67046518 0.43708041 0.21023019 0.30918674 0.39123092\n",
      " 0.32533153 0.79817804 0.77004297 0.01638282]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  1.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  0. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -2.220446049250313e-16\n",
      "\n",
      "Action: [0.85473618 0.3752824  0.38698177 0.42256927 0.18862748 0.61578456\n",
      " 0.39709849 0.96003877 0.58122277 0.11433792]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  0. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -4.440892098500626e-16\n",
      "\n",
      "Action: [0.45352104 0.31191798 0.72440947 0.32636609 0.60997837 0.2164957\n",
      " 0.55875892 0.49839743 0.63946311 0.31252207]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1. -1.  1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  0. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599458\n",
      "\n",
      "Action: [0.06523763 0.91746926 0.322544   0.56718556 0.23882524 0.61855811\n",
      " 0.14571702 0.69058052 0.95412844 0.825353  ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1. -1.  1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.21683189 0.8271579  0.26886518 0.47445927 0.3943944  0.36402676\n",
      " 0.91840509 0.15580712 0.54599645 0.68838605]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1. -1.  1.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -4.440892098500626e-16\n",
      "\n",
      "Action: [0.95491024 0.29847654 0.77961476 0.69147421 0.91422214 0.55994673\n",
      " 0.86344094 0.17598273 0.52717052 0.21072769]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1. -1.  1.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.3862943611198904\n",
      "\n",
      "Action: [0.69994583 0.94486027 0.63032978 0.2724703  0.73116396 0.91370685\n",
      " 0.95661515 0.7035508  0.35072345 0.84836843]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 1. -1. -1.  1.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.7917594692280554\n",
      "\n",
      "Action: [0.82552944 0.17092532 0.93529179 0.73688797 0.69785438 0.51749715\n",
      " 0.77096949 0.21587523 0.25007647 0.4566535 ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 1. -1. -1.  1.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.3036208  0.44424324 0.71737988 0.67350671 0.59171292 0.02984788\n",
      " 0.3832213  0.94075928 0.16636432 0.12137634]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599445\n",
      "\n",
      "Action: [0.33632807 0.46502237 0.01950399 0.20145295 0.3861441  0.49045884\n",
      " 0.32119606 0.7296667  0.1843896  0.10636842]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -2.484906649788\n",
      "\n",
      "Action: [0.07370531 0.26966101 0.08115196 0.37400138 0.73168288 0.67768744\n",
      " 0.72352503 0.26590544 0.34647485 0.66176735]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 1.3862943611198908\n",
      "\n",
      "Action: [0.96061962 0.5266381  0.67081281 0.61987607 0.0462075  0.04697471\n",
      " 0.3296753  0.20240609 0.3731567  0.27833685]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599454\n",
      "\n",
      "Action: [0.54665023 0.27636048 0.07700321 0.05931257 0.19288458 0.27021724\n",
      " 0.15849081 0.77330445 0.39294876 0.69059972]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: -0.6931471805599454\n",
      "\n",
      "Action: [0.16118628 0.02235787 0.99364842 0.36507682 0.39996935 0.84884075\n",
      " 0.09004369 0.87768323 0.91979322 0.77233281]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 0.6931471805599454\n",
      "\n",
      "Action: [0.59310278 0.69764635 0.38164363 0.4943308  0.86944253 0.25367144\n",
      " 0.93490371 0.50620676 0.13053252 0.97179392]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0. -1.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 0.0\n",
      "\n",
      "Action: [0.62639882 0.55406985 0.81653982 0.57498591 0.34722834 0.3531832\n",
      " 0.13759214 0.36336229 0.22180596 0.51333131]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0. -1.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: 1.0986122886681091\n",
      "\n",
      "Action: [0.83657106 0.88494194 0.09673738 0.20612965 0.29444245 0.96032482\n",
      " 0.50876723 0.79137361 0.27094422 0.74128315]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0. -1.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 2.484906649788\n",
      "\n",
      "Action: [0.43477825 0.85046433 0.98212726 0.89792236 0.97908004 0.36755294\n",
      " 0.37811362 0.72946051 0.62531471 0.12631949]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0. -1.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 0\n",
      "Reward: -1.0986122886681091\n",
      "\n",
      "Action: [0.3391915  0.86673931 0.49671742 0.60025772 0.76809365 0.84586382\n",
      " 0.78152671 0.63335044 0.04082932 0.20879084]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1. -1.  1.  0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0. -1.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: 1\n",
      "Winner: 0\n",
      "Reward: 3.552713678800501e-15\n",
      "\n",
      "Action: [0.15888467 0.65289917 0.73357651 0.60911884 0.62630372 0.64879918\n",
      " 0.30958234 0.60920225 0.02025578 0.19866124]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [-1. -1.  1.  0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1.  1. -1.  1. -1.  1.  0. -1.]\n",
      " [-1.  1.  1. -1.  1. -1. -1.  1.  1. -1.]]\n",
      "Turn: -1\n",
      "Winner: 1\n",
      "Reward: 1.609437912434096\n",
      "\n",
      "Winner: 1\n"
     ]
    }
   ],
   "source": [
    "class BoardEnv:\n",
    "\n",
    "    def __init__(self, board_size=7, win_length=4, reward_mode: str = 'sparse'):\n",
    "        \"\"\"RL environment for Connect 4. \n",
    "\n",
    "        Args:\n",
    "            board_size (int, optional): The size of the board. Defaults to 7.\n",
    "            win_length (int, optional): The minimum connected length to win. Defaults to 4.\n",
    "            reward_mode (str, optional): One of 'sparse', 'dense_stateless', 'dense_advantage'. \n",
    "                - For 'sparse', the reward is 1 if the player has attained a connect `win_length`,\n",
    "                    and is 0 otherwise.\n",
    "                - For 'dense_stateless', the reward increases linearly with the number of N-in-a-row's\n",
    "                    for all values of N from 0 to board_size weighted logarithmically by N.\n",
    "                - For 'dense_advantage', the reward is determined by the difference between the\n",
    "                    previous and current dense reward for each player individually.\n",
    "                `reward_mode` defaults to 'sparse'.\n",
    "        \"\"\"\n",
    "        self.board_size = board_size\n",
    "        self.win_length = win_length\n",
    "        self.reward_mode = reward_mode\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> NoBatchStep:\n",
    "        self.board = Board(self.board_size, self.win_length)\n",
    "\n",
    "        if self.reward_mode == 'dense_advantage':\n",
    "            self.prev_dense_reward = [0., 0.]\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=None,\n",
    "            action=None,\n",
    "            next_obs=self._make_obs(), \n",
    "            reward=0., \n",
    "            done=False, \n",
    "            info=dict()\n",
    "        )\n",
    "\n",
    "    def step(self, action: np.ndarray) -> NoBatchStep:\n",
    "        \"\"\"Apply agent X's action to the board and \n",
    "        returns the next agent's timestep.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): array shaped (board_size,). The arg max\n",
    "                action index is the column where next piece is placed.\n",
    "\n",
    "        Returns:\n",
    "            tuple: NoBatchStep with values:\n",
    "                obs (np.ndarray[H, W, 2]): None\n",
    "                next_obs (np.ndarray[H, W, 2]): the next board state with\n",
    "                    self's entered squares represented in channel 0 and\n",
    "                    opponent's squares represented in channel 1\n",
    "                action (np.ndarray[board_size]): None\n",
    "                reward (float): the reward for the agent\n",
    "                    If sparse_reward is True, then reward is -1, 0, or +1.\n",
    "                    If sparse_reward is False, then reward is:\n",
    "                        ego_dense_reward - opponent_dense_reward.\n",
    "                done (bool): whether the game is over\n",
    "                info (dict): extra information\n",
    "        \"\"\"\n",
    "        # Apply action\n",
    "        action_index = np.argmax(action)  # []\n",
    "        self.board.make_move(action_index)  # this flips `board.turn`\n",
    "        # Temporarily unflip `board.turn`\n",
    "        self.board.turn *= -1\n",
    "\n",
    "        # Make egocentric observation\n",
    "        obs = self._make_obs()\n",
    "\n",
    "        # Compute reward\n",
    "        # This is the lazy way to do it, but it's fast enough\n",
    "\n",
    "        # Sparse reward\n",
    "        winner = self.board.check_win()\n",
    "        sparse_reward = self.board.turn * winner\n",
    "\n",
    "        # Dense reward\n",
    "        def dense_reward_for_turn(board, turn):\n",
    "            r = 0\n",
    "            for length in range(2, self.board_size):\n",
    "                r += math.log(length) * board.num_connected(length, turn)\n",
    "            return r\n",
    "        ego_dense_reward = dense_reward_for_turn(self.board, self.board.turn)\n",
    "        opponent_dense_reward = dense_reward_for_turn(self.board, -self.board.turn)\n",
    "        dense_reward = ego_dense_reward - opponent_dense_reward  \n",
    "\n",
    "        if self.reward_mode == 'sparse':\n",
    "            reward = sparse_reward\n",
    "        elif self.reward_mode == 'dense_stateless':\n",
    "            reward = dense_reward\n",
    "        elif self.reward_mode == 'dense_advantage':\n",
    "            turn_index = (self.board.turn+1)//2\n",
    "            reward = dense_reward - self.prev_dense_reward[turn_index]\n",
    "            self.prev_dense_reward[turn_index] = dense_reward\n",
    "        else:\n",
    "            raise ValueError(f'Invalid reward_mode: {self.reward_mode}')\n",
    "\n",
    "        # Evaluate whether game is over\n",
    "        winner = self.board.check_win()\n",
    "        done = winner != 0\n",
    "\n",
    "        # Record debugging info\n",
    "        info = dict()\n",
    "\n",
    "        # Revert temporary flip on `board.turn`\n",
    "        self.board.turn *= -1\n",
    "\n",
    "        return NoBatchStep(\n",
    "            obs=None,\n",
    "            next_obs=obs, \n",
    "            action=None,\n",
    "            reward=reward, \n",
    "            done=done, \n",
    "            info=info\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "    def _make_obs(self) -> np.ndarray:\n",
    "        \"\"\"Only show ego values on first channel and opponent values on second channel\"\"\"\n",
    "        obs = np.stack([\n",
    "            self.board.turn * self.board.board, \n",
    "            -self.board.turn * self.board.board\n",
    "            ], axis=-1)  # [board_size, board_size, 2]\n",
    "        obs[obs<0] = 0  # rectify negative values\n",
    "        return obs\n",
    "\n",
    "board_size = 10\n",
    "env = BoardEnv(board_size=board_size, win_length=5, reward_mode='dense_advantage')\n",
    "\n",
    "step = env.reset()\n",
    "while not step.done:\n",
    "    action = np.random.uniform(0, 1, (board_size,))\n",
    "    step = env.step(action)\n",
    "\n",
    "    print(f'Action: {action}')\n",
    "    env.render()\n",
    "    print(f'Reward: {step.reward}\\n')\n",
    "\n",
    "print(f'Winner: {env.board.winner}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(\n",
    "    hidden_size=256,                # hidden layer size for RealDQN\n",
    "    categorical_hidden_size=32,     # hidden layer size for CategoricalDQN\n",
    "    activation=Relu,                # activation function for networks\n",
    "    optimizer=SGD(1e-3),            # optimizer for networks\n",
    "    epsilon_start=1.0,              # Starting value for epsilon\n",
    "    epsilon_decay=0.95,             # Decay rate for epsilon per epoch\n",
    "    min_epsilon=0.01,               # Final value for epsilon\n",
    "    discount=0.99,                  # Discount factor\n",
    "    epoch=0,                        # Current epoch\n",
    "    epochs=10,                      # Number of training epochs\n",
    "    board_size=8,                   # Board size\n",
    "    win_length=4,                   # Number of pieces in a row needed to win\n",
    "    min_steps_per_epoch=1000,       # Minimum number of steps per epoch\n",
    "    batch_size=32,                  # Number of samples per training batch\n",
    ")\n",
    "\n",
    "encoder = Sequential([\n",
    "    Conv2D(32, 3, 2, 'same', Relu),\n",
    "    Conv2D(64, 3, 2, 'same', Relu),\n",
    "    AxisMaxPooling(1),\n",
    "])  # [B, H, W, 2] -> [B, W, d_enc]\n",
    "agent = RealDQN(hparams['board_size'], encoder, hparams)\n",
    "\n",
    "self_play_agents = dict(\n",
    "    Bob=agent,\n",
    "    Alice=agent,\n",
    ")\n",
    "\n",
    "env = BoardEnv(board_size=hparams['board_size'], win_length=hparams['win_length'], \n",
    "               reward_mode='dense_stateless')\n",
    "\n",
    "trainer = ParallelTrainer(\n",
    "    hparams=hparams,\n",
    "    callbacks=[\n",
    "        PrintCallback(hparams=hparams, print_hparam_keys=['epoch'], print_data_keys=['reward']),\n",
    "        QEvalCallback(eval_on_collect=True, eval_on_train=True, eval_on_test=True)\n",
    "    ]).train(self_play_agents, env)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "605fe966a75bc2c3dfa708e269323e6491854b30a36f4e77953579e94649bfba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
